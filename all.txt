================================================================================
TRIFORM DOCUMENTATION - COMPLETE REFERENCE
================================================================================
Generated: Thu Oct  9 13:52:39 CEST 2025
================================================================================


================================================================================
FILE: index.mdx
================================================================================

---
title: "Welcome"
description: "Triform overview and quickstart"
---

Triform is a visual platform for building, running, and monitoring AI-powered systems. Instead of stitching together code and configs, you **compose logic on a Canvas** as Nodes (Agents, Flows, Actions) and connect them with Edges.

## Who this is for

- Engineers building end-to-end AI workflows
- Product teams iterating quickly on prototypes
- Non-technical collaborators reviewing and testing behavior

## Quickstart

1. **Sign in** at [app.triform.ai](https://app.triform.ai) with Discord or GitHub
2. You land in your first **Project** with the **Canvas** open
3. **Chat** with Triton and ask him to build something
4. **Watch** Triton plan, build, and test your solution
5. **Execute** it from the **Properties → Execute** panel

> tip: Start with a Flow: _"Build me a news gatherer that collects news from Swedish sources and summarizes them"_. Execute with a test payload to verify your setup.

## What you can build

- Tool-using **Agents** that call Actions/Flows
- **Flows** that orchestrate multi-step logic
- **Actions** that run precise Python code
- APIs or scheduled jobs that expose top-level components

## How Triform is organized

**Organizations** — Top-level ownership and access  
**Projects** — Focused workspaces inside an Organization  
**Library** — Reusable building blocks (Actions, Flows, etc.)  
**Canvas** — The visual surface where you design and debug

## Need help?

- Use the Chat Panel (Triton) to create, connect, or run components
- Invite teammates from Profile → Organization → View
- Share feedback anytime—your input shapes our roadmap


================================================================================
FILE: getting-started/login.mdx
================================================================================

---
title: "Login & Registration"
description: "Sign in with Email, Discord, or GitHub"
---

## Sign in

1. Go to [app.triform.ai](https://app.triform.ai)
2. Choose **Email**, **Discord**, or **GitHub**
3. Authorize and you'll be redirected to your Canvas

## First-time registration

- A personal Organization is created automatically
- A default Project opens on the Canvas

## Switching Organizations

You can belong to multiple Organizations. Switch via Profile → Organization.

## Troubleshooting

**Popup blocked** — Allow popups for `app.triform.ai`  
**Wrong account** — Sign out of your provider, then sign in again  
**Stuck redirect** — Clear cookies for `app.triform.ai` or use a private window  
**2FA issues** — Resolve with your provider, then repeat OAuth

If problems persist, contact support via [Discord](https://discord.gg/triform).


================================================================================
FILE: getting-started/quickstart.mdx
================================================================================

---
title: "Quickstart"
description: "Get up and running with Triform in 5 minutes"
---

## Prerequisites

- A Discord or GitHub account
- A modern browser (Chrome, Firefox, Safari, or Edge)

## Step 1: Sign in

1. Go to [app.triform.ai](https://app.triform.ai)
2. Click **Sign in with Discord** or **Sign in with GitHub**
3. Authorize Triform
4. You'll land in your workspace

## Step 2: Create your first Flow

Let's build a simple workflow using Triton:

1. Click in the **Chat Panel** at the bottom
2. Type: _"Build me a Flow that summarizes the latest news from HackerNews"_
3. Watch Triton plan, create, and connect the components

## Step 3: Execute your Flow

1. Select the Flow node on the Canvas
2. Open the **Properties Panel** on the right
3. Go to the **Execute** tab
4. Click **Execute**
5. View the results in the output panel

## Next steps

- **Modify** — Ask Triton to add more Actions or Agents
- **Deploy** — Make your Flow accessible via API
- **Explore** — Check execution history and logs

## Need help?

- Ask Triton directly in the Chat Panel
- Join our [Discord community](https://discord.gg/triform)
- See the [Workspace Overview](/getting-started/workspace-overview) for interface details



================================================================================
FILE: getting-started/workspace-overview.mdx
================================================================================

---
title: "Workspace Overview"
description: "Navigate the interface: Top Bar, Canvas, Chat, Properties"
---

## The workspace

**Top Bar** — Navigation, breadcrumbs, and deploy  
**Canvas** — Visual workspace where you build  
**Chat Panel** — Triton, your AI assistant  
**Properties Panel** — Configuration, inputs, and execution

## Basic interactions

**Click** — Select a node  
**Double-click** — Open a node (Project/Flow/Agent)  
**Right-click** — Context menus

## Quick start

1. Create a Project or Flow on the Canvas
2. Or ask Triton to build something for you
3. Execute with a test payload from Properties → Execute

> tip: Try building a Flow with Input → Action (returns a message) → Output. Execute it with a simple JSON payload to verify your setup.


================================================================================
FILE: tutorials/build-a-new-project.mdx
================================================================================

---
title: "Build a New Project"
description: "Create a complete AI system from scratch"
---

## Overview

In this tutorial, you'll build a **News Aggregator Project** that collects articles from multiple sources, summarizes them, and outputs a daily digest. You'll learn to:

- Create a Project from scratch
- Build Actions for data collection
- Design Flows for orchestration
- Configure an Agent for summarization
- Test and deploy your system

**Time required:** 20-30 minutes

## Step 1: Create the Project

1. Click the **Home** icon in the Top Bar
2. Click **New Project**
3. Name it `News Aggregator`
4. Add description: `Collects and summarizes news from multiple sources`
5. Click **Create**

Your Canvas now shows an empty Project node.

## Step 2: Build the fetch Action

Let's create an Action to fetch news from an API:

1. Open the Chat Panel
2. Say: _"Create an Action called `fetch_news` that takes a source URL and returns a list of articles"_
3. Triton will generate the Action with:
   - Input: `source_url` (string)
   - Output: `articles` (list of dicts)
   - Basic HTTP request logic

4. Review the generated `Action.py` in the Properties Panel
5. Adjust if needed (add error handling, rate limiting, etc.)

## Step 3: Create the aggregation Flow

Now build a Flow to call the Action for multiple sources:

1. Ask Triton: _"Create a Flow that takes a list of news sources and calls fetch_news for each one"_
2. Triton will create:
   - Input node with `sources` parameter
   - Loop or parallel execution of `fetch_news`
   - Output node with combined `articles`

3. Double-click the Flow node to inspect the structure
4. Verify the connections are correct

## Step 4: Add a summarization Agent

Create an Agent to summarize the collected articles:

1. Say: _"Create an Agent called `news_summarizer` that takes articles and creates a concise daily digest"_
2. Triton will configure:
   - System prompt for summarization
   - Input/output schema
   - Model selection (GPT-4 or Claude recommended)

3. Review and refine the system prompt:
   ```
   You are a news summarization expert. Given a list of news articles,
   create a concise, informative daily digest. Group by topic, highlight
   key trends, and maintain neutral tone. Output as structured markdown.
   ```

## Step 5: Connect the components

Wire everything together:

1. Create a main Flow called `daily_news_digest`
2. Structure it as:
   ```
   Input (sources list)
     → fetch_news_flow (aggregation)
     → news_summarizer (Agent)
     → Output (formatted digest)
   ```

3. You can do this via Triton or manually:
   - Drag components from the Agent Toolbox
   - Connect them with edges (click and drag from output to input ports)
   - Configure each node's parameters

## Step 6: Test with sample data

Execute your system:

1. Select the `daily_news_digest` Flow
2. Open Properties → Execute
3. Enter test payload:
   ```json
   {
     "sources": [
       "https://newsapi.org/v2/top-headlines?country=us",
       "https://api.example.com/tech-news",
       "https://api.example.com/world-news"
     ]
   }
   ```

4. Click **Execute**
5. Monitor execution in real-time
6. Review the output digest

## Step 7: Debug and iterate

If something goes wrong:

- Check execution logs in Properties → Executions
- Verify each component's input/output schema
- Test Actions individually before running the full Flow
- Ask Triton: _"Why did the execution fail?"_

Common issues:
- API authentication missing → Add API keys to Project Variables
- Timeout errors → Adjust Action timeout settings
- Empty results → Verify API endpoints are accessible

## Step 8: Deploy as API

Make your Project accessible:

1. Click **Deploy** in the Top Bar
2. Select deployment target (e.g., production)
3. Configure:
   - Endpoint path: `/daily-digest`
   - HTTP method: POST
   - Authentication: API key required

4. Click **Deploy Now**
5. Copy the generated endpoint URL
6. Test with curl:

This is an API-call that would trigger a certain flow with the 
{"sources": ["..."]} payload:

   ```bash
   curl -X POST 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784' \
     --header 'Authorization: 88ffe8398f972837ccc6e0a6cd31c5c92689040c' \
     --header 'Content-Type: application/json' \
     --data '{"sources": ["..."]}'
   ```

## Step 9: Schedule daily execution

Set up automatic runs:

1. In the Project Properties, go to **Triggers**
2. Click **Add Trigger**
3. Select **Schedule**
4. Configure:
   - Schedule: Daily at 6:00 AM UTC
   - Payload: Your default sources list
   - Notification: Email on success/failure

5. Save the trigger

## Next steps

Now that you've built a complete Project:

- **Enhance it** — Add sentiment analysis, language detection, custom formatting
- **Monitor it** — Set up alerts for failures or anomalies
- **Share it** — Invite team members to collaborate
- **Extend it** — Create variations for different news categories

## Tips for success

> **Start simple** — Build and test each component individually before connecting them

> **Use Project Variables** — Store API keys, base URLs, and configuration centrally

> **Save test payloads** — Keep example inputs for regression testing

> **Version your work** — Use meaningful names and descriptions for components

> **Iterate with Triton** — Don't hesitate to ask for modifications or explanations



================================================================================
FILE: tutorials/edit-a-specific-component.mdx
================================================================================

---
title: "Edit a Specific Component"
description: "Modify individual Agents, Flows, or Actions in detail"
---

## Overview

This tutorial focuses on editing individual components (Agents, Flows, Actions) within a Project. You'll learn component-specific techniques for each type.

**Time required:** 10-15 minutes per component type

## Editing Actions

Actions are Python functions with inputs, outputs, and dependencies.

### Opening an Action

1. Double-click the Action node on the Canvas
2. The Properties Panel shows:
   - **Content tab** — Code editor for `Action.py`
   - **Input/Output tab** — Schema definition
   - **Requirements** — Python dependencies

### Modifying the code

**Example:** Add logging to a data processing Action

1. Select the Action
2. Open Properties → Content
3. Update the code:
   ```python
   import logging
   from typing import Dict, List
   
   logger = logging.getLogger(__name__)
   
   def process_data(data: List[Dict]) -> List[Dict]:
       logger.info(f"Processing {len(data)} items")
       
       processed = []
       for item in data:
           try:
               # processing logic
               result = transform(item)
               processed.append(result)
           except Exception as e:
               logger.error(f"Failed to process item: {e}")
               continue
       
       logger.info(f"Successfully processed {len(processed)} items")
       return processed
   ```

4. Update `requirements.txt` if adding dependencies
5. Click **Test** to run with sample input
6. Save (auto-saves)

### Changing input/output schema

1. Go to Properties → Input/Output
2. Modify the schema:
   - Add new fields
   - Change types
   - Update descriptions
   - Set required vs. optional

3. Update the code to match
4. Test with new schema

### Best practices for Actions

> **Keep them focused** — One clear purpose per Action

> **Type everything** — Use proper type hints for all parameters

> **Handle errors** — Don't let exceptions propagate unhandled

> **Test independently** — Each Action should work standalone

## Editing Flows

Flows are graphs of connected nodes.

### Opening a Flow

1. Click the Flow node
2. The Canvas zooms into the Flow's internal structure
3. You see: Input node, processing nodes, Output node, edges

### Adding nodes

**Via Triton:**
1. Ask: _"Add a data validation Action to this Flow"_
2. Triton adds the node and suggests connections

**Manually:**
1. Right-click the Canvas
2. Select **Add Node**
3. Choose from: Actions, Agents, sub-Flows
4. Drag to position

### Connecting nodes

1. Click and drag from an output port
2. Drop on an input port of another node
3. The edge shows data flow direction
4. Orange edges = control flow, Blue edges = data flow

### Modifying the structure

**Serial to parallel conversion:**

Before (serial):
```
Input → Action A → Action B → Action C → Output
```

After (parallel):
```
Input → Action A ↘
                  Action B → Merge → Output
        Action C ↗
```

Steps:
1. Add a Merge/Join node
2. Disconnect B and C from serial chain
3. Connect both to Merge
4. Connect Merge to Output

### Conditional routing

Add branching logic:

1. Add a Router node (or use Agent to decide)
2. Connect multiple output paths
3. Each path processes different scenarios
4. Reconnect at a Merge node if needed

Example:
```
Input → Router → [if premium] → Enhanced Processing → Output
              → [if basic]   → Standard Processing → Output
```

### Best practices for Flows

> **Left to right** — Arrange nodes in execution order

> **Group visually** — Related nodes should be close together

> **Name clearly** — Label nodes with their purpose

> **Test incrementally** — Verify each section works before adding more

## Editing Agents

Agents are LLM-powered components with prompts and tools.

### Opening an Agent

1. Double-click the Agent node
2. Properties Panel shows:
   - **Content tab** — Prompts and model config
   - **Toolbox** — Available tools (Actions/Flows)
   - **Execute tab** — Testing interface

### Modifying the System Prompt

The System Prompt defines the Agent's behavior.

**Example:** Improve a customer service Agent

Before:
```
You are a helpful assistant.
```

After:
```
You are a customer service expert for TechCorp, specializing in
subscription management and technical support. 

Guidelines:
- Always be polite and professional
- Verify the user's identity before accessing account information
- Offer solutions, not just explanations
- Escalate to human support if unable to resolve
- Document all actions taken in the conversation

Available tools:
- check_subscription_status: Get current subscription details
- update_billing: Modify billing information
- reset_password: Send password reset email
- escalate_to_human: Transfer to human support

Always confirm actions before executing them.
```

### Adding/removing tools

1. Go to Properties → Content → Toolbox
2. Click **Add Tool**
3. Select from available Actions/Flows
4. Tools appear in the Agent's context
5. Update the System Prompt to mention new tools

### Configuring model parameters

Adjust for your use case:

| Parameter | Low Value | High Value | Use Case |
|-----------|-----------|------------|----------|
| Temperature | 0.0-0.3 | 0.7-1.0 | Low: factual, consistent<br/>High: creative, varied |
| Top P | 0.1-0.5 | 0.9-1.0 | Low: focused<br/>High: exploratory |
| Max Tokens | 100-500 | 2000-4000 | Low: concise<br/>High: detailed |

### Testing Agent changes

1. Go to Properties → Execute
2. Enter a test message or payload:
   ```json
   {
     "messages": [
       {"role": "user", "content": "I need to cancel my subscription"}
     ]
   }
   ```

3. Click **Execute**
4. Review:
   - Agent's response
   - Tools called
   - Reasoning/chain of thought
   - Token usage

5. Iterate on prompts based on results

### Best practices for Agents

> **Be specific** — Vague prompts lead to unpredictable behavior

> **Limit tools** — Too many options confuse the Agent

> **Test edge cases** — Try to break it with unusual inputs

> **Monitor usage** — Track token costs and latency

## Using Triton for edits

Triton can help with all component types:

### For Actions
- _"Add error handling to this Action"_
- _"Optimize this function for large datasets"_
- _"Add logging at each step"_

### For Flows
- _"Add parallel processing to this Flow"_
- _"Insert a validation step after the input"_
- _"Add error routing to this Flow"_

### For Agents
- _"Make this Agent more concise in responses"_
- _"Add a tool for checking inventory"_
- _"Improve the prompt for customer service"_

## Version management

Track changes to components:

1. **Export before major changes**
   - Right-click component → Export
   - Save the JSON locally

2. **Meaningful naming**
   - Rename nodes with version info if needed
   - `CustomerServiceAgent_v2`

3. **Test before overwriting**
   - Create a copy to test changes
   - Compare performance
   - Merge if improvement is confirmed

## Common editing scenarios

### Scenario 1: Action is too slow

**Diagnosis:** Profile the Action, find bottlenecks

**Solutions:**
- Add caching for expensive operations
- Use async/await for I/O operations
- Batch API calls instead of one-by-one
- Optimize algorithms (O(n²) → O(n log n))

### Scenario 2: Flow produces wrong output

**Diagnosis:** Trace the data through each node

**Solutions:**
- Check schema mismatches between nodes
- Add logging to intermediate nodes
- Test each node individually
- Verify edge connections are correct

### Scenario 3: Agent doesn't use tools

**Diagnosis:** Prompt doesn't encourage tool use

**Solutions:**
- Explicitly instruct: "Use available tools to answer"
- Provide examples of tool usage in prompt
- Reduce temperature for more deterministic behavior
- Simplify tool descriptions

### Scenario 4: Component works in test, fails in production

**Diagnosis:** Environment differences

**Solutions:**
- Check Project Variables are set in production
- Verify API keys and credentials
- Review rate limits and quotas
- Check for hardcoded values (don't do this!)

## Next steps

Continue exploring the documentation to learn about building new Projects, integrating them into your apps, and understanding Actions, Agents, and Flows.



================================================================================
FILE: tutorials/edit-an-existing-project.mdx
================================================================================

---
title: "Edit an Existing Project"
description: "Modify and enhance Projects you've built or inherited"
---

## Overview

This tutorial covers how to safely modify existing Projects, whether you created them, inherited them from teammates, or are exploring community templates.

**Time required:** 15-20 minutes

## Step 1: Open the Project

1. Navigate to **Home** in the Top Bar
2. Find your Project in the Projects list
3. Click to open it on the Canvas

You'll see the Project's component graph.

## Step 2: Understand the structure

Before making changes, map out what exists:

1. **Review the Canvas** — Note all Flows, Agents, and Actions
2. **Check dependencies** — See which components depend on others
3. **Read documentation** — Look for README or descriptions
4. **Ask Triton** — Type: _"Explain what this Project does"_

Triton will analyze the structure and provide a summary.

## Step 3: Identify what to change

Common modification scenarios:

### Adding a new feature

**Example:** Add email notifications to a data processing pipeline

1. Create a new Action: `send_email`
2. Add it to the relevant Flow
3. Connect it after the processing step
4. Configure SMTP settings in Project Variables

### Modifying an existing component

**Example:** Update an Agent's prompt

1. Select the Agent node
2. Open Properties → Content
3. Edit the System Prompt
4. Save changes (auto-saved)
5. Test with a sample payload

### Replacing a component

**Example:** Swap out an Action for a better implementation

1. Create the new Action
2. Ensure it has the same input/output schema
3. Disconnect the old Action
4. Connect the new one
5. Delete the old Action
6. Test thoroughly

### Debugging an issue

**Example:** A Flow is failing intermittently

1. Go to Properties → Executions
2. Find failed execution
3. Examine logs and errors
4. Identify the failing component
5. Ask Triton: _"Why is this Action failing?"_
6. Apply the fix
7. Re-run execution

## Step 4: Make the changes

Let's walk through a specific example: **Adding retry logic to an API call**

### Current state
Your `fetch_data` Action sometimes fails due to network timeouts.

### Goal
Add automatic retry with exponential backoff.

### Steps

1. **Open the Action**
   - Double-click the `fetch_data` node
   - Review the current `Action.py`

2. **Ask Triton for help**
   - _"Add retry logic with exponential backoff to this Action"_
   - Triton will update the code with a retry decorator

3. **Review the changes**
   ```python
   from tenacity import retry, stop_after_attempt, wait_exponential
   
   @retry(
       stop=stop_after_attempt(3),
       wait=wait_exponential(multiplier=1, min=2, max=10)
   )
   def fetch_data(url: str) -> dict:
       # existing implementation
   ```

4. **Update requirements.txt**
   - Add `tenacity==8.2.3`

5. **Test the change**
   - Execute with a payload that previously failed
   - Verify retries happen (check logs)
   - Confirm eventual success

## Step 5: Test your changes

Always test modifications before deploying:

### Unit test individual components

1. Select the modified component
2. Use Properties → Execute with test data
3. Verify expected behavior

### Integration test the full Flow

1. Select the top-level Flow
2. Execute with realistic payloads
3. Check all outputs are correct

### Edge case testing

Test with:
- Empty inputs
- Maximum size inputs  
- Invalid data
- Network failures (if applicable)

## Step 6: Document your changes

Help your future self and teammates:

1. **Update descriptions**
   - Select the component
   - Update the description field
   - Note what changed and why

2. **Update README**
   - If the Project has a README, add notes
   - Document new dependencies or requirements

3. **Add comments in code**
   - Explain non-obvious logic
   - Note any workarounds or constraints

## Step 7: Deploy the updates

Once tested:

1. Click **Deploy** in the Top Bar
2. Select the environment
3. Review changes (diff view if available)
4. Add deployment notes
5. Click **Deploy Now**

For production Projects:
- Deploy to staging first
- Run smoke tests
- Monitor for issues
- Then deploy to production

## Step 8: Monitor post-deployment

After deploying:

1. **Watch initial executions**
   - Check the first few runs succeed
   - Look for unexpected errors

2. **Compare metrics**
   - Success rate before vs. after
   - Execution time
   - Error types

3. **Set up alerts**
   - Get notified if new issues arise
   - Track key metrics

## Common modification patterns

### Adding authentication

1. Create Project Variable for API key
2. Update relevant Actions to use the key
3. Test with valid and invalid keys

### Scaling for more data

1. Identify bottlenecks (single-threaded Actions)
2. Add parallel processing in Flows
3. Implement batching if needed
4. Test with large payloads

### Improving error handling

1. Add try-catch blocks in Actions
2. Return structured error responses
3. Add error routing in Flows
4. Set up failure notifications

### Optimizing performance

1. Profile slow components
2. Add caching where appropriate
3. Reduce unnecessary data passing
4. Parallelize independent operations

## Best practices

> **Version control** — Save the current state before major changes (export/backup)

> **Small iterations** — Make one change at a time, test, then move on

> **Ask for help** — Use Triton to explain complex parts before modifying

> **Test thoroughly** — Don't skip testing, especially for production Projects

> **Communicate** — If working in a team, coordinate changes

## Troubleshooting

**Problem:** Changes aren't taking effect  
**Solution:** Ensure you saved, redeploy if already deployed, clear any caches

**Problem:** Breaking existing functionality  
**Solution:** Review execution logs, compare with previous version, test each component

**Problem:** Can't understand the existing code  
**Solution:** Ask Triton to explain it, check for documentation, trace execution flow

**Problem:** Changes work locally but fail in production  
**Solution:** Check environment differences, verify Project Variables are set, review logs

## Next steps

Continue exploring the documentation to learn about editing specific components, integrating Projects into your apps, and deployment best practices.



================================================================================
FILE: tutorials/integrate-project-into-your-app.mdx
================================================================================

---
title: "Integrate Project into Your App"
description: "Connect your Triform Project to external applications via API"
---

## Overview

Once you've built and tested a Project in Triform, you'll want to integrate it into your application. This tutorial covers API integration, authentication, and best practices.

**Time required:** 20-30 minutes

## Prerequisites

- A deployed Triform Project
- API key for authentication
- Basic knowledge of HTTP requests

## Step 1: Deploy your Project

Before integrating, deploy your Project:

1. Open your Project on the Canvas
2. Click **Deploy** in the Top Bar
3. Select the target environment (staging or production)
4. Configure the deployment:
   - **Endpoint name:** Choose a memorable name (e.g., `news-digest`)
   - **HTTP method:** POST (most common) or GET
   - **Authentication:** API key required (recommended)

5. Click **Deploy Now**
6. Copy the generated endpoint URL:
   ```
   https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784
   ```

## Step 2: Generate an API key

1. Go to Profile → API Keys
2. Click **Create New Key**
3. Name it: `Production Integration`
4. Set permissions: Execute Projects
5. Copy the key (shown only once!)
6. Store securely (use environment variables)

> **Security note:** Never commit API keys to version control. Use environment variables or secret managers.

## Step 3: Make your first API call

### Using cURL

This is an API-call that would trigger a certain flow with the 
{"sources": ["https://newsapi.org/v2/top-headlines?country=us", "https://api.example.com/tech-news"]} payload:

```bash
curl -X POST 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784' \
  --header 'Authorization: 88ffe8398f972837ccc6e0a6cd31c5c92689040c' \
  --header 'Content-Type: application/json' \
  --data '{"sources": ["https://newsapi.org/v2/top-headlines?country=us", "https://api.example.com/tech-news"]}'
```

### Using Python

```python
import requests
import os

API_KEY = os.environ.get('TRIFORM_API_KEY')
ENDPOINT = 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784'

headers = {
    'Authorization': API_KEY,
    'Content-Type': 'application/json'
}

payload = {
    'sources': [
        'https://newsapi.org/v2/top-headlines?country=us',
        'https://api.example.com/tech-news'
    ]
}

response = requests.post(ENDPOINT, json=payload, headers=headers)

if response.status_code == 200:
    result = response.json()
    print('Success:', result)
else:
    print('Error:', response.status_code, response.text)
```

### Using JavaScript (Node.js)

```javascript
const axios = require('axios');

const API_KEY = process.env.TRIFORM_API_KEY;
const ENDPOINT = 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784';

const headers = {
  'Authorization': API_KEY,
  'Content-Type': 'application/json'
};

const payload = {
  sources: [
    'https://newsapi.org/v2/top-headlines?country=us',
    'https://api.example.com/tech-news'
  ]
};

axios.post(ENDPOINT, payload, { headers })
  .then(response => {
    console.log('Success:', response.data);
  })
  .catch(error => {
    console.error('Error:', error.response?.status, error.response?.data);
  });
```

### Using JavaScript (Browser/React)

```javascript
const callTriformAPI = async () => {
  const API_KEY = process.env.REACT_APP_TRIFORM_API_KEY;
  const ENDPOINT = 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784';
  
  try {
    const response = await fetch(ENDPOINT, {
      method: 'POST',
      headers: {
        'Authorization': API_KEY,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        sources: [
          'https://newsapi.org/v2/top-headlines?country=us',
          'https://api.example.com/tech-news'
        ]
      })
    });
    
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    
    const data = await response.json();
    console.log('Success:', data);
    return data;
  } catch (error) {
    console.error('Error:', error);
  }
};
```

## Step 4: Handle responses

Triform API responses follow this structure:

### Success response (200 OK)

```json
{
  "execution_id": "exec_abc123",
  "status": "completed",
  "output": {
    "digest": "Top stories today include...",
    "article_count": 42,
    "topics": ["technology", "politics", "science"]
  },
  "execution_time_ms": 3420,
  "timestamp": "2025-10-01T10:30:00Z"
}
```

### Error response (4xx/5xx)

```json
{
  "error": {
    "code": "invalid_input",
    "message": "Field 'sources' is required",
    "details": {
      "field": "sources",
      "expected_type": "array"
    }
  },
  "request_id": "req_xyz789"
}
```

### Handling in code

```python
def call_triform_api(payload):
    response = requests.post(ENDPOINT, json=payload, headers=headers)
    
    if response.status_code == 200:
        data = response.json()
        return {
            'success': True,
            'data': data['output'],
            'execution_id': data['execution_id']
        }
    elif response.status_code == 400:
        error = response.json()['error']
        return {
            'success': False,
            'error': error['message'],
            'code': error['code']
        }
    elif response.status_code == 429:
        # Rate limit exceeded
        return {
            'success': False,
            'error': 'Rate limit exceeded. Try again later.'
        }
    elif response.status_code >= 500:
        # Server error
        return {
            'success': False,
            'error': 'Triform service temporarily unavailable'
        }
    else:
        return {
            'success': False,
            'error': f'Unexpected error: {response.status_code}'
        }
```

## Step 5: Implement error handling

### Retry logic with exponential backoff

```python
import time
from typing import Optional

def call_with_retry(payload, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.post(
                ENDPOINT,
                json=payload,
                headers=headers,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                # Rate limited, wait and retry
                wait_time = 2 ** attempt  # 1s, 2s, 4s
                time.sleep(wait_time)
                continue
            elif response.status_code >= 500:
                # Server error, retry
                wait_time = 2 ** attempt
                time.sleep(wait_time)
                continue
            else:
                # Client error, don't retry
                raise Exception(f"API error: {response.text}")
                
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            raise
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            raise
    
    raise Exception(f"Failed after {max_retries} attempts")
```

### Timeout handling

Always set timeouts to prevent hanging requests:

```python
# Set reasonable timeout (connect timeout, read timeout)
response = requests.post(
    ENDPOINT,
    json=payload,
    headers=headers,
    timeout=(5, 30)  # 5s to connect, 30s to read
)
```

## Step 6: Integrate into your application

### Example: Web dashboard

```python
# Flask app
from flask import Flask, render_template, request, jsonify

app = Flask(__name__)

@app.route('/dashboard')
def dashboard():
    return render_template('dashboard.html')

@app.route('/api/refresh-news', methods=['POST'])
def refresh_news():
    sources = request.json.get('sources', [])
    
    result = call_triform_api({
        'sources': sources
    })
    
    if result['success']:
        return jsonify({
            'digest': result['data']['digest'],
            'article_count': result['data']['article_count']
        })
    else:
        return jsonify({'error': result['error']}), 400
```

### Example: Scheduled job

```python
# Daily cron job
import schedule
import time

def fetch_daily_news():
    print("Fetching daily news digest...")
    
    payload = {
        'sources': [
            'https://newsapi.org/v2/top-headlines?country=us',
            'https://api.example.com/tech-news'
        ]
    }
    
    result = call_with_retry(payload)
    
    # Save to database or send email
    save_to_database(result['output']['digest'])
    send_email_digest(result['output']['digest'])
    
    print("Daily news digest completed")

# Schedule for 6 AM daily
schedule.every().day.at("06:00").do(fetch_daily_news)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### Example: Real-time processing

```python
# Process incoming webhooks
from fastapi import FastAPI, BackgroundTasks

app = FastAPI()

@app.post('/webhook/new-content')
async def handle_new_content(data: dict, background_tasks: BackgroundTasks):
    # Trigger Triform processing in background
    background_tasks.add_task(process_with_triform, data)
    return {'status': 'processing'}

def process_with_triform(data):
    result = call_triform_api({
        'content': data['content'],
        'metadata': data['metadata']
    })
    
    # Store results
    store_results(result)
```

## Step 7: Monitor and optimize

### Track API usage

```python
import logging

logger = logging.getLogger(__name__)

def call_triform_api_with_logging(payload):
    start_time = time.time()
    
    try:
        response = requests.post(ENDPOINT, json=payload, headers=headers)
        elapsed = time.time() - start_time
        
        logger.info(f"Triform API call completed in {elapsed:.2f}s, status: {response.status_code}")
        
        return response.json()
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"Triform API call failed after {elapsed:.2f}s: {str(e)}")
        raise
```

### Caching results

```python
from functools import lru_cache
import hashlib
import json

# Simple in-memory cache
cache = {}
CACHE_TTL = 3600  # 1 hour

def get_cache_key(payload):
    return hashlib.md5(json.dumps(payload, sort_keys=True).encode()).hexdigest()

def call_triform_cached(payload):
    cache_key = get_cache_key(payload)
    
    # Check cache
    if cache_key in cache:
        cached_data, timestamp = cache[cache_key]
        if time.time() - timestamp < CACHE_TTL:
            logger.info("Returning cached result")
            return cached_data
    
    # Call API
    result = call_triform_api(payload)
    
    # Store in cache
    cache[cache_key] = (result, time.time())
    
    return result
```

## Best practices

> **Use environment variables** — Never hardcode API keys

> **Implement retries** — Network issues happen; retry with backoff

> **Set timeouts** — Don't let requests hang indefinitely

> **Log everything** — Track success/failure for debugging

> **Cache when appropriate** — Save API calls for expensive operations

> **Monitor quota** — Track usage against your plan limits

> **Test error paths** — Ensure your app handles API failures gracefully

> **Version your integration** — Track which API version you're using

## Troubleshooting

**Problem:** 401 Unauthorized  
**Solution:** Check API key is correct and has Execute permissions

**Problem:** 429 Too Many Requests  
**Solution:** Implement rate limiting, add delays between requests

**Problem:** 500 Server Error  
**Solution:** Retry with backoff, check Triform status page

**Problem:** Slow responses  
**Solution:** Optimize your Project, add caching, use async requests

**Problem:** Unexpected output format  
**Solution:** Verify Project schema matches your expectations, check docs

## Next steps

Continue exploring the documentation to learn about deployments, API key management, monitoring, quotas, and security best practices.



================================================================================
FILE: triton/building.mdx
================================================================================

---
title: "Building with Triton"
description: "Create Projects, Agents, Flows, and Actions through conversation"
---

## Overview

Once you've defined your Project description (and possibly requirements), Triton can build it for you. This guide covers how to effectively use Triton to generate components, connect them, and get a working system.

## Building Actions

Actions are Python functions that do specific tasks.

### Basic Action creation

**You:** _"Create an Action that fetches data from a REST API"_

**Triton generates:**
- `Action.py` with:
  - Function signature
  - Type hints
  - Error handling
  - Documentation
- Input/output schema
- `requirements.txt` with dependencies

**Example output:**
```python
import requests
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

def fetch_data(url: str, headers: Dict[str, str] = None) -> Dict[str, Any]:
    """
    Fetches data from a REST API endpoint.
    
    Args:
        url: The API endpoint URL
        headers: Optional HTTP headers
        
    Returns:
        JSON response as dictionary
        
    Raises:
        requests.exceptions.RequestException: If the request fails
    """
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        logger.error(f"Timeout fetching {url}")
        raise
    except requests.exceptions.RequestException as e:
        logger.error(f"Error fetching {url}: {e}")
        raise
```

### Action with specific requirements

**You:** _"Create an Action that validates email addresses using a regex and checks against our blocklist in Project Variables"_

**Triton:**
- Generates regex pattern
- Adds Global Variable reference
- Includes validation logic
- Tests it

### Data transformation Action

**You:** _"Create an Action that takes a list of user objects and returns just their email addresses"_

**Triton generates:**
```python
from typing import List, Dict

def extract_emails(users: List[Dict]) -> List[str]:
    """
    Extracts email addresses from a list of user objects.
    
    Args:
        users: List of user dictionaries with 'email' key
        
    Returns:
        List of email addresses
    """
    emails = []
    for user in users:
        if 'email' in user and user['email']:
            emails.append(user['email'])
    return emails
```

## Building Agents

Agents are LLM-powered components with prompts and tools.

### Simple Agent

**You:** _"Create an Agent that summarizes text"_

**Triton creates:**
- System prompt:
  ```
  You are a summarization expert. Given text input, create a concise
  summary that captures the key points. Keep summaries under 200 words.
  Use clear, professional language.
  ```
- Input schema: `{ "text": "string" }`
- Output schema: `{ "summary": "string" }`
- Model: GPT-4 (default)

### Agent with tools

**You:** _"Create an Agent that answers questions about our product catalog using a search Action"_

**Triton:**
1. First checks if `search_catalog` Action exists
2. If not, asks: _"Should I create the search_catalog Action first?"_
3. Creates the Action
4. Creates the Agent with:
   - System prompt mentioning available tools
   - Tools configuration including `search_catalog`
   - Examples of tool usage

**System prompt:**
```
You are a product expert helping customers find information about our catalog.

Available tools:
- search_catalog: Search products by keyword, category, or ID

When a customer asks about products:
1. Use search_catalog to find relevant products
2. Present the information clearly
3. Suggest related products if appropriate

Always be helpful and accurate. If you can't find something, say so clearly.
```

### Decision-making Agent

**You:** _"Create an Agent that triages support tickets into urgent/normal/low priority"_

**Triton creates:**
- Detailed system prompt with triage criteria
- Input schema: `{ "ticket": { "subject": "string", "body": "string", "user_tier": "string" } }`
- Output schema: `{ "priority": "enum", "reason": "string", "suggested_assignment": "string" }`
- Lower temperature (0.3) for consistent decisions

## Building Flows

Flows connect components into workflows.

### Linear Flow

**You:** _"Create a Flow that validates input, processes it, and returns results"_

**Triton creates:**
```
Input → validate_input → process_data → format_output → Output
```

Each node is properly connected with appropriate data mappings.

### Parallel processing Flow

**You:** _"Create a Flow that fetches data from 3 different sources simultaneously, then combines the results"_

**Triton creates:**
```
Input → fetch_source_1 ↘
      → fetch_source_2 → combine_results → Output
      → fetch_source_3 ↗
```

Includes:
- Parallel execution of fetch Actions
- Combine node that waits for all inputs
- Proper error handling if any source fails

## Building complete Projects

Combine everything into a Project.

### End-to-end example

**You:** _"Build a complete Project for processing customer feedback: collect feedback, analyze sentiment, extract themes, generate report"_

**Triton creates:**

**1. Actions:**
- `validate_feedback` — Check feedback format
- `store_feedback` — Save to database
- `generate_report` — Format results

**2. Agent:**
- `analyze_feedback` — Sentiment analysis and theme extraction

**3. Main Flow:**
```
Input (feedback list)
  → validate_feedback (for each item)
  → store_feedback (batch)
  → analyze_feedback (Agent)
  → generate_report
  → Output (formatted report)
```

**4. Project node:**
- Exposes the main Flow as the Project entry point
- Configured with input/output schemas
- Ready to deploy

### Incremental building

You don't have to build everything at once:

**You:** _"Start a new Project called 'Email Processor'"_  
**Triton:** Creates empty Project

**You:** _"Add an Action to parse email headers"_  
**Triton:** Creates `parse_headers` Action

**You:** _"Add an Agent to classify emails"_  
**Triton:** Creates `classify_email` Agent

**You:** _"Connect them in a Flow"_  
**Triton:** Creates Flow linking parse_headers → classify_email

**You:** _"Add error handling for malformed emails"_  
**Triton:** Adds validation Action and error routing

## Building with context

Triton understands your existing work.

### Building on existing components

**You:** _"Create a new Flow that uses my existing Actions: fetch_data and process_results"_

**Triton:**
- Scans your Project
- Finds both Actions
- Creates Flow with proper connections
- Uses correct input/output schemas

### Consistent naming and patterns

**You:** _"Add another Action similar to fetch_user_data but for products"_

**Triton:**
- Reviews `fetch_user_data` implementation
- Creates `fetch_product_data` with similar structure
- Maintains consistent error handling
- Uses same coding style

### Building variants

**You:** _"Create a version of this Agent that's more concise"_

**Triton:**
- Copies existing Agent
- Modifies system prompt for brevity
- Names it `{original_name}_concise`
- Keeps same tools and config

## Building best practices

> **Start with the core path** — Build the main workflow first, add enhancements later

> **Test as you go** — Use Execute panel to verify each component

> **Be specific about behavior** — Tell Triton exactly what you want, including edge cases

> **Let Triton suggest** — If unsure, ask "How should I structure this?"

> **Review generated code** — Always check what Triton built before deploying

## Common building patterns

### Pattern: Extract-Transform-Load (ETL)

**You:** _"Build an ETL pipeline for customer data"_

**Triton creates:**
- Extract Action (API/database/file)
- Transform Agent/Action (cleanse, normalize)
- Load Action (destination)
- Flow connecting them with error handling

### Pattern: Request-Process-Notify

**You:** _"Build a workflow that takes requests, processes them, and notifies users"_

**Triton creates:**
- Validation Action
- Processing Agent/Flow
- Notification Action (email/webhook)
- Status tracking

### Pattern: Fan-out/Fan-in

**You:** _"Process 100 items in parallel, then aggregate results"_

**Triton creates:**
- Splitter (fan-out)
- Processing Action (parallel)
- Aggregator (fan-in)
- Flow with proper concurrency

### Pattern: Retry with backoff

**You:** _"Call this API with automatic retries"_

**Triton adds:**
- Retry decorator
- Exponential backoff
- Max attempts configuration
- Logging

## Troubleshooting building

### Problem: Triton doesn't understand the request

**Solution:** Break it down
- ❌ _"Build the whole system"_
- ✅ _"First, create an Action that validates input"_

### Problem: Generated code doesn't match needs

**Solution:** Provide examples
- _"Create an Action like this: [paste example]"_
- _"The output should look like: [paste JSON]"_

### Problem: Components aren't connecting properly

**Solution:** Be explicit
- _"Connect the output of fetch_data to the input of process_data"_
- _"Map the 'results' field to 'data' in the next node"_

### Problem: Missing error handling

**Solution:** Ask for it
- _"Add try-catch to handle network failures"_
- _"What if the input is empty?"_

## Next steps

After building:
1. Test each component with Execute panel
2. Edit and refine as needed
3. Debug any issues
4. Deploy your Project

Continue exploring the Triton documentation to learn about defining Projects, editing components, and debugging.



================================================================================
FILE: triton/defining.mdx
================================================================================

---
title: "Defining Projects with Triton"
description: "Structure and plan your Projects through conversation"
---

## Overview

Before building, it's helpful to define the high-level structure and goals of your Project. Triton can help you think through requirements, design decisions, and component architecture.

## Starting a new Project definition

### Describe the goal

Begin with what you want to accomplish:

**Examples:**
- _"I need a system that monitors social media for brand mentions and sends alerts"_
- _"I want to build a customer onboarding workflow that sends emails, provisions accounts, and notifies the team"_
- _"I'm building a content moderation system that flags inappropriate posts"_

Triton will ask clarifying questions:
- What inputs does it need?
- What outputs should it produce?
- What external services will it use?
- What's the expected volume/frequency?

### Sketch the architecture

Let Triton help you structure it:

**You:** _"I want to build a research assistant that gathers information from multiple sources"_

**Triton might propose:**
```
Project: Research Assistant
├── Input: Research query, sources list
├── Flow: Orchestration
│   ├── Action: fetch_from_source (for each source)
│   ├── Agent: synthesize_results
│   └── Action: format_output
└── Output: Formatted research report
```

You can then refine: _"Actually, add a validation step to filter low-quality sources first"_

## Defining component types

Triton helps you choose the right component for each task.

### When to use Actions

**Criteria:**
- Deterministic logic
- API calls
- Data transformation
- File operations

**Ask:** _"Should this be an Action or an Agent?"_

**Example:**  
_"I need to validate email addresses and check if they're on our blocklist"_  
**Triton:** This should be an Action—it's deterministic validation logic.

### When to use Agents

**Criteria:**
- Natural language processing
- Decision-making with context
- Creative generation
- Tool orchestration

**Ask:** _"Do I need an Agent here?"_

**Example:**  
_"I need to read customer feedback and categorize it into themes"_  
**Triton:** Yes, an Agent is appropriate—it requires understanding nuanced text.

### When to use Flows

**Criteria:**
- Multi-step processes
- Branching logic
- Parallel operations
- Composing other components

**Ask:** _"How should I structure this workflow?"_

**Example:**  
_"I need to process uploads, validate them, store them, and notify users"_  
**Triton:** Create a Flow with:
1. Input (file upload)
2. Validation Action
3. Storage Action
4. Notification Action
5. Output (confirmation)

## Defining inputs and outputs

Be clear about data shapes upfront:

### Input definition

**You:** _"My Project will receive user requests"_

**Triton:** _"What fields will each request have?"_

**You:** _"User ID, request type, and optional parameters"_

**Triton proposes:**
```json
{
  "user_id": "string (required)",
  "request_type": "enum: ['support', 'feature', 'bug'] (required)",
  "parameters": "object (optional)",
  "priority": "enum: ['low', 'medium', 'high'] (default: 'medium')"
}
```

### Output definition

**You:** _"It should return a status and details"_

**Triton proposes:**
```json
{
  "status": "enum: ['success', 'pending', 'failed']",
  "message": "string",
  "details": {
    "ticket_id": "string",
    "assigned_to": "string",
    "estimated_resolution": "ISO 8601 datetime"
  }
}
```

## Defining dependencies

Clarify what external resources you'll need:

### API dependencies

**You:** _"I need to integrate with Stripe and SendGrid"_

**Triton notes:**
- Actions will need `stripe` and `sendgrid` Python packages
- Project Variables needed: `STRIPE_API_KEY`, `SENDGRID_API_KEY`
- Rate limits to consider: Stripe (100/sec), SendGrid (varies by plan)

### Data dependencies

**You:** _"The system needs access to our product catalog"_

**Triton asks:**
- How is the catalog stored? (Database, API, file)
- How often does it change?
- Should we cache it?

### Authentication requirements

**You:** _"Users need to authenticate before making requests"_

**Triton suggests:**
- Add authentication Action or use built-in API key validation
- Define user roles if needed
- Handle token refresh if using OAuth

## Defining error handling

Plan for failures upfront:

**You:** _"What if the API call fails?"_

**Triton proposes:**
- Retry logic with exponential backoff
- Fallback to cached data if available
- Return graceful error message
- Log failure for monitoring

**You:** _"What if the input is malformed?"_

**Triton suggests:**
- Validation Action at Flow entry
- Return 400 error with clear message
- Log validation failures

## Defining scale and performance

Think about volume:

**You:** _"I expect about 1000 requests per day"_

**Triton notes:**
- Sequential processing is fine
- No special optimization needed
- Standard quotas should cover this

**You:** _"Actually, 1000 requests per hour during peak times"_

**Triton adjusts:**
- Consider parallel processing
- Add caching layer
- Monitor for rate limits
- Use batching where possible

## Example: Complete definition session

### You
_"I want to build a meeting notes processor"_

### Triton
What should it do with the meeting notes?

### You
_"Take transcripts, extract action items, identify owners, and send reminders"_

### Triton proposes

**Project: Meeting Notes Processor**

**Input:**
```json
{
  "transcript": "string (long text)",
  "participants": ["string"],
  "meeting_date": "ISO 8601 datetime"
}
```

**Components:**
1. **Agent: extract_action_items**
   - Input: transcript, participants
   - Output: list of action items with owners and due dates
   - Tools: none needed

2. **Action: validate_participants**
   - Input: participants list
   - Output: validated participants with contact info
   - External: Query user database

3. **Action: send_reminders**
   - Input: action items with owners
   - Output: reminder status
   - External: Email service (SendGrid)

4. **Flow: orchestration**
   - validate_participants → extract_action_items → send_reminders

**Output:**
```json
{
  "action_items": [
    {
      "description": "string",
      "owner": "string",
      "due_date": "ISO 8601 date",
      "reminder_sent": "boolean"
    }
  ],
  "summary": "string"
}
```

**Dependencies:**
- OpenAI/Anthropic API for Agent
- SendGrid for emails
- User database access

**Error handling:**
- If participant validation fails: continue with available participants
- If email sending fails: log and retry later
- If extraction fails: return transcript with error message

### You
_"Perfect! Let's build it."_

### Triton
I'll start with the `extract_action_items` Agent...

## Best practices for defining

> **Think through errors early** — It's easier to plan for failures than to add handling later

> **Be specific about data types** — Clear schemas prevent integration issues

> **Consider the happy path and edge cases** — Don't just think about perfect inputs

> **Start simple, then elaborate** — Define core functionality before adding features

> **Ask "what if"** — What if the API is down? What if input is huge? What if...

## Common definition anti-patterns

### ❌ Too vague
_"Make something that does AI stuff with data"_

### ✅ Specific
_"Create a Project that takes CSV files, extracts customer sentiment from a 'feedback' column, and returns a summary with positive/negative/neutral counts"_

---

### ❌ Skipping error handling
_"Just build it to work"_

### ✅ Planning for failure
_"Add retry logic for API calls, validation for inputs, and graceful degradation if the ML model is unavailable"_

---

### ❌ No clear boundaries
_"It should do everything related to customers"_

### ✅ Focused scope
_"This Project handles customer onboarding: account creation, welcome email, initial setup wizard. Support and billing are separate Projects"_

## Next steps

Once you've defined your Project, continue with:
1. Building it with Triton
2. Editing and refining components

Explore the Triton documentation for more details on these topics.



================================================================================
FILE: triton/editing.mdx
================================================================================

---
title: "Editing with Triton"
description: "Modify and refine your Projects using AI assistance"
---

## Overview

After building components, you'll often need to refine them. Triton can help you modify code, update prompts, restructure Flows, and optimize performance through natural conversation.

## Editing Actions

### Code modifications

**Simple changes:**

_"Add logging to this Action"_  
_"Add a timeout parameter"_  
_"Remove the deprecated function"_  
_"Rename the parameter from 'data' to 'input_data'"_

**Refactoring:**

_"Extract the validation logic into a separate function"_  
_"Convert this to async/await"_  
_"Replace the for loop with a list comprehension"_  
_"Add type hints to all functions"_

**Error handling:**

_"Add try-catch for JSON parsing errors"_  
_"Handle the case where the API returns 404"_  
_"Add validation for negative numbers"_  
_"Return a default value if the request fails"_

### Example: Adding retry logic

**You:** _"Add retry logic to this API call"_

**Triton modifies:**

Before:
```python
def fetch_data(url: str) -> dict:
    response = requests.get(url)
    return response.json()
```

After:
```python
from tenacity import retry, stop_after_attempt, wait_exponential
import requests

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def fetch_data(url: str) -> dict:
    """Fetches data with automatic retry on failure."""
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    return response.json()
```

And updates `requirements.txt`:
```
requests==2.31.0
tenacity==8.2.3
```

### Performance optimizations

_"Cache the results of this expensive function"_  
_"Batch the API calls instead of calling one at a time"_  
_"Use a connection pool"_  
_"Add async for I/O operations"_

## Editing Agents

### Prompt refinement

**Tone adjustments:**

_"Make this Agent more formal"_  
_"Make responses more concise"_  
_"Add a friendly, conversational tone"_  
_"Make it sound like a technical expert"_

**Behavior changes:**

_"Always ask for confirmation before taking actions"_  
_"Include sources for all facts mentioned"_  
_"Format responses as markdown"_  
_"Add step-by-step reasoning"_

**Example: Refining a support Agent**

**You:** _"Make this support Agent more empathetic and proactive"_

**Triton updates system prompt:**

Before:
```
You are a customer support assistant. Answer questions about our products.
```

After:
```
You are an empathetic customer support specialist dedicated to helping customers succeed.

Approach:
1. Acknowledge the customer's situation and any frustration
2. Provide clear, actionable solutions
3. Proactively suggest related help or preventive measures
4. Always confirm if the issue is resolved

Tone: Warm, patient, and professional. Show genuine care for the customer's experience.

Use available tools to check account status, create tickets, or look up documentation.
Always explain what you're doing and why.
```

### Tool management

**Adding tools:**

_"Add the search_documentation Action as a tool"_  
_"Give this Agent access to the billing_update Flow"_  

**Removing tools:**

_"Remove the delete_account tool—it's too risky"_  
_"This Agent shouldn't have access to user_data anymore"_

**Tool usage guidance:**

_"Update the prompt to encourage using the search tool first"_  
_"Add examples of when to use each tool"_  
_"Clarify that create_ticket should be used only when unable to resolve"_

### Model and parameter adjustments

_"Switch this Agent to Claude instead of GPT-4"_  
_"Lower the temperature to 0.2 for more consistent output"_  
_"Increase max tokens to 2000 for longer responses"_  
_"Add top_p=0.9 to reduce repetition"_

## Editing Flows

### Structural changes

**Adding nodes:**

_"Add a validation Action before processing"_  
_"Insert a logging step after each Action"_  
_"Add an error handler at the end"_

**Removing nodes:**

_"Remove the deprecated transform_data Action"_  
_"Delete the unused notification step"_

**Reordering:**

_"Swap the order of validation and normalization"_  
_"Move the notification to happen before storage, not after"_

### Converting serial to parallel

**You:** _"Make this Flow process items in parallel instead of sequentially"_

**Triton restructures:**

Before:
```
Input → process_item_1 → process_item_2 → process_item_3 → Output
```

After:
```
Input → [parallel]
          → process_item_1 ↘
          → process_item_2 → [merge] → Output
          → process_item_3 ↗
```

### Adding conditional logic

**You:** _"Route premium users to enhanced_process and regular users to standard_process"_

**Triton adds:**
```
Input → check_user_tier → [premium] → enhanced_process → Output
                        → [regular] → standard_process → Output
```

### Error handling in Flows

_"Add error routing so failed items go to a retry queue"_  
_"If validation fails, log the error and continue with the next item"_  
_"Add a fallback path if the primary API is down"_

## Editing entire Projects

### High-level modifications

**Feature additions:**

_"Add rate limiting to this Project"_  
_"Add authentication checks"_  
_"Implement caching throughout"_  
_"Add comprehensive logging"_

**Architectural changes:**

_"Split this monolithic Flow into three smaller sub-Flows"_  
_"Move all validation logic into a dedicated validation Flow"_  
_"Create a shared error handling Flow that all components use"_

**Performance improvements:**

_"Optimize this Project to handle 10x more requests"_  
_"Reduce the number of API calls"_  
_"Add batching to reduce execution time"_

### Refactoring for reusability

**You:** _"Extract the email sending logic so other Projects can use it"_

**Triton:**
1. Creates a new Action: `send_email`
2. Extracts the logic from the current Flow
3. Replaces inline logic with the new Action
4. Updates dependencies

Now multiple Projects can use `send_email`.

## Contextual editing

Triton understands your Project context when editing.

### Editing multiple related components

**You:** _"Update all Actions to use the new error logging format"_

**Triton:**
1. Scans all Actions in the Project
2. Identifies logging statements
3. Updates each one consistently
4. Reports what was changed

### Maintaining consistency

**You:** _"Make this new Action follow the same patterns as my existing ones"_

**Triton:**
- Reviews your existing Actions
- Matches coding style
- Uses same error handling approach
- Follows same naming conventions

### Bulk edits

**You:** _"Add a timeout parameter to all Actions that make API calls"_

**Triton:**
- Finds all relevant Actions
- Adds `timeout` parameter to each
- Updates function signatures
- Sets reasonable defaults

## Iterative refinement

Edit through conversation:

### Iteration example

**You:** _"Create an Action that fetches user data"_  
**Triton:** [Creates basic Action]

**You:** _"Add caching"_  
**Triton:** [Adds cache logic]

**You:** _"Cache should expire after 5 minutes"_  
**Triton:** [Adds TTL]

**You:** _"Add logging when cache hits vs misses"_  
**Triton:** [Adds logging]

**You:** _"Handle the case where the user doesn't exist"_  
**Triton:** [Adds not-found handling]

Each iteration builds on the previous changes.

## Editing with constraints

Give Triton constraints to guide edits:

**Performance:**
- _"Optimize this, but keep it under 100ms"_
- _"Reduce API calls without changing functionality"_

**Compatibility:**
- _"Update this but maintain backward compatibility"_
- _"Migrate to the new API while supporting the old one"_

**Style:**
- _"Refactor this to follow PEP 8"_
- _"Make this more Pythonic"_

**Safety:**
- _"Add input sanitization but don't break existing tests"_
- _"Improve error handling without changing output format"_

## Reviewing Triton's edits

Always review changes:

1. **Check the diff** — What exactly changed?
2. **Test the component** — Does it still work?
3. **Verify edge cases** — Does it handle errors properly?
4. **Review new dependencies** — Are they necessary and safe?

If something's not right:
- _"Undo that change"_
- _"That broke the validation, please fix"_
- _"Actually, let's try a different approach"_

## Best practices for editing

> **Be specific** — _"Add error handling"_ is vague; _"Add try-catch for JSON decode errors"_ is clear

> **Test after edits** — Always run Execute to verify changes work

> **Edit incrementally** — One change at a time is easier to debug

> **Explain why** — _"Add caching because this API is rate-limited"_ gives Triton better context

> **Review before deploying** — Changes look different in production

## Common editing scenarios

### Scenario: Action is failing in production

**Diagnosis:** _"Execution #1234 failed. What's wrong?"_

**Triton identifies:** Timeout error on API call

**Fix:** _"Add retry with exponential backoff and increase timeout to 30s"_

**Triton edits:** Adds tenacity retry decorator, increases timeout

**Verify:** Test with Execute, redeploy

### Scenario: Agent responses are too verbose

**Problem:** _"This Agent is writing essays instead of concise answers"_

**Fix:** _"Update the prompt to limit responses to 2-3 sentences"_

**Triton edits:** Adds constraint to system prompt

**Verify:** Test with sample inputs

### Scenario: Flow is too slow

**Problem:** _"This Flow takes 30 seconds to process 50 items"_

**Analysis:** _"Why is it slow?"_

**Triton identifies:** Sequential processing of independent items

**Fix:** _"Convert to parallel processing"_

**Triton edits:** Restructures Flow for parallel execution

**Result:** Processing time drops to ~5 seconds

## Advanced editing techniques

### Conditional edits

_"If the Action uses requests, add retry logic"_  
_"For all Agents that use tools, add tool usage examples in the prompt"_

### Pattern-based edits

_"Find all places where we call external APIs and add timeout handling"_  
_"Update all validation Actions to return structured error objects"_

### Version-based edits

_"Create a v2 of this Agent with the improvements, but keep v1 for now"_  
_"Update all components to use the new API, but create a flag to fall back to the old one"_

================================================================================
FILE: triton/overview.mdx
================================================================================

---
title: "Triton Overview"
description: "Your AI assistant for building, editing, and debugging Triform Projects"
---

## What is Triton?

Triton is Triform's built-in AI assistant that helps you build, modify, and debug your Projects through natural conversation. Instead of manually wiring nodes and writing code from scratch, you can describe what you want and Triton handles the implementation.

## Where to find Triton

Triton lives in the **Chat Panel** at the bottom of your workspace. Click the chat icon or press `Cmd+K` (Mac) / `Ctrl+K` (Windows) to open it.

## What Triton can do

### Build new components

- **Create Projects** — _"Build me a Project for processing customer feedback"_
- **Generate Agents** — _"Create an Agent that classifies support tickets by urgency"_
- **Write Actions** — _"Make an Action that fetches data from the GitHub API"_
- **Design Flows** — _"Build a Flow that validates input, processes it, and sends notifications"_

### Modify existing work

- **Update code** — _"Add error handling to this Action"_
- **Restructure Flows** — _"Convert this sequential Flow to run Actions in parallel"_
- **Refine prompts** — _"Make this Agent more concise in its responses"_
- **Add features** — _"Add logging to all Actions in this Project"_

### Debug and troubleshoot

- **Explain errors** — _"Why did this execution fail?"_
- **Analyze behavior** — _"Why is this Agent not using the tools?"_
- **Optimize performance** — _"How can I make this Flow faster?"_
- **Find issues** — _"What's causing the timeout in this Action?"_

### Answer questions

- **Explain concepts** — _"How do Project Variables work?"_
- **Provide guidance** — _"What's the best way to structure a multi-step workflow?"_
- **Review code** — _"Is this Action following best practices?"_
- **Suggest improvements** — _"How can I improve this Agent's prompt?"_

## How to use Triton effectively

### Be specific

❌ **Vague:** _"Fix this"_  
✅ **Specific:** _"Add retry logic with exponential backoff to the API call in this Action"_

❌ **Vague:** _"Make it better"_  
✅ **Specific:** _"Optimize this Flow to process items in parallel instead of sequentially"_

### Provide context

When asking about errors or issues:
- Mention which component (Action, Agent, Flow)
- Describe what you expected vs. what happened
- Share relevant error messages
- Indicate when it works vs. when it fails

**Example:**  
_"The `fetch_news` Action works in testing but times out in production when processing more than 10 sources. Can you add batching?"_

### Break down complex requests

Instead of: _"Build a complete customer service system with ticket routing, sentiment analysis, auto-responses, and escalation"_

Try:
1. _"Create an Agent that classifies support tickets"_
2. _"Add sentiment analysis to the ticket classifier"_
3. _"Build a Flow that routes tickets based on classification"_
4. _"Add auto-response Action for common issues"_
5. _"Implement escalation logic for high-priority tickets"_

### Iterate and refine

Triton's first attempt might not be perfect. Provide feedback:

- _"That's close, but make the prompt more formal"_
- _"Good, now add error handling"_
- _"The logic works, but can you add logging?"_

## Triton's workflow

When you make a request, Triton:

1. **Analyzes** your current Project context
2. **Plans** the implementation approach
3. **Executes** the changes (creates/modifies components)
4. **Validates** the changes work together
5. **Explains** what was done and next steps

You'll see this happening in real-time in the Chat Panel.

## Triton's capabilities

### Code generation

Triton writes production-ready Python code for Actions:
- Proper error handling
- Type hints
- Logging
- Input validation
- Idiomatic patterns

### Graph understanding

Triton comprehends your Flow structures:
- Identifies bottlenecks
- Suggests optimizations
- Maintains data flow integrity
- Preserves connections when editing

### Prompt engineering

For Agents, Triton crafts effective prompts:
- Clear instructions
- Example patterns
- Tool usage guidance
- Error handling behavior

### Best practices

Triton follows Triform conventions:
- Naming conventions
- Project structure
- Security patterns
- Performance optimizations

## Limitations

While Triton is powerful, it:

- **Can't** execute Projects (use Execute panel for that)
- **Can't** access external systems (unless via your Actions)
- **Can't** modify deployed Projects (must redeploy after changes)
- **May** need multiple iterations for complex requests
- **Works best** with clear, specific instructions

## Tips for success

> **Start with "why"** — Explain the goal, not just the task. "I need to process 1000 items quickly" gives better context than "make this parallel"

> **Review changes** — Always check what Triton built before executing. It's your code!

> **Use it for learning** — Ask "How does this work?" or "Why did you do it this way?"

> **Iterate incrementally** — Build and test small pieces, then expand

> **Combine with Execute** — Use Triton to build, Execute panel to test

## Common patterns

### Create-test-refine loop

1. _"Create an Action that calls the weather API"_
2. Test it with Execute panel
3. _"Add caching to reduce API calls"_
4. Test again
5. _"Add error handling for network failures"_
6. Test edge cases
7. Deploy

### Explore-then-modify

1. _"What does this Project do?"_
2. _"Why is this Agent using 3 tools?"_
3. _"How can I make it faster?"_
4. _"Implement that optimization"_

### Debug-fix-verify

1. _"Why did execution #123 fail?"_
2. _"Fix the null pointer error in the validation Action"_
3. Re-run execution
4. _"Add tests to prevent this in the future"_

## Next steps

Continue exploring the Triton documentation to learn about:
- Defining Projects conversationally
- Building components step-by-step
- Editing and modifying existing work

## Examples

See Triton in action:

### Example 1: Build a sentiment analyzer

**You:** _"Create an Agent that analyzes sentiment of customer reviews"_

**Triton:** Creates:
- Agent with appropriate system prompt
- Input schema for review text
- Output schema for sentiment (positive/negative/neutral) and confidence
- Example test case

### Example 2: Optimize a slow Flow

**You:** _"This Flow is taking 30 seconds to process 100 items. How can I speed it up?"_

**Triton:** 
- Analyzes the Flow structure
- Identifies sequential bottleneck
- Suggests parallel processing
- Implements batching with 10 items per batch
- Estimates new processing time (~5 seconds)

### Example 3: Fix an error

**You:** _"Execution failed with 'KeyError: source_url'. What's wrong?"_

**Triton:**
- Reviews the execution logs
- Identifies the Action expecting `source_url` but receiving `url`
- Suggests either: (1) fix the input mapping, or (2) update the Action to accept `url`
- Implements your chosen solution

## Feedback

Triton improves based on usage. If you encounter issues or have suggestions:
- Report via [Support](/support/report-a-bug)
- Share patterns that work well
- Request new capabilities

Your feedback shapes Triton's evolution!



================================================================================
FILE: workspace/canvas/agent-toolbox.mdx
================================================================================

---
title: "Agent Toolbox View"
description: "Configure agent tools and tips for curation"
---

## Characteristics

- Toolbox items are not connected by Edges.
- Tools are available for the Agent to invoke during execution.
- Tools can themselves contain other components.

## Creating tools

- Click the large ＋ to add a tool:
  - Action — atomic Python logic
  - Flow — orchestrated logic
  - Agent — nested agent for complex strategies

## Tips

- Keep tools small and testable.
- Prefer descriptive names (e.g., `fetch_user_profile`).
- Start with a few high‑value tools; expand as needs grow.

> note: Execution control — The Agent decides when to use tools; you define which tools it can use by curating the toolbox.

================================================================================
FILE: workspace/canvas/flow-view.mdx
================================================================================

---
title: "Flow View"
description: "Navigate and understand Flow internals"
---

## Overview

When you double-click a Flow node on the Canvas, you enter **Flow View**—a detailed visualization of the Flow's internal structure showing all nodes, edges, and data flow.

## What you see in Flow View

### Nodes

- **Input Node** (green) — Entry point(s) for the Flow
- **Processing Nodes** (blue/purple) — Actions, Agents, sub-Flows
- **Output Node** (orange) — Exit point(s) from the Flow
- **Control Nodes** (gray) — Routers, merges, loops

### Edges

- **Blue edges** — Data flow connections
- **Orange edges** — Control flow (conditions, loops)
- **Dotted edges** — Optional connections

### Ports

- **Input ports** (left side) — Where data enters a node
- **Output ports** (right side) — Where data exits a node
- **Named ports** — Labeled for clarity (e.g., "success", "error")

## Basic interactions

### Navigation

- **Pan** — Click and drag on empty Canvas space
- **Zoom** — Mouse wheel or trackpad pinch
- **Fit to screen** — Press `F` or click the fit icon
- **Reset view** — Press `R` or click reset icon

### Selection

- **Select node** — Click on it
- **Multi-select** — Hold `Shift` and click multiple nodes
- **Select all** — `Cmd+A` (Mac) / `Ctrl+A` (Windows)
- **Deselect** — Click on empty Canvas space

### Node details

- **View properties** — Click a node, check Properties Panel
- **Edit node** — Double-click to open (if it's a Flow/Agent)
- **See connections** — Hover over ports to highlight edges

## Understanding data flow

### Following execution path

Edges show how data moves through your Flow:

```
Input → validate → [if valid] → process → format → Output
                   [if invalid] → error_handler → Output
```

**Left to right** — Execution generally flows from left to right  
**Branching** — Multiple outgoing edges = conditional routing  
**Merging** — Multiple incoming edges = combining results

## Common Flow patterns

### Linear Flow

```
Input → Action A → Action B → Action C → Output
```
Sequential processing, simple and predictable.

### Parallel Flow

```
Input → Action A ↘
              Action B → Merge → Output
      → Action C ↗
```
Independent operations run simultaneously, then results combine.

### Conditional Flow

```
Input → Router → [condition 1] → Action A → Output
              → [condition 2] → Action B → Output
              → [else] → Action C → Output
```
Different paths based on input data or logic.

### Loop Flow

```
Input → Process Item ↺ (for each item) → Collect Results → Output
```
Repeat an operation for multiple items.

### Error handling Flow

```
Input → Action → [success] → Format → Output
              → [error] → Log Error → Retry or Fail
```
Separate paths for success and failure scenarios.

## Creating and connecting nodes

### Adding nodes

**Method 1: Via Triton**
- _"Add a Action that validates output to this Flow"_
- Triton adds and suggests connections

**Method 2: Right-click menu**
1. Right-click on Canvas
2. Select **Add Node**
3. Choose: Action, Agent, Flow, Control
4. Position the node

### Connecting nodes

1. **Click and drag** from an output port
2. **Drop** on an input port of another node
3. The edge appears, showing data flow

**Tips:**
- You can only connect compatible types
- Hover over ports to see data schema
- Hold `Shift` while dragging for precise placement

### Disconnecting nodes

1. **Click** the edge to select it
2. **Press** `Delete` or `Backspace`
3. Or **right-click** edge → **Delete**

## Node interactions

### Moving nodes

- **Drag** to reposition
- **Arrow keys** for fine adjustment
- **Hold Shift** while dragging to constrain to axis

### Copying nodes

1. Select node(s)
2. `Cmd+C` / `Ctrl+C` to copy
3. `Cmd+V` / `Ctrl+V` to paste

### Deleting nodes

1. Select node(s)
2. Press `Delete` or `Backspace`
3. Confirm deletion (edges are also removed)

### Renaming nodes

1. Select the node
2. Edit the name in Properties Panel
3. Or double-click the node label (if editable)

## Input and Output nodes

### Input Node

Defines what data enters the Flow.

**Configuration:**
- **Schema** — Define expected fields and types
- **Validation** — Set required vs. optional fields
- **Defaults** — Provide default values
- **Description** — Document what this input represents

**Example:**
```json
{
  "user_id": "string (required)",
  "action": "enum: ['create', 'update', 'delete'] (required)",
  "data": "object (optional)"
}
```

### Output Node

Defines what data exits the Flow.

**Configuration:**
- **Schema** — Define output fields and types
- **Multiple outputs** — Named exit points (success, error, etc.)
- **Mapping** — Map internal data to output format
- **Description** — Document what this output represents

**Example:**
```json
{
  "status": "string",
  "result": "object",
  "execution_time_ms": "number"
}
```

## Advanced Flow View features

### Minimap

Shows the full Flow with current viewport indicator.

- Toggle with `M` key
- Click to jump to that area
- Useful for large Flows

### Node search

Find nodes in complex Flows:

1. Press `Cmd+F` / `Ctrl+F`
2. Type node name
3. Canvas highlights and zooms to matching nodes

### Layout auto-arrange

Organize messy Flows:

1. Select all nodes (or leave unselected for all)
2. Right-click → **Auto Layout**
3. Choose: Horizontal, Vertical, or Grid
4. Flow rearranges automatically

### Comments and annotations

Add notes to your Flow:

1. Right-click empty space
2. Select **Add Comment**
3. Type your note
4. Position it near relevant nodes

Use comments to:
- Explain complex logic
- Mark TODOs
- Document assumptions
- Highlight temporary workarounds

## Debugging in Flow View

### Viewing execution state

When an execution is selected:

- **Green nodes** — Executed successfully
- **Red nodes** — Failed
- **Gray nodes** — Skipped (not in execution path)
- **Yellow nodes** — Currently executing (live view)

### Inspecting node outputs

1. Select an execution in Properties → Executions
2. Click a node in Flow View
3. Properties Panel shows:
   - Input data received
   - Output data produced
   - Execution time
   - Logs and errors

### Finding bottlenecks

Node execution times are displayed:

- **Thick borders** — Slower nodes (longer execution)
- **Thin borders** — Fast nodes
- **Tooltip** — Shows exact timing on hover

Identify slow nodes to optimize.

## Best practices for Flow View

> **Left-to-right flow** — Arrange nodes in execution order

> **Group related nodes** — Position them close together

> **Use vertical space** — Separate parallel branches vertically

> **Minimize edge crossings** — Crossing edges are hard to follow

> **Label nodes clearly** — Descriptive names beat generic ones

> **Add comments** — Document non-obvious logic

> **Keep it shallow** — If too deep, split into sub-Flows

## Keyboard shortcuts

| Action | Shortcut (Mac) | Shortcut (Windows) |
|--------|----------------|-------------------|
| Fit to screen | `F` | `F` |
| Reset view | `R` | `R` |
| Select all | `Cmd+A` | `Ctrl+A` |
| Copy | `Cmd+C` | `Ctrl+C` |
| Paste | `Cmd+V` | `Ctrl+V` |
| Delete | `Delete` | `Delete` |
| Undo | `Cmd+Z` | `Ctrl+Z` |
| Redo | `Cmd+Shift+Z` | `Ctrl+Shift+Z` |
| Find | `Cmd+F` | `Ctrl+F` |
| Toggle minimap | `M` | `M` |

## Troubleshooting

**Problem:** Can't connect two nodes  
**Solution:** Check that output type matches input type. Hover over ports to see schemas.

**Problem:** Flow is too large to see  
**Solution:** Press `F` to fit all, use minimap, or zoom out with mouse wheel.

**Problem:** Edges are crossing and confusing  
**Solution:** Use auto-layout, rearrange nodes manually, or split into sub-Flows.

**Problem:** Don't know what a node does  
**Solution:** Select it, check Properties Panel for description, or ask Triton.

## Related topics

- [Creating and Connecting Nodes](/workspace/canvas/flow-view/create-connect)
- [Understanding I/O Nodes](/workspace/canvas/flow-view/io-nodes)
- [Node Interactions](/workspace/canvas/flow-view/node-interactions)
- [Flow Basics](/workspace/canvas/flow-view/basics)

## Next steps

Now that you understand Flow View:

- Practice building a simple Flow
- Experiment with different patterns (linear, parallel, conditional)
- Use Triton to generate and modify Flows
- Review execution traces to understand behavior




================================================================================
FILE: workspace/canvas/flow-view/basics.mdx
================================================================================

---
title: "Flow View Basics"
description: "Understanding the fundamentals of Flow visualization"
---

## What is Flow View?

Flow View is the detailed canvas that appears when you double-click a Flow node. It shows the internal structure of the Flow: all nodes, connections (edges), and data flow paths.

## Why Flow View matters

- **Visualize logic** — See exactly how data moves through your system
- **Debug easily** — Identify where failures occur
- **Understand complexity** — Grasp multi-step workflows at a glance
- **Edit confidently** — Modify structure without breaking connections

## Anatomy of Flow View

### The Canvas

The main area where nodes and edges appear.

- **Grid background** — Helps align nodes
- **Infinite canvas** — Pan and zoom freely
- **Selection box** — Drag to select multiple nodes

### Node types

**Input Node (green)**
- Entry point for data into the Flow
- Defines expected input schema
- Usually one per Flow (can have multiple for complex scenarios)

**Processing Nodes (blue/purple)**
- Actions: Deterministic Python functions
- Agents: LLM-powered decision-makers
- Sub-Flows: Nested workflows

**Output Node (orange)**
- Exit point for data from the Flow
- Defines output schema
- Can have multiple for different outcomes (success, error, etc.)

**Control Nodes (gray)**
- Router: Conditional branching
- Merge: Combine multiple paths
- Loop: Iterate over items

### Edge types

**Data edges (blue)**
- Solid lines connecting nodes
- Show data flow direction
- Labeled with field names when hovering

**Control edges (orange)**
- Show conditional or loop logic
- Labeled with conditions (e.g., "if status == 'urgent'")

**Inactive edges (gray)**
- Not used in recent executions
- May indicate dead code

## Reading a Flow

### Following the path

Flows generally read **left to right**:

```
Input → Process → Transform → Output
```

### Branching logic

When edges split, it means conditional routing:

```
Input → Check → [if valid] → Process → Output
              → [if invalid] → Error Handler → Output
```

### Parallel operations

When multiple nodes are side-by-side at the same level:

```
Input → Fetch A ↘
            Fetch B → Combine → Output
        Fetch C ↗
```

This means Fetch A, B, and C run simultaneously.

### Sequential operations

When nodes are in a straight line:

```
Input → Step 1 → Step 2 → Step 3 → Output
```

Each step waits for the previous one to complete.

## Interacting with Flow View

### Basic navigation

**Pan:** Click and drag empty space  
**Zoom:** Mouse wheel or trackpad pinch  
**Fit:** Press `F` to see the entire Flow  
**Reset:** Press `R` to reset zoom and position

### Selecting nodes

**Single select:** Click a node  
**Multi-select:** Hold `Shift` and click multiple nodes  
**Box select:** Click and drag to create selection box  
**Select all:** `Cmd+A` (Mac) / `Ctrl+A` (Windows)

### Viewing node details

**Click** a node to see its properties in the Properties Panel:
- Input/output schema
- Configuration
- Execution history
- Logs and errors

**Double-click** a node to drill deeper:
- For Flows: Opens the Flow's internal structure
- For Agents: Shows configuration (in Properties Panel)
- For Actions: Shows code (in Properties Panel)

### Inspecting edges

**Hover** over an edge to see:
- Source node and output port
- Destination node and input port
- Data type being passed
- Field mapping (if any)

**Click** an edge to select it (for deletion or modification).

## Understanding execution visualization

When you select a past execution from Properties → Executions, the Canvas shows what happened:

### Node states

**Green** — Executed successfully  
**Red** — Failed with error  
**Yellow** — Currently executing (live view)  
**Gray** — Skipped (not part of this execution path)

### Edge highlighting

**Bold blue** — Data flowed through this connection  
**Faded gray** — Not used in this execution

### Timing information

Hover over nodes to see:
- Execution start time
- Execution duration
- Wait time (if queued)

## Common Flow patterns

### Pattern 1: Linear pipeline

```
Input → Validate → Process → Format → Output
```

**Use case:** Simple, sequential data processing  
**Benefits:** Easy to understand, predictable  
**Limitations:** Can be slow for large datasets

### Pattern 2: Fan-out / Fan-in

```
Input → Split → Process A ↘
                 Process B → Merge → Output
             Process C ↗
```

**Use case:** Parallel processing of independent tasks  
**Benefits:** Faster, utilizes concurrency  
**Limitations:** Requires merge logic

### Pattern 3: Conditional routing

```
Input → Router → [urgent] → Fast Track → Output
              → [normal] → Standard → Output
              → [low] → Batch → Output
```

**Use case:** Different handling based on conditions  
**Benefits:** Optimized for each scenario  
**Limitations:** More complex to maintain

### Pattern 4: Error handling

```
Input → Process → [success] → Output
              → [error] → Log → Retry → Output (or Dead Letter Queue)
```

**Use case:** Robust systems that handle failures gracefully  
**Benefits:** Reliability, observability  
**Limitations:** Additional nodes and complexity

## Flow complexity indicators

### Simple Flow
- 3-7 nodes
- Linear or single branch
- Easy to understand at a glance

### Moderate Flow
- 8-15 nodes
- Multiple branches or parallel paths
- Some conditional logic

### Complex Flow
- 16+ nodes
- Nested conditions
- Multiple parallel paths
- Consider splitting into sub-Flows

## Best practices

> **Keep it visual** — Arrange nodes clearly, use space wisely

> **Name nodes descriptively** — "Validate Email" beats "Action_1"

> **Use sub-Flows** — Break complex logic into manageable pieces

> **Test incrementally** — Verify each section works before adding more

> **Document non-obvious logic** — Add comments for future you

## Next steps

Continue exploring the Flow View documentation to learn about creating and connecting nodes, understanding I/O nodes, and node interactions.


================================================================================
FILE: workspace/canvas/flow-view/create-connect.mdx
================================================================================

---
title: "Creating and Connecting Nodes"
description: "Build Flows by adding and wiring nodes together"
---

## Overview

Creating nodes and connecting them is the core of building Flows. This guide covers all the ways to add nodes and create connections between them.

## Creating nodes

### Method 1: Ask Triton

The easiest way to add nodes is through conversation.

**Examples:**
- _"Add a validation Action to this Flow"_
- _"Insert a sentiment analysis Agent after the input"_
- _"Add three Actions: fetch_data, process_data, and store_results"_

**Benefits:**
- Triton creates the node
- Suggests appropriate connections
- Configures basic settings
- Maintains Flow structure

### Method 2: Right-click menu

For manual control:

1. **Right-click** on empty Canvas space
2. Select **Add Node**
3. Choose node type:
   - **Action** — Select from your existing Actions or create new
   - **Agent** — Select from existing Agents or create new
   - **Flow** — Add a sub-Flow
   - **Control** — Router, Merge, Loop
4. Click to place on Canvas

### Method 3: Drag from Agent Toolbox

For visual workflows:

1. Open **Agent Toolbox** panel (right side)
2. Browse or search for component
3. **Drag** onto Canvas
4. **Drop** where you want it

**Tip:** Recently used components appear at the top for quick access.

### Method 4: Copy and paste

To duplicate existing nodes:

1. **Select** the node(s) you want to copy
2. **Copy:** `Cmd+C` (Mac) / `Ctrl+C` (Windows)
3. **Paste:** `Cmd+V` (Mac) / `Ctrl+V` (Windows)
4. The duplicate appears offset slightly

**Use case:** Creating parallel processing paths with identical Actions.

### Method 5: Quick add (keyboard)

For power users:

1. Press **`A`** (Add) or **`/`** (Command)
2. Type to search for component
3. Arrow keys to select
4. `Enter` to add to Canvas at current view center

## Connecting nodes

### Method 1: Click and drag

The standard way to create edges:

1. **Hover** over the source node's output port (right side)
2. **Click and hold** on the port
3. **Drag** toward the destination node
4. **Drop** on the destination's input port (left side)
5. Edge appears, connecting the nodes

**Visual feedback:**
- Compatible ports **highlight green** when hovering
- Incompatible ports **highlight red**
- Edge follows cursor while dragging

### Method 2: Select and connect

For precision connections:

1. **Select** the source node
2. **Right-click** → **Connect to...**
3. Choose the destination node from list
4. Select which output port → which input port
5. Click **Connect**

**Use case:** When Canvas is zoomed out or nodes are far apart.

### Method 3: Ask Triton

Let AI handle connections:

**Examples:**
- _"Connect validate_input to process_data"_
- _"Wire the output of fetch_user to the input of notify_user"_
- _"Connect all three fetch Actions to the merge node"_

**Benefits:**
- Automatic port mapping
- Handles type conversions if needed
- Maintains data flow logic

### Method 4: Auto-connect

When adding a node between existing ones:

1. **Drag** a new node onto an existing edge
2. **Drop** it on the edge line
3. The edge automatically splits and connects through the new node

**Result:**
```
Before: A → C
After:  A → B → C (B is the new node)
```

## Connection validation

Triform validates connections to prevent errors.

### Compatible connections

Connections work when:
- **Type match:** Output type equals input type
- **Schema compatible:** Fields align (or can be mapped)
- **No cycles:** Would not create an infinite loop (unless intentional)

### Incompatible connections

Connections fail when:
- **Type mismatch:** String output → Number input
- **Missing required fields:** Output lacks required input fields
- **Cycle detected:** Would create unintended loop

**What happens:**
- Port highlights **red** when hovering
- Cannot drop to create connection
- Error message explains why

### Type conversion

Sometimes Triton can add automatic conversion:

**Example:**
- Source outputs: `{ "value": 123 }`
- Destination expects: `{ "value": "string" }`
- Triton offers to add a type conversion step

## Mapping fields

When schemas don't match exactly, you need field mapping.

### Automatic mapping

Triton maps fields automatically when names match:

**Source output:**
```json
{
  "user_id": "123",
  "user_name": "Alice"
}
```

**Destination input:**
```json
{
  "user_id": "string",
  "name": "string"
}
```

**Triton maps:**
- `user_id` → `user_id` ✓ (exact match)
- `user_name` → `name` (no match, needs manual mapping)

### Manual mapping

When auto-mapping isn't enough:

1. **Click** the edge
2. Properties Panel shows **Field Mapping**
3. Map each destination field:
   - Select source field from dropdown
   - Or write a transformation expression
4. Click **Save**

**Example transformations:**
- `user_name` → `name` (simple rename)
- `first_name + " " + last_name` → `full_name` (combine fields)
- `price * 1.1` → `price_with_tax` (calculation)

## Disconnecting nodes

### Remove a single connection

1. **Click** the edge to select it
2. **Delete:** Press `Delete` or `Backspace`
3. Or **right-click** edge → **Delete**

### Remove all connections from a node

1. **Select** the node
2. **Right-click** → **Disconnect All**
3. Confirm

**Use case:** Quickly removing a node without deleting it.

### Remove node and reconnect

To remove a node but keep the Flow intact:

1. **Select** the node
2. **Right-click** → **Delete and Bridge**
3. The node is removed and edges reconnect around it

**Example:**
```
Before: A → B → C
After:  A → C (B deleted, A now connects directly to C)
```

## Advanced connection techniques

### Conditional connections

Create edges that activate based on conditions:

1. Connect nodes normally
2. **Click** the edge
3. Properties Panel → **Add Condition**
4. Define condition: `status == "approved"`
5. Edge now labeled with condition

**Use case:** Routing different data through different paths.

### Named output ports

Some nodes have multiple output ports:

**Example:**
```
Process Node
├── success (output port)
├── error (output port)
└── warning (output port)
```

Connect each port to appropriate downstream nodes:
- `success` → Continue normal flow
- `error` → Error handling
- `warning` → Log and continue

### Multiple inputs

Some nodes accept multiple inputs:

**Merge node:**
```
Source A ↘
Source B → Merge → Destination
Source C ↗
```

The Merge node combines all inputs before passing to destination.

**Configuration:**
- **Wait for all:** Waits for A, B, and C before proceeding
- **First wins:** Proceeds as soon as any input arrives
- **Custom logic:** Combine inputs with a function

## Connection best practices

> **Connect left to right** — Makes flow easy to follow

> **Avoid crossing edges** — Rearrange nodes to minimize crossings

> **Use named ports** — Clarifies purpose of each connection

> **Validate schemas** — Ensure type compatibility before connecting

> **Test connections** — Run executions to verify data flows correctly

## Common connection patterns

### Pattern 1: Linear connection

```
A → B → C → D
```

Simple, sequential processing.

### Pattern 2: Fan-out

```
A → B ↘
      C
  → D ↗
```

One source, multiple destinations (broadcast).

### Pattern 3: Fan-in

```
A ↘
B → D
C ↗
```

Multiple sources, one destination (merge).

### Pattern 4: Error handling

```
A → [success] → B
  → [error] → Error Handler
```

Different paths for success and failure.

### Pattern 5: Loop

```
Input → Process → [not done] → Process (loop back)
                → [done] → Output
```

Iteration over items or until condition met.

## Troubleshooting connections

### Problem: Can't connect two nodes

**Possible causes:**
- Type mismatch between ports
- Schema incompatibility
- Would create invalid cycle

**Solutions:**
- Check output/input types (hover over ports)
- Add transformation node between them
- Ask Triton: _"Why can't I connect A to B?"_

### Problem: Connection exists but data doesn't flow

**Possible causes:**
- Condition on edge isn't met
- Upstream node failed
- Field mapping incorrect

**Solutions:**
- Check execution trace
- Verify edge condition
- Review field mapping

### Problem: Too many crossing edges

**Solution:**
- Rearrange nodes to reduce crossings
- Use auto-layout: Right-click → Auto Layout
- Split into sub-Flows

## Next steps

Continue exploring the Flow View documentation to learn about I/O nodes, node interactions, and flow view basics. Use Triton to help you build Flows efficiently.


================================================================================
FILE: workspace/canvas/flow-view/io-nodes.mdx
================================================================================

---
title: "Input and Output Nodes"
description: "Define Flow interfaces with I/O nodes"
---

## Overview

Every Flow has **Input** and **Output** nodes that define its interface—what data it accepts and what it returns. These nodes are the contract between your Flow and the outside world.

## Input Nodes

The Input node is the entry point for data into your Flow.

### Structure

**Green node** on the left side of the Canvas with:
- **Name:** Usually "Input" (can be customized)
- **Schema:** Defines expected data structure
- **Output ports:** Send data to downstream nodes

### Defining the input schema

1. **Select** the Input node
2. Go to **Properties → Input/Output**
3. Define fields:
   - Field name
   - Data type
   - Required vs. optional
   - Default value
   - Description

**Example schema:**
```json
{
  "user_id": {
    "type": "string",
    "required": true,
    "description": "Unique identifier for the user"
  },
  "action_type": {
    "type": "enum",
    "options": ["create", "update", "delete"],
    "required": true,
    "description": "Type of action to perform"
  },
  "data": {
    "type": "object",
    "required": false,
    "description": "Additional data for the action"
  },
  "priority": {
    "type": "enum",
    "options": ["low", "medium", "high"],
    "required": false,
    "default": "medium",
    "description": "Processing priority"
  }
}
```

### Data types

Supported types:

| Type | Description | Example |
|------|-------------|---------|
| `string` | Text | `"Hello"` |
| `number` | Integer or float | `42`, `3.14` |
| `boolean` | True or false | `true`, `false` |
| `array` | List of items | `[1, 2, 3]` |
| `object` | Nested structure | `{"key": "value"}` |
| `enum` | Limited options | `"small"` from `["small", "medium", "large"]` |
| `any` | Any type | Flexible but less safe |

### Required vs. optional fields

**Required fields:**
- Must be present in input
- Execution fails if missing
- Use for essential data

**Optional fields:**
- Can be omitted
- Can have default values
- Use for configuration or preferences

### Default values

Set defaults for optional fields:

```json
{
  "timeout": {
    "type": "number",
    "required": false,
    "default": 30,
    "description": "Timeout in seconds"
  },
  "retry": {
    "type": "boolean",
    "required": false,
    "default": true,
    "description": "Whether to retry on failure"
  }
}
```

**Benefits:**
- Simpler API for consumers
- Backward compatibility when adding new fields
- Clear default behavior

### Validation rules

Add validation beyond just types:

**String validation:**
- Min/max length
- Regex pattern
- Format (email, URL, UUID, etc.)

**Number validation:**
- Min/max value
- Integer only
- Positive only

**Array validation:**
- Min/max items
- Unique items only
- Item type

**Example:**
```json
{
  "email": {
    "type": "string",
    "required": true,
    "format": "email",
    "description": "User's email address"
  },
  "age": {
    "type": "number",
    "required": true,
    "min": 0,
    "max": 150,
    "description": "User's age"
  },
  "tags": {
    "type": "array",
    "items": "string",
    "unique": true,
    "max_items": 10,
    "description": "User tags"
  }
}
```

### Multiple Input nodes

Most Flows have one Input node, but you can have multiple for different entry points:

**Use cases:**
- Different trigger types (webhook, schedule, manual)
- Different input formats (JSON, form data, file upload)
- Different access levels (admin vs. user)

**Example:**
```
Webhook Input → Validate → Process → Output
Schedule Input ↗              ↓
                         Store → Output
```

## Output Nodes

The Output node is the exit point for data from your Flow.

### Structure

**Orange node** on the right side of the Canvas with:
- **Name:** Usually "Output" (can be customized)
- **Schema:** Defines returned data structure
- **Input ports:** Receive data from upstream nodes

### Defining the output schema

1. **Select** the Output node
2. Go to **Properties → Input/Output**
3. Define fields (same process as Input)

**Example schema:**
```json
{
  "status": {
    "type": "enum",
    "options": ["success", "partial", "failed"],
    "required": true,
    "description": "Execution status"
  },
  "result": {
    "type": "object",
    "required": false,
    "description": "Result data if successful"
  },
  "error": {
    "type": "object",
    "required": false,
    "description": "Error details if failed"
  },
  "execution_time_ms": {
    "type": "number",
    "required": true,
    "description": "Time taken in milliseconds"
  }
}
```

### Output mapping

Map internal data to output schema:

**Scenario:**
- Processing returns `{ "processed_data": [...], "count": 10 }`
- Output expects `{ "result": object, "item_count": number }`

**Mapping:**
1. Click connection to Output node
2. Properties → Field Mapping
3. Map:
   - `processed_data` → `result`
   - `count` → `item_count`

### Multiple Output nodes

Flows can have multiple outputs for different outcomes:

**Common pattern:**
```
Process → [success] → Success Output
        → [error] → Error Output
```

**Success Output schema:**
```json
{
  "status": "success",
  "data": "object",
  "message": "string"
}
```

**Error Output schema:**
```json
{
  "status": "error",
  "error_code": "string",
  "error_message": "string",
  "details": "object"
}
```

**Benefits:**
- Type-safe error handling
- Clear success vs. failure contracts
- Easier debugging

### Partial outputs

Some Flows stream or emit data progressively:

**Use case:** Processing a large list

```
Input (100 items)
  ↓
Process (batches of 10)
  ↓ (after each batch)
Partial Output (progress update)
  ↓ (after all batches)
Final Output (complete results)
```

**Configuration:**
- Enable "Allow partial outputs" in Output node settings
- Define partial output schema
- Connect processing node to Output with "partial" flag

## I/O best practices

> **Be explicit** — Clear schemas prevent integration errors

> **Validate early** — Check input immediately after Input node

> **Document fields** — Descriptions help users understand the API

> **Version schemas** — When changing, maintain backward compatibility

> **Use specific types** — Avoid `any` unless truly necessary

> **Provide examples** — Sample input/output helps users

## Common I/O patterns

### Pattern 1: Simple transformation

**Input:**
```json
{ "text": "string" }
```

**Output:**
```json
{ "result": "string" }
```

Clean, focused interface.

### Pattern 2: Batch processing

**Input:**
```json
{ "items": ["array of objects"] }
```

**Output:**
```json
{
  "processed": "number",
  "failed": "number",
  "results": ["array of objects"]
}
```

Clear accounting of what happened.

### Pattern 3: Configuration with data

**Input:**
```json
{
  "data": "object",
  "config": {
    "format": "enum",
    "validate": "boolean",
    "timeout": "number"
  }
}
```

**Output:**
```json
{
  "data": "object",
  "metadata": {
    "processing_time": "number",
    "validation_passed": "boolean"
  }
}
```

Flexible configuration with results and metadata.

### Pattern 4: Status + details

**Output:**
```json
{
  "status": "enum: ['success', 'failed']",
  "message": "string (human-readable)",
  "data": "object (if success)",
  "error": "object (if failed)"
}
```

Consistent structure for both outcomes.

## Testing I/O

### Test Input

1. Select the Flow (or Input node)
2. Go to Properties → Execute
3. Provide test payload matching input schema
4. Click Execute
5. View output

**Example test payload:**
```json
{
  "user_id": "test_123",
  "action_type": "create",
  "data": {
    "name": "Test User",
    "email": "test@example.com"
  },
  "priority": "high"
}
```

### Validate schema

Triform validates input against schema automatically:

**Valid input:** Execution proceeds  
**Invalid input:** Execution fails with validation error

**Common validation errors:**
- Missing required field
- Wrong type (string instead of number)
- Value out of range
- Invalid enum value
- Malformed email/URL

### Edge case testing

Test with:
- **Empty input:** `{}` or `[]`
- **Minimal input:** Only required fields
- **Maximum input:** All optional fields, large values
- **Invalid input:** Wrong types, missing fields
- **Boundary values:** Min/max numbers, empty strings

## Documenting I/O

### In Triform

- Add descriptions to each field
- Provide examples in the Input/Output panel
- Use clear, consistent naming

### For external users

When exposing as API:

1. Generate docs from schema (Triform can do this)
2. Include:
   - Field descriptions
   - Example request/response
   - Error codes and meanings
   - Rate limits and quotas

**Example doc:**
```markdown
## POST /v1/process-user

Process user action.

### Request Body

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| user_id | string | Yes | User identifier |
| action_type | enum | Yes | One of: create, update, delete |
| data | object | No | Additional data |
| priority | enum | No | One of: low, medium, high (default: medium) |

### Example Request

\```json
{
  "user_id": "user_123",
  "action_type": "create",
  "data": {"name": "Alice"},
  "priority": "high"
}
\```

### Response

| Field | Type | Description |
|-------|------|-------------|
| status | enum | success, partial, or failed |
| result | object | Result data (if successful) |
| error | object | Error details (if failed) |

### Example Response (Success)

\```json
{
  "status": "success",
  "result": {"id": "new_123"},
  "execution_time_ms": 250
}
\```
```

## Next steps

Continue exploring the documentation to learn about creating and connecting nodes, node interactions, understanding Flows, and building with Triton.


================================================================================
FILE: workspace/canvas/flow-view/node-interactions.mdx
================================================================================

---
title: "Node Interactions"
description: "Select, move, edit, and manage nodes in Flow View"
---

## Overview

Nodes are the building blocks of Flows. This guide covers how to interact with them: selection, movement, editing, configuration, and deletion.

## Selecting nodes

### Single selection

**Click** a node to select it.

**Visual feedback:**
- Node highlights with border
- Properties Panel updates to show node details
- Output/input ports become visible

### Multiple selection

**Method 1: Shift-click**
1. Click first node
2. Hold `Shift`
3. Click additional nodes
4. All selected nodes highlight

**Method 2: Box select**
1. Click and drag on empty Canvas space
2. Drag to create selection box
3. All nodes inside box are selected
4. Release to complete selection

**Method 3: Select all**
- `Cmd+A` (Mac) / `Ctrl+A` (Windows) — Select all nodes
- Useful for bulk operations

### Deselecting

- **Click empty Canvas** — Deselects everything
- **Cmd+click** (Mac) / **Ctrl+click** (Windows) — Toggle individual node in/out of selection

## Moving nodes

### Basic movement

**Click and drag** a node to reposition it.

**Tips:**
- Grid snapping helps align nodes
- Edges automatically update as you move
- Other selected nodes move together

### Precise movement

**Arrow keys** — Move selected node(s) by small increments (1-5 pixels)

**Shift + Arrow keys** — Move by larger increments (10-20 pixels)

**Use case:** Fine-tuning alignment after rough positioning

### Constrained movement

**Hold Shift while dragging** — Constrains movement to horizontal or vertical axis

**Use case:** Keeping nodes aligned in rows or columns

### Moving multiple nodes

1. **Select multiple nodes** (Shift-click or box select)
2. **Drag any selected node**
3. All selected nodes move together, maintaining relative positions

## Editing nodes

### Accessing node properties

**Click** a node → Properties Panel shows:
- **Content tab** — Code, prompts, configuration
- **Input/Output tab** — Schema and field definitions
- **Info tab** — Name, description, metadata
- **Executions tab** — History of runs

### Editing Action nodes

1. Select the Action node
2. Properties → Content tab
3. Edit `Action.py` code directly
4. Update `requirements.txt` if needed
5. Changes auto-save

**Common edits:**
- Add logging
- Improve error handling
- Optimize performance
- Update logic

### Editing Agent nodes

1. Select the Agent node
2. Properties → Content tab
3. Modify:
   - **System Prompt** — Agent instructions
   - **Model** — GPT-4, Claude, etc.
   - **Temperature** — Randomness (0-1)
   - **Max Tokens** — Response length
   - **Tools** — Available Actions/Flows
4. Changes auto-save

**Common edits:**
- Refine prompts for better results
- Add/remove tools
- Adjust temperature for consistency/creativity
- Switch models for cost/performance

### Editing Flow nodes (sub-Flows)

1. Select the Flow node
2. **Double-click** to open its internal structure
3. Edit as you would any Flow (add/remove/connect nodes)
4. Use breadcrumbs to return to parent Flow

### Editing Control nodes

**Router nodes:**
- Define conditions for each output branch
- Set default path if no condition matches

**Merge nodes:**
- Choose merge strategy (wait for all, first wins, custom)
- Configure field mapping if combining data

**Loop nodes:**
- Set iteration logic (for each item, until condition, etc.)
- Define max iterations to prevent infinite loops

## Configuring nodes

### Renaming nodes

**Method 1:**
1. Select node
2. Properties → Info tab
3. Edit "Name" field
4. Press Enter

**Method 2 (if supported):**
1. Double-click node label on Canvas
2. Type new name
3. Press Enter or click away

**Best practices:**
- Use descriptive names: `Validate_Email` not `Action_1`
- Follow naming convention: `verb_noun` format
- Be consistent across your Project

### Adding descriptions

1. Select node
2. Properties → Info tab
3. Add description: what it does, why it exists, any gotchas

**Benefits:**
- Helps teammates understand
- Aids future you
- Useful for documentation generation

### Node settings

Each node type has specific settings:

**Actions:**
- Timeout (seconds)
- Retry count
- Error handling behavior

**Agents:**
- Model and parameters
- Tool permissions
- Response format

**Flows:**
- Execution mode (sequential, parallel)
- Error propagation

**Access:** Select node → Properties → Settings

## Copying and pasting nodes

### Copy/paste within a Flow

1. **Select** node(s)
2. **Copy:** `Cmd+C` (Mac) / `Ctrl+C` (Windows)
3. **Paste:** `Cmd+V` (Mac) / `Ctrl+V` (Windows)
4. Duplicate appears with offset

**Note:** Edges are NOT copied—you must reconnect manually

### Copy across Flows

1. Select node(s) in source Flow
2. Copy (`Cmd+C` / `Ctrl+C`)
3. Open destination Flow
4. Paste (`Cmd+V` / `Ctrl+V`)
5. Reconnect as needed

**Use case:** Reusing common patterns (validation, logging, etc.)

## Duplicating nodes

**Right-click** node → **Duplicate**

Creates a copy with "_copy" appended to the name.

**Use case:** Creating parallel processing paths with same logic

## Deleting nodes

### Delete single node

1. **Select** the node
2. **Delete:** Press `Delete` or `Backspace`
3. Or **right-click** → **Delete**
4. Confirm if prompted

**Result:** Node and its connections are removed

### Delete multiple nodes

1. **Select multiple nodes**
2. Press `Delete` or `Backspace`
3. Confirm

All selected nodes and their connections are removed.

### Delete and bridge

Remove a node but maintain flow:

1. **Select** the node
2. **Right-click** → **Delete and Bridge**
3. Node is removed, incoming and outgoing edges reconnect around it

**Example:**
```
Before: A → B → C
After:  A → C
```

**Use case:** Removing obsolete preprocessing/formatting steps

## Node context menu

**Right-click** a node for quick actions:

- **Edit** — Open editor (same as double-click)
- **Duplicate** — Create copy
- **Delete** — Remove node
- **Delete and Bridge** — Remove and reconnect edges
- **Disconnect All** — Remove all edges but keep node
- **Copy** — Copy to clipboard
- **Find Usages** — See where this component is used
- **Export** — Export as JSON
- **View Executions** — See execution history for this node
- **Add to Favorites** — Quick access for reuse

## Grouping nodes

### Visual grouping

Organize related nodes:

1. **Select multiple nodes**
2. **Right-click** → **Group**
3. Choose: Add to new group
4. Name the group (e.g., "Validation Logic")
5. A visual box appears around grouped nodes

**Benefits:**
- Clarifies structure
- Collapsible for cleaner Canvas
- Can move group as unit

### Ungrouping

1. **Click** the group header
2. **Right-click** → **Ungroup**
3. Nodes remain but visual grouping is removed

## Node states

Nodes display different visual states:

### Design time (editing)

- **Default** — Gray/blue, ready to configure
- **Selected** — Highlighted border
- **Hovered** — Slight highlight, ports visible
- **Error** — Red indicator (configuration error)

### Execution time

- **Pending** — Gray, waiting to run
- **Running** — Yellow, currently executing
- **Success** — Green, completed successfully
- **Failed** — Red, error occurred
- **Skipped** — Faded, not part of execution path

## Advanced interactions

### Node metadata

View detailed info:

1. Select node
2. Properties → Info tab → Metadata section

Shows:
- Created date
- Last modified
- Creator
- Version
- Dependencies
- Usage count

### Node performance

Analyze performance:

1. Select node
2. Properties → Executions tab
3. View:
   - Average execution time
   - Min/max time
   - Success rate
   - Error types

**Use case:** Identifying bottlenecks

### Node search within Flow

Large Flows can have dozens of nodes:

1. Press `Cmd+F` (Mac) / `Ctrl+F` (Windows)
2. Type node name
3. Canvas highlights matching nodes
4. Arrow keys to jump between matches

## Best practices

> **Name nodes clearly** — Future you will thank present you

> **Group related logic** — Visual organization aids understanding

> **Delete unused nodes** — Clutter confuses

> **Use descriptions** — Document non-obvious decisions

> **Test nodes individually** — Verify each works before connecting

> **Align nodes neatly** — Use grid snapping and arrow keys

## Keyboard shortcuts

| Action | Mac | Windows |
|--------|-----|---------|
| Select all | `Cmd+A` | `Ctrl+A` |
| Copy | `Cmd+C` | `Ctrl+C` |
| Paste | `Cmd+V` | `Ctrl+V` |
| Delete | `Delete` / `Backspace` | `Delete` / `Backspace` |
| Duplicate | `Cmd+D` | `Ctrl+D` |
| Find | `Cmd+F` | `Ctrl+F` |
| Move node | Arrow keys | Arrow keys |
| Move node (large) | `Shift` + Arrow keys | `Shift` + Arrow keys |
| Undo | `Cmd+Z` | `Ctrl+Z` |
| Redo | `Cmd+Shift+Z` | `Ctrl+Shift+Z` |

## Troubleshooting

**Problem:** Can't select a node  
**Solution:** Check if it's in a locked group, or Canvas is in view-only mode

**Problem:** Node won't move  
**Solution:** Ensure it's selected, not locked, and you're dragging the node (not a port)

**Problem:** Deleted node by accident  
**Solution:** `Cmd+Z` / `Ctrl+Z` to undo

**Problem:** Can't find a specific node  
**Solution:** Use Find (`Cmd+F` / `Ctrl+F`) to search by name

## Next steps

Continue exploring the documentation to learn about creating and connecting nodes, understanding I/O nodes, flow view basics, and editing with Triton.


================================================================================
FILE: workspace/canvas/overview.mdx
================================================================================

---
title: "Canvas: Overview"
description: "Compose systems visually with Nodes and Edges"
---

## What you see

- Nodes: visual building blocks (Agents, Flows, Actions, Projects).
- Edges: connections that define data & control flow.

## Core interactions

- Select: click a Node.
- Open: double‑click to drill into Projects, Agents, or Flows.
- Context menu: right‑click a Node or the Canvas for actions.
- Rearrange: drag Nodes to clarify structure.

## Why Canvas

- Makes complex logic legible at a glance.
- Encourages modular, reusable patterns.
- Enables debugging without diving into raw logs.

> note: Where execution happens — You run Nodes from Properties → Execute. The Canvas reflects structure; the Properties panel manages configuration and runs.

## Best practices

- Name Nodes clearly (e.g., `Normalize Input`, `Route by Type`).
- Group related Nodes visually.
- Keep flows shallow where possible; compose via sub‑Flows.

================================================================================
FILE: workspace/canvas/project-view.mdx
================================================================================

---
title: "Project View"
description: "Top‑level components overview and common actions"
---

## Key points

- Items are not connected here; it’s an overview/list.
- Only top‑level Agents/Flows can be exposed via API or Schedule.
- Top‑level components can contain other components.

## Common actions

- Open a top‑level Agent/Flow to work inside it.
- Create a new Agent or Flow from the Canvas.
- Rename and organize to reflect your domain.

> tip: Design for exposure — If you plan to expose something as an API or Schedule, keep it top‑level and give it a stable name early.

================================================================================
FILE: workspace/chat-panel.mdx
================================================================================

---
title: "Chat Panel (Triton)"
description: "Conversational interface to author, wire, run, and explain"
---

## What you can do

- Author components ("Create a Flow named ingest with input `text`").
- Wire connections ("Connect `Input.text` to `normalize.input`").
- Run executions ("Run the Flow with `{ \"text\": \"hello\" }`").
- Explain structures ("What does this Flow do?").

## @‑context

- Type `@` to reference a component (e.g., `@summarize_text`).
- Triton focuses actions and answers on what you mention.

## Activity updates

- Triton posts status messages when it performs changes or runs tasks.

> tip: Be explicit — Include component names and ports in your messages to avoid ambiguity and speed up Triton’s actions.

================================================================================
FILE: workspace/properties/components/content/actions.mdx
================================================================================

---
title: "Properties → Components → Content → Actions"
description: "Action structure, guidelines, and testing"
---

## Structure

- Action.py — the Python implementation.
- README — technical documentation of purpose, I/O, examples.
- requirements.txt — Python dependencies (pure‑Python only; no binary installs in the image).

## Guidelines

- Keep Actions small and single‑purpose.
- Define a clear input → output contract.
- Pin dependencies conservatively in `requirements.txt`.

## Testing

- Use Execute with small payloads to validate behavior in isolation.
- Save payloads as examples for reproducibility.

> warning: Dependencies — Only dependencies that do not require binary installations are supported in the build image.

================================================================================
FILE: workspace/properties/components/content/agents.mdx
================================================================================

---
title: "Properties → Components → Content → Agents"
description: "Model config, prompts, tools, and best practices"
---

## Model configuration

- Model selection — choose from curated providers or Triform‑hosted OSS models.
- Prompts
  1. System Prompt — non‑negotiable instructions that define Agent behavior.
  2. Messages List (toggle) — allow messages as input/output to pass chat history.
  3. User Prompt — optional pre‑set user instruction for fixed‑purpose Agents.
- Advanced settings (enable as needed)
  - Temperature
  - Top‑p
  - Max tokens

## Tools

- Add Actions, Flows, or Agents to the Agent’s Toolbox (see Canvas → Agent Toolbox View).
- Tools are callable by the Agent during execution.

## Best practices

- Keep the System Prompt specific and outcome‑oriented.
- Limit tools to the fewest needed to achieve goals.
- Use Messages List when context/history matters.

> tip: Iterate with payloads — Save example payloads and run the Agent from Execute to tune prompts and settings quickly.

================================================================================
FILE: workspace/properties/components/content/flows.mdx
================================================================================

---
title: "Properties → Components → Content → Flows"
description: "Arrange sub‑components and configure flow behavior"
---

## Configure behavior

- Arrange sub‑components (Actions/Agents/Flows) on the Canvas.
- Use Input/Output ports to define data entering and leaving the Flow.
- Optionally design parallel paths (see Input & Output → Parallel).

## When to use a Flow

- Multistep transformations
- Branching logic
- Orchestration of tool calls

> note: Composability — A Flow can be used as a Node inside another Flow or as a tool for an Agent.

================================================================================
FILE: workspace/properties/components/content/projects.mdx
================================================================================

---
title: "Properties → Components → Content → Projects"
description: "Top‑level surfaces and exposure guidance"
---

## What they contain

- Top‑level Nodes (Agents or Flows) that may contain other components.
- Configuration that applies to the Project scope (e.g., variables).

## Exposure

- Only top‑level Agents/Flows can be exposed as APIs or Schedules (configured under Execute).

> tip: Model your domain — Map your domain’s entry points (API endpoints, schedules) to top‑level Nodes and keep deeper logic inside sub‑Flows or Actions.

================================================================================
FILE: workspace/properties/components/definition.mdx
================================================================================

---
title: "Properties → Components → Definition & Information"
description: "Capture description, requirements, and constraints"
---

## What to include

- Description — what the component does and why it exists.
- Requirements — inputs expected, dependencies, or assumptions.
- Constraints — limits, known edge cases, or quotas to consider.

## Why it matters

- Creates shared understanding across the team.
- Speeds up onboarding and troubleshooting.
- Serves as living documentation close to the code.

> tip: Write for the next person — Assume the reader has context on Triform but not on your domain. Short, precise, and example‑driven descriptions work best.

================================================================================
FILE: workspace/properties/execute.mdx
================================================================================

---
title: "Properties → Execute"
description: "Run components and manage exposure and API keys"
---

## Run a component

1. Select a Node (Project, Flow, Agent, or Action).
2. Open Execute.
3. Edit the JSON payload (starting from the default).
4. Click Run.

## Project‑level exposure

- APIs/Schedules: choose which top‑level components to expose.
- API key: create and manage keys for access (Project scope).

## Tips

- Keep small, realistic payload examples for quick regression checks.
- Use parallel inputs thoughtfully (see Input & Output).

> note: Traceability — Executions emit logs, traces, and metrics that you can inspect later to understand runtime behavior.

================================================================================
FILE: workspace/properties/input-output.mdx
================================================================================

---
title: "Properties → Input & Output"
description: "Define ports, types, defaults, and parallelism"
---

## Controls

- Parallel (toggle) — when ON, the component expects a list of inputs and executes each in parallel.
- Inputs/Outputs — add or remove ports (unless they’re fixed).
- Typing — annotate the expected type for each I/O.
- Defaults — set default input values used at execution time.

## Usage examples

- Add input `text: string` with default `"hello"`.
- Add output `summary: string`.
- Toggle Parallel ON to process an array of `text` items concurrently.

> tip: Keep contracts stable — Changing I/O shapes breaks downstream edges. Stabilize names and types early to reduce refactors.

================================================================================
FILE: workspace/properties/overview.mdx
================================================================================

---
title: "Properties Panel: Overview"
description: "Configure and run components; sections and context awareness"
---

## Sections

- Global — Organization/Project‑level settings available to all components in the Project (e.g., Variables).
- Components — Definition/Information and Content for the selected Node.
- Execute — Run the selected Node with a JSON payload.
- Input & Output — Define inputs/outputs, types, defaults, and parallel execution behavior.

> note: Context aware — The Properties Panel shows only the sections relevant to your current selection (Project, Flow, Agent, or Action).

================================================================================
FILE: workspace/properties/project-variables.mdx
================================================================================

---
title: "Properties → Global Variables"
description: "Define and reference Project‑scoped configuration"
---

## What to store

- Configuration values (e.g., feature flags, dataset names).
- Non‑secret constants used across Actions and Flows.

## Referencing variables

- Use variables in your Actions and configuration to avoid duplicating values.

## Tips

- Prefer descriptive names (e.g., `default_temperature`, `region`).
- Scope values per Project to keep experiments isolated.

> warning: Secrets — Store sensitive credentials in your secure mechanism of choice. Avoid committing secrets directly as plain variables.

================================================================================
FILE: workspace/top-bar.mdx
================================================================================

---
title: "Top Bar"
description: "Navigation breadcrumbs, profile, and deploy controls"
---

## Breadcrumbs

- Always shows your current context (e.g., Org / Project / Flow).
- Click any crumb to jump directly to that level.

## Home

- Click the Triform logo to go to the Projects dashboard.

## Profile Menu

- Organization — switch active Org or open Organizations overview.
- Account — account overview (coming soon).
- Log out — end session and clear tokens.

## Deploy button

- Appears when local changes are ahead of the last deployed state.
- Click to deploy the latest changes for your current Project.

> tip: When in doubt, use breadcrumbs — fastest way to move between Project view, Flow view, and back to the Org without losing your place.

================================================================================
FILE: concepts/actions.mdx
================================================================================

---
title: "Actions"
description: "Atomic Python units: structure, contracts, workflow"
---

## Structure

- Action.py — Implementation.
- README — Purpose, I/O, examples.
- requirements.txt — Pure‑Python dependencies (no binary installs).

## Contract

- Inputs — Typed, minimal, documented.
- Outputs — Deterministic structure; document edge cases.

## Workflow

1. Define input/output and write `Action.py`.
2. Pin dependencies in `requirements.txt`.
3. Test with small Payloads via Execute.
4. Reuse across Flows and as Agent tools.

> warning: Dependencies — Only packages that do not require binary installs in the build image are supported.

================================================================================
FILE: concepts/agents.mdx
================================================================================

---
title: "Agents"
description: "LLM logic with prompts and tools; good patterns and observability"
---

## Anatomy

- Prompts
  - System Prompt — Non‑negotiable instructions.
  - Messages List (toggle) — Pass conversation history as input/output.
  - User Prompt — Optional pre‑set prompt for fixed‑purpose Agents.
- Model configuration — Provider/model, temperature, top‑p, max tokens.
- Toolbox — The set of callable tools made available to the Agent.

## Good patterns

- Keep System Prompts specific and outcome‑oriented.
- Start with few high‑leverage tools; expand gradually.
- Use Messages List when memory/context matters.

## Observability

- Executions emit logs, traces, metrics for inspection.
- Use Payloads to reproduce and compare runs after prompt changes.

> tip: Tool hygiene — Name tools by verb + object (`fetch_user`, `summarize_text`). Small, predictable tools make plans more reliable.

================================================================================
FILE: concepts/dependencies.mdx
================================================================================

---
title: "Dependencies"
description: "Manage Python packages and dependencies for Actions"
---

## Overview

Actions in Triform run Python code, and often need external packages (requests, pandas, etc.). Requirements define which packages your Actions depend on and which versions to use.

## requirements.txt

Each Action can have a `requirements.txt` file listing its Python dependencies.

### Format

Standard pip requirements format:

```txt
requests==2.31.0
pandas>=1.5.0,<2.0.0
openai~=1.0.0
python-dateutil
```

### Version specifiers

| Specifier | Meaning | Example |
|-----------|---------|---------|
| `==` | Exact version | `requests==2.31.0` |
| `>=` | Minimum version | `pandas>=1.5.0` |
| `<=` | Maximum version | `numpy<=1.24.0` |
| `>` | Greater than | `python>3.8` |
| `<` | Less than | `scipy<1.11` |
| `~=` | Compatible release | `openai~=1.0.0` (1.0.x) |
| `!=` | Exclude version | `urllib3!=1.25.0` |
| No specifier | Latest version | `requests` |

### Combining specifiers

```txt
pandas>=1.5.0,<2.0.0
numpy>=1.20,!=1.24.0,<1.25
```

## Adding dependencies

### Via UI

1. Open your Action
2. Go to Properties → Content
3. Scroll to `requirements.txt`
4. Add package name and version
5. Save (auto-saves)

**Example:**
```txt
# API clients
requests==2.31.0
httpx==0.24.1

# Data processing
pandas==1.5.3
numpy==1.24.3

# Utilities
python-dateutil==2.8.2
```

### Via Triton

Ask: _"Add the stripe package to this Action"_

Triton will:
- Determine latest stable version
- Add to requirements.txt
- Update code if needed

### Via code

When writing Action code, Triton may suggest dependencies:

```python
import stripe  # Triton suggests: Add stripe==5.5.0 to requirements.txt
```

## Supported packages

### Pure Python packages

✅ **Supported:** Packages without binary dependencies

**Examples:**
- requests
- httpx
- pydantic
- python-dateutil
- Jinja2
- PyYAML

### Packages with binaries

⚠️ **Limited support:** Some binary packages work, others don't

**Works:**
- pandas (includes C extensions)
- numpy (precompiled wheels)
- Pillow (image processing)
- cryptography

**Doesn't work:**
- Packages requiring system libraries
- Packages with custom C compilation
- Packages needing OS-specific binaries

### Restricted packages

❌ **Not allowed:**

- Packages that fork processes
- Packages requiring root access
- Packages that modify system files
- Malicious or unmaintained packages

## Version pinning

### Why pin versions?

**Reproducibility** — Same code + same packages = same behavior  
**Stability** — Avoid breaking changes from updates  
**Security** — Control when to update vulnerable packages

### Pinning strategies

**Exact pinning (most stable):**
```txt
requests==2.31.0
pandas==1.5.3
```

**Range pinning (allow patches):**
```txt
requests>=2.31.0,<2.32.0
pandas~=1.5.0  # 1.5.x only
```

**Minimum version (flexible):**
```txt
requests>=2.31.0
```

**Recommended:** Exact pinning for production, ranges for development

## Common dependencies

### HTTP requests

```txt
requests==2.31.0         # Sync HTTP client
httpx==0.24.1            # Async HTTP client
urllib3==2.0.4           # Low-level HTTP
```

### Data processing

```txt
pandas==1.5.3            # DataFrames
numpy==1.24.3            # Arrays and math
```

### LLM clients

```txt
openai==1.0.0            # OpenAI API
anthropic==0.3.0         # Anthropic Claude
```

### Date/time

```txt
python-dateutil==2.8.2   # Date parsing
pytz==2023.3             # Timezones
```

### JSON/Data formats

```txt
pydantic==2.0.0          # Data validation
marshmallow==3.20.0      # Serialization
jsonschema==4.19.0       # JSON validation
```

### Text processing

```txt
beautifulsoup4==4.12.2   # HTML parsing
lxml==4.9.3              # XML processing
Jinja2==3.1.2            # Templating
```

### API clients

```txt
stripe==5.5.0            # Stripe payments
sendgrid==6.10.0         # SendGrid email
twilio==8.5.0            # Twilio SMS
boto3==1.28.0            # AWS SDK
google-cloud-storage==2.10.0  # Google Cloud
```

### Utilities

```txt
python-dotenv==1.0.0     # Environment variables
tenacity==8.2.3          # Retry logic
cachetools==5.3.1        # Caching
```

## Dependency conflicts

### Detecting conflicts

Triform validates requirements on save:

**Conflict example:**
```txt
packageA==1.0.0
packageB==2.0.0  # Requires packageA>=2.0.0
```

**Error:**
```
Dependency conflict: packageB requires packageA>=2.0.0, 
but packageA==1.0.0 is specified
```

### Resolving conflicts

**Option 1:** Update versions
```txt
packageA==2.0.0  # Updated
packageB==2.0.0
```

**Option 2:** Use compatible versions
```txt
packageA==1.5.0
packageB==1.8.0  # Older version compatible with packageA 1.x
```

**Option 3:** Remove conflicting package
```txt
# Only keep one, or find alternative
packageB==2.0.0
```

## Updating dependencies

### Check for updates

1. Open Action
2. Go to requirements.txt
3. Click **Check Updates**
4. See available updates with changelog links

**Example:**
```
requests: 2.31.0 → 2.32.0 (security fix)
pandas: 1.5.3 → 2.0.0 (major update, breaking changes)
```

### Update strategy

**Security updates:** Apply immediately
```txt
requests==2.31.0 → requests==2.31.1  # CVE fix
```

**Patch updates:** Low risk, test and apply
```txt
pandas==1.5.3 → pandas==1.5.4
```

**Minor updates:** Test thoroughly
```txt
pandas==1.5.3 → pandas==1.6.0
```

**Major updates:** Review breaking changes
```txt
pandas==1.5.3 → pandas==2.0.0  # Breaking changes!
```

### Bulk updates

Update all packages in one go:

1. Select all outdated packages
2. Click **Update Selected**
3. Review changes
4. Test Action
5. Deploy if successful

**Use with caution:** Test thoroughly after bulk updates.

## Dependency size

### Size limits

**Free tier:** 100 MB total dependencies  
**Pro tier:** 500 MB total dependencies  
**Enterprise:** Custom limits

**Check size:**
```txt
requests (5 MB)
pandas (30 MB)
numpy (15 MB)
---
Total: 50 MB / 100 MB
```

### Reducing size

**Remove unused packages:**
```txt
# Before
requests==2.31.0
pandas==1.5.3  # Not actually used
numpy==1.24.3  # Not actually used

# After
requests==2.31.0
```

**Use lighter alternatives:**
```txt
# Heavy
pandas==1.5.3

# Lighter alternative for simple cases
csv (built-in, no install needed)
```

**Avoid large packages unless necessary:**
```txt
tensorflow  # 500+ MB - only if you need ML
```

## Built-in packages

Python standard library packages don't need to be listed:

**Available by default:**
- json
- csv
- os
- sys
- datetime
- math
- re
- urllib
- collections
- itertools
- functools
- typing

**No need to add to requirements.txt**

## Virtual environments

Triform manages virtual environments automatically:

1. **Parse** requirements.txt
2. **Create** isolated environment
3. **Install** packages
4. **Cache** for faster subsequent runs
5. **Isolate** from other Actions

**You don't need to manage venvs manually.**

## Best practices

> **Pin exact versions in production** — Reproducibility matters

> **Document why** — Comment unusual version pins
>
> ```txt
> # Using old version due to API compatibility
> packageA==1.0.0
> ```

> **Review security advisories** — Check for CVEs regularly

> **Test after updates** — Don't assume compatibility

> **Keep requirements minimal** — Only add what you need

> **Audit dependencies** — Check for abandoned packages

## Troubleshooting

**Problem:** Package installation fails  
**Solution:** Check package name spelling, verify version exists, try different version

**Problem:** Import error despite listing in requirements  
**Solution:** Ensure requirements.txt is saved, redeploy, check for typos

**Problem:** Slow Action startup  
**Solution:** Large dependencies slow cold starts, consider lighter alternatives

**Problem:** "Package not found"  
**Solution:** Check PyPI for correct package name, some packages renamed

**Problem:** Version conflict  
**Solution:** Review error message, update conflicting packages to compatible versions

## Example requirements files

### Minimal API client

```txt
requests==2.31.0
python-dotenv==1.0.0
```

### Data processing

```txt
pandas==1.5.3
numpy==1.24.3
python-dateutil==2.8.2
```

### LLM integration

```txt
openai==1.0.0
anthropic==0.3.0
tenacity==8.2.3
```

### Web scraping

```txt
requests==2.31.0
beautifulsoup4==4.12.2
lxml==4.9.3
```

### Full-featured Action

```txt
# HTTP
requests==2.31.0
httpx==0.24.1

# Data
pandas==1.5.3
numpy==1.24.3

# LLM
openai==1.0.0

# Utilities
python-dateutil==2.8.2
tenacity==8.2.3
pydantic==2.0.0

# API clients
stripe==5.5.0
sendgrid==6.10.0
```

## Next steps

Continue exploring the documentation to learn about Actions, building with Triton, security, and resource-based quotas.



================================================================================
FILE: concepts/executions.mdx
================================================================================

---
title: "Executions"
description: "What runs, how it runs, and practical tips"
---

## How executions start

- Manual — Run from Properties → Execute.
- API/Schedule — Triggered externally (for exposed top‑level Nodes).
- Internal — Invoked by another Node within a Flow.

## Characteristics

- Isolated — No shared state between runs.
- Reproducible — Inputs, modifiers, and context are logged.
- Traceable — Inspect logs, traces, and metrics historically.
- Evaluated — Optionally score outputs with Evaluations.

## Flow of an execution

1. Trigger — Manual, schedule, or API.
2. Input — JSON payload (from trigger or saved Payload).
3. Process — Node(s) execute and pass data along Edges.
4. Output — Result returned or stored.

## Practical tips

- Save Payloads for common scenarios.
- Keep inputs small and typed for easier regression.
- Use consistent correlation fields to stitch traces across components.

> note: Parallelism — For components with Parallel ON, provide a list of inputs; each item is executed concurrently and aggregated downstream.

================================================================================
FILE: concepts/flows.mdx
================================================================================

---
title: "Flows"
description: "Connect nodes into executable graphs; patterns and practices"
---

## Building blocks

- Input Node — Named entry ports.
- Processing Nodes — Actions/Agents/Flows.
- Output Node — Named exit ports.
- Edges — Data/control movement between Nodes.

## Patterns

- Linear — Input → Action → Output.
- Branching — Route by condition into multiple paths.
- Parallel — Fan‑out work; aggregate results downstream.

## Practices

- Keep Flows legible top down.
- Favor shallow graphs; compose via sub‑Flows for depth.
- Remove unused ports and edges to reduce confusion.

> tip: Design for testing — Save example Payloads at the Flow level to lock in expected shapes and make regression checks easy.

================================================================================
FILE: concepts/glossary.mdx
================================================================================

---
title: "Glossary"
description: "Key terms and concepts in Triform"
---

## Core Concepts

### Action
A Python function that performs a specific task. Actions are deterministic, reusable building blocks for Flows. Examples: API calls, data transformations, validations.

### Agent
An LLM-powered component that can make decisions, generate content, and use tools. Agents have system prompts, model configurations, and access to Actions/Flows as tools.

### API Key
A secret token used to authenticate requests to Triform's API. Required for executing deployed Projects and accessing Triform services programmatically.

### Canvas
The visual workspace where you build and view Projects, Flows, and their components. The Canvas displays nodes and edges showing system structure.

### Deployment
The process of publishing a Project to a live environment (staging or production) where it's accessible via API or triggers.

### Edge
A connection between two nodes in a Flow, showing data or control flow direction. Edges define how information moves through your system.

### Execution
A single run of a Project, Flow, Agent, or Action. Each execution has inputs, outputs, logs, and metrics. Executions are tracked for observability and debugging.

### Flow
An orchestration graph that connects Actions, Agents, and sub-Flows. Flows define multi-step logic with inputs, outputs, and data flow between components.

### Project Variable
An environment-specific value (API keys, config, secrets) accessible across Projects. Variables keep sensitive data out of code and enable environment-specific configuration.

### Node
A visual element on the Canvas representing a component (Action, Agent, Flow, Project) or control structure (Router, Merge, Loop).

### Organization
The top-level entity that owns Projects, members, and billing. Organizations provide team collaboration and access control.

### Payload
The JSON input data provided when executing a component. Payloads must match the defined input schema.

### Project
A complete, deployable AI system containing Actions, Agents, and Flows. Projects are the primary unit of work in Triform.

### Trigger
An automated mechanism that executes a Project in response to events (webhooks, schedules, manual API calls).

## Platform Components

### Agent Toolbox
A panel showing available components (Actions, Agents, Flows) that can be dragged onto the Canvas or reused across Projects.

### Chat Panel
The interface for interacting with Triton, Triform's AI assistant. The Chat Panel enables conversational building, editing, and debugging.

### Input Node
The entry point of a Flow, defining what data the Flow expects. Input nodes have schemas specifying required and optional fields.

### Output Node
The exit point of a Flow, defining what data the Flow returns. Output nodes shape the API response for deployed Projects.

### Properties Panel
A panel showing detailed configuration for selected components. Properties include content (code, prompts), input/output schemas, executions, and settings.

### Top Bar
The navigation bar showing breadcrumbs, home, profile, and deploy controls. The Top Bar provides Project navigation and actions.

### Triton
Triform's built-in AI assistant that helps you build, modify, and debug Projects through natural conversation.

## Technical Terms

### Breadcrumb
Navigation elements showing your current location in the hierarchy (e.g., Home > Project > Flow > Agent). Click any level to jump back.

### Cron Expression
A time-based schedule format for triggering periodic executions. Example: `0 6 * * *` means daily at 6 AM.

### Dependency
An external Python package required by an Action. Dependencies are listed in `requirements.txt`.

### Port
An input or output connection point on a node. Data flows from output ports to input ports via edges.

### Rate Limit
A restriction on how many requests can be made in a time period. Protects against abuse and ensures fair usage.

### Requirements.txt
A file listing Python package dependencies for an Action. Format follows pip conventions.

### Retry Logic
Automatic re-execution of failed operations, typically with exponential backoff. Handles transient failures gracefully.

### Rollback
Reverting a deployment to a previous version. Used when new deployments have issues.

### Schema
A definition of data structure specifying field names, types, and validation rules. Used for inputs, outputs, and payloads.

### System Prompt
Instructions given to an Agent defining its behavior, personality, and capabilities. System prompts guide Agent responses.

### Timeout
Maximum time allowed for an execution before it's canceled. Prevents infinite loops and hung processes.

### Token
In LLM context, a unit of text (roughly a word or sub-word). Token usage is tracked for cost and limits.

### Trace
A detailed record of execution showing which nodes ran, in what order, with what data and timing. Essential for debugging.

### Webhook
An HTTP endpoint that receives event notifications from external services. Triggers Project execution based on external events.

## Workflow Terms

### Branching
When a Flow splits into multiple paths based on conditions. Example: route by user tier (free vs. premium).

### Cold Start
The initial delay when executing a component that hasn't run recently. Caused by environment setup and dependency loading.

### Concurrency
Number of simultaneous executions. Can be limited to control resource usage or ensure serial processing.

### Fanout/Fanin
Fanout: One node sends data to multiple nodes (parallel processing). Fanin: Multiple nodes send data to one node (merging results).

### Merge Node
A control node that combines multiple inputs into one output. Used after parallel operations.

### Parallel Processing
Executing multiple operations simultaneously instead of sequentially. Improves performance for independent tasks.

### Router Node
A control node that directs data down different paths based on conditions. Enables branching logic in Flows.

### Serial Processing
Executing operations one at a time in sequence. Each step waits for the previous one to complete.

### Sub-Flow
A Flow used as a component within another Flow. Enables modularity and reusability.

## Environment Terms

### Development (Dev)
The environment where you actively build and test. Changes are immediate, no deployment needed.

### Production (Prod)
The live environment serving real users. Requires deployment and should be stable.

### Staging
A pre-production environment for testing. Mirrors production configuration but isolated from real traffic.

## Access Control

### Admin
A role with full permissions: create, edit, delete, deploy, manage members, billing.

### Editor
A role with permission to create and edit Projects but not manage members or billing.

### Member
A user belonging to an Organization with assigned roles and permissions.

### Viewer
A role with read-only access. Can view Projects and executions but not modify.

## Monitoring & Debugging

### Alert
A notification triggered by specific conditions (e.g., execution failure, high error rate). Sent via email, Slack, or webhook.

### Error Rate
Percentage of executions that failed. Used to monitor system health.

### Execution Log
Detailed text output from an execution including debug, info, warning, and error messages.

### Metric
Quantitative measurement of system behavior (execution time, success rate, token usage, cost).

### Success Rate
Percentage of executions that completed successfully. Key health indicator.

## Integration Terms

### API Endpoint
A URL where your deployed Project is accessible. Format: `https://app.triform.ai/api/in/{project-id}/{flow-id}`.

### Bearer Token
Authentication method where API key is sent in Authorization header without the "Bearer" prefix: `Authorization: {api-key}`.

### CI/CD
Continuous Integration/Continuous Deployment. Automated testing and deployment pipelines.

### SDK
Software Development Kit. Client libraries for accessing Triform API from code (Python, JavaScript, etc.).

### Webhook Payload
The JSON data sent to your webhook endpoint by external services. Must be transformed to match your Project's input schema.

## Billing & Quotas

### Execution-based Quota
Limits on number of executions per time period. Varies by plan tier.

### Resource-based Quota
Limits on storage, dependencies size, and similar resources. Varies by plan tier.

### Token Usage
For LLMs, the number of tokens consumed. Tracked for billing and optimization.

## Best Practices Terms

### DRY (Don't Repeat Yourself)
Extract repeated logic into reusable Actions or Flows instead of duplicating.

### Idiomatic Code
Code following language conventions and best practices. Python Actions should follow PEP 8.

### Type Hints
Python annotations specifying variable and function types. Improves code clarity and enables validation.

### Validation
Checking input data meets expected format and constraints before processing. Fail early, fail clearly.

## Need more help?

If a term isn't listed here:
- Ask Triton in the Chat Panel
- Check relevant concept pages in the documentation
- Join our [Discord community](https://discord.gg/triform)



================================================================================
FILE: concepts/nodes.mdx
================================================================================

---
title: "Nodes"
description: "Building blocks: properties, types, and lifecycle"
---

## Properties of a Node

- Inputs / Outputs — Typed connection points that define data contracts.
- Definition — Purpose, requirements, constraints.
- Content — Implementation/configuration (varies by Node type).
- Execute — Run with a JSON payload; view results.
- Observability — Executions emit logs, traces, and metrics.

## Types of Nodes

- Project — A scoped workspace that contains top‑level Agents/Flows.
- Agent — LLM‑driven logic that can call tools (Actions, Flows, other Agents).
- Flow — A directed graph of Nodes connected by Edges.
- Action — Atomic Python logic.
- Input / Output — Special Nodes that mark Flow boundaries.

## Lifecycle

1. Draft — Define intent and I/O.
2. Test — Use Payloads and small runs.
3. Compose — Chain into Flows or add as Agent tools.
4. Expose — Promote top‑level Agents/Flows to API/Schedule.
5. Deploy — Push current state; track changes over time.

> tip: Keep contracts stable — Name and type your Inputs/Outputs early. Stable interfaces reduce downstream refactors.

================================================================================
FILE: concepts/payloads.mdx
================================================================================

---
title: "Payloads"
description: "Structure input data for Projects, Flows, and Agents"
---

## What is a Payload?

A Payload is the input data you provide when executing a Project, Flow, Agent, or Action. It's a JSON object that contains all the information needed for that execution.

Think of it as:
- The **arguments** to a function
- The **request body** of an API call
- The **input** to your system

## Why Payloads

- Speed — One‑click reruns for common cases.
- Reproducibility — Lock in known‑good inputs as your baseline.
- Documentation — Show how a component is intended to be used.

## Good Payloads include

- Minimal required fields with realistic values.
- Edge‑case variants (empty, large, malformed types).
- Comments or README references for context.

## Workflow

1. Create a Payload for a Node (Action/Flow).
2. Refine the Node until the output is acceptable.
3. Keep the Payload as a regression check when you iterate.

> tip: Name by intent — Prefer names like `minimal`, `typical_customer`, `edge_empty_messages` over `test1`, `test2`.

## Payload structure

### Basic payload

```json
{
  "field1": "value",
  "field2": 123,
  "field3": true
}
```

### Nested payload

```json
{
  "user": {
    "id": "user_123",
    "email": "alice@example.com",
    "tier": "premium"
  },
  "action": "process_order",
  "items": [
    {"id": "item_1", "quantity": 2},
    {"id": "item_2", "quantity": 1}
  ]
}
```

### Payload with metadata

```json
{
  "data": {
    "user_input": "Summarize this article..."
  },
  "config": {
    "format": "bullets",
    "max_length": 200
  },
  "metadata": {
    "request_id": "req_xyz",
    "source": "web_app"
  }
}
```

## Payload schema

The schema defines what fields are expected, their types, and validation rules.

### Defining a schema

In your Project/Flow Input node:

```json
{
  "user_id": {
    "type": "string",
    "required": true,
    "description": "Unique user identifier"
  },
  "action": {
    "type": "enum",
    "options": ["create", "update", "delete"],
    "required": true,
    "description": "Operation to perform"
  },
  "data": {
    "type": "object",
    "required": false,
    "description": "Operation-specific data"
  },
  "options": {
    "type": "object",
    "required": false,
    "default": {},
    "description": "Optional configuration"
  }
}
```

### Data types

| Type | Example | Description |
|------|---------|-------------|
| `string` | `"hello"` | Text |
| `number` | `42`, `3.14` | Integer or float |
| `boolean` | `true`, `false` | True/false |
| `array` | `[1, 2, 3]` | List of items |
| `object` | `{"key": "value"}` | Nested structure |
| `enum` | `"red"` from `["red", "green", "blue"]` | Fixed options |
| `null` | `null` | Absence of value |

### Required vs. optional

**Required fields:**
- Must be present in every payload
- Execution fails if missing
- No default value

**Optional fields:**
- Can be omitted
- Can have default values
- Execution continues without them

**Example:**
```json
{
  "required_field": "must be here",
  "optional_field": "can be omitted"
}
```

## Creating payloads

### In the UI

1. Select your Project/Flow/Agent
2. Go to **Properties → Execute**
3. Enter payload in the JSON editor
4. Click **Execute**

**Tips:**
- Use the schema viewer to see expected fields
- Auto-complete helps with field names
- Syntax highlighting catches errors

### Via API

This is an API-call that would trigger a certain flow with the 
{"user_id": "123", "action": "process", "data": {"value": 42}} payload:

```bash
curl -X POST 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784' \
  --header 'Authorization: 88ffe8398f972837ccc6e0a6cd31c5c92689040c' \
  --header 'Content-Type: application/json' \
  --data '{"user_id": "123", "action": "process", "data": {"value": 42}}'
```

### Programmatically

**Python:**
```python
import requests

payload = {
    "user_id": "123",
    "action": "process",
    "data": {"value": 42}
}

response = requests.post(
    "https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784",
    headers={
        "Authorization": API_KEY,
        "Content-Type": "application/json"
    },
    json=payload
)

result = response.json()
```

**JavaScript:**
```javascript
const payload = {
  user_id: "123",
  action: "process",
  data: { value: 42 }
};

const response = await fetch(
  "https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784",
  {
    method: "POST",
    headers: {
      "Authorization": API_KEY,
      "Content-Type": "application/json"
    },
    body: JSON.stringify(payload)
  }
);

const result = await response.json();
```

## Payload validation

Triform validates payloads against the schema before execution.

### Common validation errors

**Missing required field:**
```json
{
  "error": "ValidationError",
  "message": "Missing required field: user_id",
  "field": "user_id"
}
```

**Wrong type:**
```json
{
  "error": "ValidationError",
  "message": "Expected number but got string",
  "field": "age",
  "expected": "number",
  "actual": "string"
}
```

**Invalid enum value:**
```json
{
  "error": "ValidationError",
  "message": "Invalid value for action. Must be one of: create, update, delete",
  "field": "action",
  "value": "invalid_action",
  "allowed": ["create", "update", "delete"]
}
```

**Out of range:**
```json
{
  "error": "ValidationError",
  "message": "Value must be between 0 and 100",
  "field": "percentage",
  "value": 150,
  "min": 0,
  "max": 100
}
```

## Saved payloads

Save frequently-used payloads for quick testing.

### Saving a payload

1. Enter payload in Execute panel
2. Click **Save Payload**
3. Name it: `Test Case 1: Happy Path`
4. Click **Save**

### Loading a saved payload

1. Go to Execute panel
2. Click **Load Payload**
3. Select from list
4. Click **Load**
5. Optionally modify
6. Execute

### Payload library

Organize saved payloads:

**By category:**
- Happy path examples
- Edge cases
- Error scenarios
- Performance tests
- Regression tests

**By purpose:**
- `demo_payload` — For demonstrations
- `test_minimal` — Minimum required fields
- `test_full` — All fields populated
- `test_edge_empty_list` — Edge case testing

## Payload best practices

> **Provide examples** — Include sample payloads in documentation

> **Use meaningful values** — `"user_123"` is better than `"test"`

> **Test edge cases** — Empty arrays, null values, max sizes

> **Save regression tests** — Keep payloads that found bugs

> **Document schema** — Clear descriptions for each field

> **Version payloads** — If schema changes, update saved payloads

## Payload patterns

### Pattern 1: Simple request

```json
{
  "input": "string to process"
}
```

**Use case:** Single-purpose operations (text summarization, translation)

### Pattern 2: User + action + data

```json
{
  "user_id": "123",
  "action": "create",
  "data": {
    "name": "New Item",
    "description": "Item details"
  }
}
```

**Use case:** CRUD operations, user-specific actions

### Pattern 3: Batch processing

```json
{
  "items": [
    {"id": "1", "value": "A"},
    {"id": "2", "value": "B"},
    {"id": "3", "value": "C"}
  ],
  "options": {
    "parallel": true,
    "fail_fast": false
  }
}
```

**Use case:** Processing multiple items, bulk operations

### Pattern 4: Configuration-heavy

```json
{
  "data": "content to process",
  "config": {
    "format": "markdown",
    "style": "formal",
    "length": "medium",
    "audience": "technical",
    "include_citations": true
  }
}
```

**Use case:** Configurable behavior, user preferences

### Pattern 5: Streaming/chunked

```json
{
  "chunk_id": "chunk_1",
  "total_chunks": 10,
  "data": "partial data...",
  "metadata": {
    "session_id": "session_xyz"
  }
}
```

**Use case:** Large payloads split into chunks, streaming data

## Dynamic payloads

Generate payloads programmatically.

### From user input

```python
def create_payload(user_input):
    return {
        "query": user_input,
        "timestamp": datetime.now().isoformat(),
        "user_id": get_current_user_id()
    }
```

### From database

```python
def create_batch_payload(user_ids):
    users = fetch_users(user_ids)
    return {
        "users": [
            {
                "id": u.id,
                "email": u.email,
                "tier": u.tier
            }
            for u in users
        ]
    }
```

### From webhook

```python
@app.route('/webhook', methods=['POST'])
def handle_webhook():
    webhook_data = request.json
    
    payload = {
        "event_type": webhook_data['type'],
        "data": webhook_data['data'],
        "received_at": datetime.now().isoformat()
    }
    
    # Send to Triform
    execute_project(payload)
    
    return {'status': 'received'}
```

## Payload transformation

Sometimes you need to transform external data into Triform payload format.

### Example transformation

**External API response:**
```json
{
  "userId": "123",
  "fullName": "Alice Smith",
  "emailAddress": "alice@example.com",
  "accountLevel": "PREMIUM"
}
```

**Triform payload:**
```json
{
  "user_id": "123",
  "name": "Alice Smith",
  "email": "alice@example.com",
  "tier": "premium"
}
```

**Transformation code:**
```python
def transform_to_payload(api_response):
    return {
        "user_id": api_response["userId"],
        "name": api_response["fullName"],
        "email": api_response["emailAddress"],
        "tier": api_response["accountLevel"].lower()
    }
```

## Debugging payloads

### Common issues

**Problem:** Execution fails with validation error  
**Solution:** Check payload against schema, fix type mismatches or missing fields

**Problem:** Payload is valid but execution fails  
**Solution:** Check execution logs, data might be valid format but wrong content

**Problem:** Payload too large  
**Solution:** Check quotas, consider chunking or reducing data size

**Problem:** Payload works in UI but not via API  
**Solution:** Check JSON encoding, content-type header, authentication

### Testing payloads

**Step 1:** Start simple
```json
{
  "minimal_required_field": "value"
}
```

**Step 2:** Add optional fields
```json
{
  "minimal_required_field": "value",
  "optional_field": "value"
}
```

**Step 3:** Add complexity
```json
{
  "minimal_required_field": "value",
  "optional_field": "value",
  "nested_object": {
    "key": "value"
  },
  "array": [1, 2, 3]
}
```

**Step 4:** Test edge cases
```json
{
  "empty_string": "",
  "empty_array": [],
  "null_value": null,
  "large_number": 999999999
}
```

## Payload size limits

### Default limits

**Free tier:** 1 MB per payload  
**Pro tier:** 10 MB per payload  
**Enterprise:** Custom limits

### Handling large payloads

**Option 1: Chunking**
Split large payloads into smaller chunks, process sequentially

**Option 2: Reference by URL**
```json
{
  "data_url": "https://s3.amazonaws.com/bucket/large-file.json",
  "process_type": "batch"
}
```

## Next steps

Continue exploring the documentation to learn about Executions, Variables, and integrating Projects into your apps.



================================================================================
FILE: concepts/projects.mdx
================================================================================

---
title: "Projects"
description: "Organize and deploy AI systems as cohesive units"
---

## What is a Project?

A Project is a complete, deployable AI system in Triform. It's a repository that holds all the components (Agents, Flows, Actions) needed to accomplish a specific goal.

## Project structure

Projects contain:

**Actions** — Python functions for deterministic logic  
**Agents** — LLM-powered components with tools  
**Flows** — Orchestration graphs connecting components  
**Sub-Projects** — Nested Projects for modularity  
**Variables** — Configuration accessible to all components  
**Triggers** — What starts executions

Example structure:
```
Project: Customer Support System
├── Flows
│   ├── ticket_triage
│   └── response_generation
├── Agents
│   ├── sentiment_analyzer
│   └── response_writer
├── Actions
│   ├── fetch_ticket
│   ├── update_status
│   └── send_notification
└── Project Variables
    ├── API_KEYS
    └── CONFIG
```

## Creating Projects

### Via UI

1. Click **Home** in Top Bar
2. Click **New Project**
3. Name it and add a description
4. Click **Create**

### Via Triton

Ask: _"Create a new Project for processing customer feedback"_

Triton will create the structure, ask clarifying questions, and suggest initial components.

## Working with Projects

### Opening and navigating

1. Navigate to **Home**
2. Click your Project to open on Canvas
3. **Double-click** nodes to drill into Flows, Agents, or Actions
4. Use **breadcrumbs** at top to track your location

### Building in a Project

**Via Triton:**
- _"Add a validation Flow to this Project"_
- _"Create an Action that calls the Stripe API"_

**Via UI:**
- Right-click Canvas → Add Node
- Choose component type, configure, and connect

## Project lifecycle

**1. Design** — Plan inputs, outputs, and data flow  
**2. Build** — Create Actions, configure Agents, build Flows  
**3. Integrate** — Wire components together  
**4. Test** — Validate with sample payloads  
**5. Deploy** — Push to staging, then production  
**6. Monitor** — Track execution success rate and performance  
**7. Iterate** — Add features and optimize

## Common patterns

### Request-Response API
```
Input → Validate → Process → Format → Output
```
Use case: API endpoints, data transformations

### Multi-stage pipeline
```
Input → Extract → Transform → Load → Output
```
Use case: ETL, data processing, content generation

### Agent with tools
```
Input (query) → Agent (with tools) → Output (response)
  Tools: search_database, call_api, send_email
```
Use case: Conversational interfaces, decision support

### Parallel processing
```
Input → Split → [Process A, B, C] → Merge → Output
```
Use case: High-volume processing, independent tasks

## Best practices

> **Single responsibility** — Each Project should have one clear purpose

> **Self-contained** — Minimize dependencies on other Projects

> **Well-documented** — Clear descriptions for all components

> **Tested** — Save example payloads for regression testing

> **Monitored** — Track success rates and performance

## Project scope guidelines

**Too small** ❌  
Project: "Uppercase a string" → Should be an Action

**Just right** ✅  
Project: "User Onboarding" → Includes validation, account creation, welcome email, setup

**Too large** ❌  
Project: "Entire Customer Platform" → Split into multiple Projects: Support, Billing, Notifications

**Rule of thumb:** If you can't explain what the Project does in one sentence, it's too large.

## Troubleshooting

**Project won't deploy**  
Check all components are configured, Project Variables are set, no circular dependencies

**Execution fails in Project but components work individually**  
Check connections, field mappings, and data flow

**Project is slow**  
Profile execution, identify bottlenecks, add parallel processing or caching

**Can't find my Project**  
Check filters, search by name, verify you're in the right Organization

## Next steps

See [Agents](/concepts/agents), [Flows](/concepts/flows), [Actions](/concepts/actions), and [Build a New Project](/tutorials/build-a-new-project).



================================================================================
FILE: concepts/triggers.mdx
================================================================================

---
title: "Triggers"
description: "Automate Project execution with webhooks, schedules, and events"
---

## What are Triggers?

Triggers automatically execute your Projects in response to events, without manual intervention. Instead of clicking "Execute" every time, set up Triggers to run your Projects when specific conditions are met.

## Types of Triggers

### Webhook Triggers

Execute when an HTTP request hits your webhook endpoint.

**Use cases:**
- Payment notifications (Stripe, PayPal)
- GitHub events (push, PR, issue)
- Form submissions
- Third-party service events

**Setup:**
1. Go to Project → Triggers
2. Click **Add Trigger → Webhook**
3. Configure:
   - Path: `/webhook/payment-received`
   - Authentication: API key or HMAC signature
   - Payload transformation (if needed)
4. Save → Get webhook URL
5. Register URL with external service

**Example URL:**
```
https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784
```

This is an API-call that would trigger a certain flow with the 
{"event": "payment.success", "amount": 99.99, "customer_id": "cust_123"} payload:

```bash
curl -X POST 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784' \
  --header 'Authorization: 88ffe8398f972837ccc6e0a6cd31c5c92689040c' \
  --header 'Content-Type: application/json' \
  --data '{"event": "payment.success", "amount": 99.99, "customer_id": "cust_123"}'
```

**Your Project receives:**
```json
{
  "event_type": "payment.success",
  "amount": 99.99,
  "customer_id": "cust_123",
  "received_at": "2025-10-01T10:30:00Z"
}
```

### Schedule Triggers

Execute at specific times or intervals.

**Use cases:**
- Daily reports
- Periodic data sync
- Cleanup jobs
- Monitoring checks

**Setup:**
1. Go to Project → Triggers
2. Click **Add Trigger → Schedule**
3. Configure schedule:
   - **Cron expression:** `0 6 * * *` (daily at 6 AM)
   - **Interval:** Every 15 minutes
   - **One-time:** Specific date/time
4. Set timezone
5. Define payload (static or dynamic)
6. Save

**Schedule types:**

| Type | Example | Description |
|------|---------|-------------|
| Cron | `0 6 * * *` | Daily at 6 AM UTC |
| Interval | `Every 15 minutes` | Repeating interval |
| Hourly | `Every hour at :00` | Top of each hour |
| Daily | `Daily at 9:00 AM` | Specific time daily |
| Weekly | `Every Monday at 8:00 AM` | Specific day/time weekly |
| Monthly | `1st of month at 00:00` | Specific day each month |
| One-time | `2025-10-15 14:00:00` | Single execution |

**Cron expression format:**
```
* * * * *
│ │ │ │ │
│ │ │ │ └─── Day of week (0-7, 0=Sunday)
│ │ │ └───── Month (1-12)
│ │ └─────── Day of month (1-31)
│ └───────── Hour (0-23)
└─────────── Minute (0-59)
```

**Common patterns:**
- `0 * * * *` — Every hour
- `*/15 * * * *` — Every 15 minutes
- `0 9 * * 1-5` — Weekdays at 9 AM
- `0 0 1 * *` — First of each month

### Manual Triggers

Execute via Execution button.

**Use cases:**
- On-demand processing
- Admin actions
- Testing
- User-initiated workflows


### Viewing Triggers

1. Go to Project → Executions
2. See list of all triggers:
   - Type (webhook, schedule)

### Editing Triggers

1. Select trigger from list
2. Click **Edit**
3. Modify configuration
4. **Save** → Changes take effect immediately

**Note:** Schedule changes apply to future executions, not currently running ones.


### Deleting Triggers

Permanently remove a trigger:

1. Select trigger
2. Click **Delete**
3. Confirm
4. Trigger is removed

**Warning:** Cannot be undone. Webhook URLs will stop working.

## Trigger configuration

### Payload

Define what data the triggered execution receives.

**Static payload:**
```json
{
  "source": "scheduled_job",
  "timestamp": "{{now}}"
}
```

**Dynamic payload from webhook:**
```json
{
  "event_type": "{{webhook.event}}",
  "data": "{{webhook.body}}",
  "headers": "{{webhook.headers}}"
}
```

**Using variables:**
```json
{
  "api_key": "{{global.API_KEY}}",
  "environment": "{{global.ENV}}"
}
```

### Timeout

Set maximum execution time:

- Default: 60 seconds
- Max: 15 minutes (Pro), 60 minutes (Enterprise)

**If timeout reached:**
- Execution is canceled
- Error logged
- Retry policy applies (if configured)


## Webhook authentication

Secure your webhook endpoints.

### API Key

Require API key in header:

This is an API-call that would trigger a certain flow with the 
{"data": "value"} payload:

```bash
curl -X POST 'https://app.triform.ai/api/in/31460071-17aa-43e8-a0e6-fb22984c0bdc/26420f15-2143-443f-bca9-45c5b0720784' \
  --header 'Authorization: 88ffe8398f972837ccc6e0a6cd31c5c92689040c' \
  --header 'Content-Type: application/json' \
  --data '{"data": "value"}'
```

**Configuration:**
1. Enable API key authentication
2. Provide key to webhook sender
3. Triform validates before execution

## Common patterns

### Pattern 1: Webhook → Process → Notify

```
Webhook receives event
  → Validate and transform
  → Process data
  → Send notification
```

**Example:** Payment received → Update database → Email customer

### Pattern 2: Schedule → Fetch → Process → Store

```
Daily at 6 AM
  → Fetch data from APIs
  → Process and analyze
  → Store results
```

**Example:** Daily report generation

### Pattern 3: Event chain

```
Project A completes
  → Triggers Project B
  → Triggers Project C
```

**Example:** Onboarding flow: Create account → Send welcome → Provision services

### Pattern 4: Error handling

```
Project fails
  → Event trigger fires
  → Error logger Project executes
  → Alert sent to team
```

**Example:** Automated incident response

## Troubleshooting

**Problem:** Webhook not triggering  
**Solution:** Check URL is correct, authentication is valid, check logs for rejected requests

**Problem:** Schedule not running  
**Solution:** Verify cron expression, check timezone, ensure trigger is not paused

**Problem:** Trigger fires but execution fails  
**Solution:** Check payload format, verify required fields, review execution logs

## Next steps

Continue exploring the documentation to learn about Executions and webhook configuration.



================================================================================
FILE: concepts/project/a_project.mdx
================================================================================

---
title: "Projects"
description: "Focused workspaces: contents, why, and workflow"
---

## What a Project contains

- Top‑level Nodes — Agents or Flows that may be exposed as APIs/Schedules.
- Configuration — Project‑scoped settings like Variables.
- Artifacts — Actions, sub‑Flows, Agents that support the top‑level surface.

## Why Projects

- Isolation — Experiment safely within a bounded scope.
- Access control — Limit who can see or change components.
- Deployment unit — Deploy and expose from the Project surface.

## Typical workflow

1. Create or open a Project.
2. Model top‑level entry points (Agents/Flows).
3. Compose sub‑Flows and Actions beneath them.
4. Save Payloads, validate behavior, and Deploy.
5. Expose selected top‑level Nodes via API or Schedule.

> note: Only top‑level gets exposed — You can call sub‑components internally, but only top‑level Agents/Flows are eligible for API/Schedule exposure.

================================================================================
FILE: concepts/project/api-keys.mdx
================================================================================

---
title: "API Keys"
description: "Manage keys, scope, usage, and hygiene"
---

## Manage keys

- Go to Project → Properties → Execute to create/revoke keys.
- Store keys securely; rotate on a schedule.

## Scope & usage

- Keys are scoped to the Project.
- Send keys via an Authorization header or the mechanism your gateway specifies.

## Good hygiene

- Separate keys by environment (dev/test/prod).
- Log key ID (not the secret) alongside requests for traceability.
- Revoke keys immediately if leaked.

> warning: Never commit keys — Do not embed keys in code or client apps. Use server‑side storage or a secrets manager.

================================================================================
FILE: concepts/project/deployments.mdx
================================================================================

---
title: "Deployments"
description: "Publish Project state and recommended flow"
---

## What deploy does

- Captures current Node definitions and wiring.
- Makes exposed top‑level Agents/Flows use the latest configuration.

## Recommended flow

1. Finalize changes on the Canvas.
2. Re‑run Payloads for sanity checks.
3. Click Deploy.
4. Re‑hit APIs/Schedules if applicable.

## Troubleshooting

- Mismatched inputs — Verify Input/Output port names.
- Toolbox drift — Ensure Agent tools are up to date pre‑deploy.
- Missing variables — Double‑check required Project Variables.

> tip: Deploy in small steps — Smaller, frequent deploys make issues easier to pinpoint and roll forward from.

================================================================================
FILE: concepts/project/expose.mdx
================================================================================

---
title: "Expose Components (API & Schedule)"
description: "Expose top‑level Agents/Flows and best practices"
---

## API exposure

- Select the Project and open Properties → Execute.
- Choose a top‑level Agent/Flow to Expose as API.
- Confirm request/response shapes (based on the Node’s I/O).

## Schedule exposure

- From Properties → Execute, pick a top‑level Agent/Flow.
- Configure a Schedule (cron‑style cadence).

## Best practices

- Keep API payloads explicitly typed.
- Perform basic validation at the edge.
- Use Payloads that mirror your API contract for local testing.

> note: Non top‑level components — Sub‑Flows and Actions can’t be exposed directly. Wrap them in a top‑level Flow/Agent.

================================================================================
FILE: concepts/project/projects-list.mdx
================================================================================

---
title: "Projects List View"
description: "Create and navigate Projects from the dashboard"
---

## Create a Project

1. Open the Projects page (Top Bar → Home).
2. Click ＋ and provide a name (and optional description).
3. Open the new Project to start building on the Canvas.

## Manage Projects

- Open to continue work.
- Describe concisely so teammates can find the right Project.
- Switch active Organization from Profile if needed.

## Tips

- Organize Projects by product surface or team, not by single feature.
- Keep descriptions current to reflect scope.

> note: Default Project — New users start with a default Project that’s ready for immediate use.

================================================================================
FILE: concepts/project/variables.mdx
================================================================================

---
title: "Variables"
description: "Manage configuration and secrets with Project Variables"
---

## What are Project Variables?

Project Variables are project-specific values accessible across all components in your Project. They store:

- **API keys and secrets** — Credentials for external services
- **Configuration** — Environment-specific settings
- **Constants** — Shared values used across Projects
- **Feature flags** — Toggle features on/off

## Why use Project Variables?

**Security** — Keep secrets out of code  
**Flexibility** — Change config without redeploying  
**Environments** — Different values for dev/staging/prod  
**Reusability** — One variable, many usages  

## Creating Variables

### Via UI

1. Go to **Project → Variables**
2. Click **Add Variable**
3. Configure:
   - **Name:** `OPENAI_API_KEY`
   - **Value:** `sk-...` (hidden)
   - **Type:** Secret
   - **Description:** API key for OpenAI
4. Click **Save**

### Variable names

**Conventions:**
- `UPPERCASE_WITH_UNDERSCORES`
- Descriptive: `SENDGRID_API_KEY` not `API_KEY_1`
- Prefixed by service: `STRIPE_SECRET_KEY`, `STRIPE_PUBLISHABLE_KEY`

**Reserved names:**
Avoid system variables: `PATH`, `HOME`, `USER`

## Using Variables

### In Actions

```python
import os

def send_email(to: str, subject: str, body: str):
    api_key = os.environ.get('SENDGRID_API_KEY')
    
    # Use api_key...
```

Variables are automatically injected as environment variables.

### In Agent prompts

```
You are a customer service agent for {{COMPANY_NAME}}.

When customers ask about pricing, refer to our pricing page:
{{PRICING_URL}}

For technical support, escalate to: {{SUPPORT_EMAIL}}
```

Variables in `{{VAR_NAME}}` format are replaced before execution.

### In Flow configurations

**Conditional routing:**
```
If {{ENVIRONMENT}} == "production":
  → Production Flow
Else:
  → Test Flow
```

### In webhook payloads

```json
{
  "data": "{{webhook.body}}",
  "api_key": "{{EXTERNAL_API_KEY}}",
  "environment": "{{ENVIRONMENT}}"
}
```


## Managing Variables

### Viewing Variables

1. Go to **Organization → Project Variables**
2. See list of all variables
3. Secrets show as `***`
4. Click to view metadata (not secret values)

### Editing Variables

1. Select variable
2. Click **Edit**
3. Modify value or description
4. Click **Save**

**Note:** Editing a secret requires re-entering the full value.

### Deleting Variables

1. Select variable
2. Click **Delete**
3. Confirm

**Warning:**
- Any components using this variable will fail
- Check usages before deleting

### Finding usages

1. Select variable
2. Click **Show Usages**
3. See list of Projects/components using it

**Use case:** Before deleting or changing a variable.

## Environment Variables

Triform provides built-in environment variables:

| Variable | Value | Description |
|----------|-------|-------------|
| `TRIFORM_PROJECT_ID` | `proj_abc123` | Current Project ID |
| `TRIFORM_EXECUTION_ID` | `exec_xyz789` | Current execution ID |
| `TRIFORM_ORG_ID` | `org_abc123` | Organization ID |
| `TRIFORM_USER_ID` | `user_xyz789` | User who triggered execution |
| `TRIFORM_TIMESTAMP` | ISO 8601 datetime | Execution start time |

**Usage:**
```python
import os

project_id = os.environ.get('TRIFORM_PROJECT_ID')
execution_id = os.environ.get('TRIFORM_EXECUTION_ID')

print(f"Running in project {project_id}, execution {execution_id}")
```

## Best practices

> **Never hardcode secrets** — Always use Project Variables

> **Use descriptive names** — Clear what each variable is for

> **Document variables** — Add descriptions explaining usage

> **Rotate secrets regularly** — Update API keys periodically

> **Test with dummy values** — Use fake keys in dev/staging

## Security considerations

### Secret handling

**Do:**
- ✅ Store all secrets in Project Variables
- ✅ Use `secret` type for sensitive data
- ✅ Limit access to who can view secrets
- ✅ Rotate secrets regularly
- ✅ Audit secret access

**Don't:**
- ❌ Hardcode secrets in code
- ❌ Log secret values
- ❌ Share secrets in chat/email
- ❌ Commit secrets to git
- ❌ Use production secrets in dev

### Access control

**Organization Admins:** Can create, edit, delete all variables  
**Organization Editors:** Can view and use variables  
**Organization Viewers:** Can see variable names but not values

**Project-specific permissions:** Override Organization permissions

### Audit logs

All variable operations are logged:

- Created by whom, when
- Modified by whom, when, what changed
- Accessed by which execution
- Deleted by whom, when

**View audit log:**
1. Go to Organization → Project Variables
2. Select variable
3. Click **Audit Log**

## Migrating Variables

### From hardcoded to variables

**Before:**
```python
def send_notification():
    api_key = "sk_live_abc123"  # Hardcoded!
    # ...
```

**After:**
```python
def send_notification():
    api_key = os.environ.get('NOTIFICATION_API_KEY')
    # ...
```

**Steps:**
1. Create Project Variable
2. Update code to use `os.environ.get()`
3. Test
4. Remove hardcoded value
5. Deploy

## Troubleshooting

**Problem:** Variable not found  
**Solution:** Check variable name and project

**Problem:** Secret value showing as `***`  
**Solution:** Intentional for security. Re-enter to update.

**Problem:** Variable changes not taking effect  
**Solution:** Redeploy Project, or restart execution

## Next steps

Continue exploring the documentation to learn about deployments, API keys, security, and organization management.



================================================================================
FILE: orgs/admin.mdx
================================================================================

---
title: "Organization Admin"
description: "Central management for Owners/Admins"
---

## What you can do

- Switch the active Organization.
- View members, invite users, and change roles.
- Configure visibility, licensing, and integrations (e.g., GitHub).
- Review usage and plan details.

## Access

- Open Profile → Organization → Overview / View.

> tip: Delegate routine tasks — Give day‑to‑day responsibilities to Admins; reserve sensitive changes (billing, ownership) for Owners.

================================================================================
FILE: orgs/members.mdx
================================================================================

---
title: "Members & Invitations"
description: "View, invite, manage roles, and cautions"
---

## View members

- Profile → Organization → View to see current members and roles.

## Invite users

- From the Organization view, Invite by email or provider identity.
- Assign a role (Member, Admin, or Owner transfer when appropriate).

## Manage roles

- Promote/demote users as responsibilities change.
- Remove users who no longer require access.

> warning: Owner transfers — Transferring ownership grants full control, including billing and user removal. Proceed carefully.

================================================================================
FILE: orgs/roles-permissions.mdx
================================================================================

---
title: "Roles & Permissions"
description: "Owner, Admin, Member and guidance"
---

Each user in an Organization has one of three roles.

| Role       | Permissions                                                       |
| ---------- | ----------------------------------------------------------------- |
| **Owner**  | Full access, including billing, GitHub linking, and user removal. |
| **Admin**  | Manage Nodes, Projects, and users (except removing Owners).       |
| **Member** | Create/edit/run Nodes; cannot manage users or billing.            |

## Guidance

- Keep the number of Owners small.
- Promote trusted Members to Admin for day‑to‑day management.
- Review roles quarterly.

> tip: Principle of least privilege — Grant only the access required to perform the job effectively.

================================================================================
FILE: accounts/admin.mdx
================================================================================

---
title: "Account Admin (Coming Soon)"
description: "Upcoming personal account management features"
---

An upcoming page to manage personal account settings beyond the profile:

- Notification preferences
- Connected identities
- Security controls

Stay tuned as we expand account‑level administration features.

================================================================================
FILE: quotas/account-specific.mdx
================================================================================

---
title: "Account‑Specific Quotas"
description: "Environments, variables/secrets, and custom alerts/evaluations"
---

## Dimensions

### Environments (per Organization)

- What: Max named environments (e.g., `dev`, `staging`, `prod`).
- Why: Keep management simple and costs predictable.
- Symptoms: Cannot create additional environments.

### Variables/Secrets (per environment)

- What: Total number of configuration entries.
- Why: Encourages consolidation and tidy configs.
- Symptoms: New entries rejected until you prune.

### Custom Alerts/Evaluations (per Project)

- What: Maximum number of alert rules/evaluation definitions.
- Why: Bounds cardinality and keeps monitoring snappy.
- Symptoms: Cannot add new rules until you archive old ones.

## Good hygiene

- Review Variables quarterly; remove dead flags and stale endpoints.
- Keep secrets in your chosen secure store; reference them, don’t duplicate.
- Tag Projects and Nodes with owners so quota notifications reach the right people.

> note: Naming convention — Adopt a consistent prefixing scheme (e.g., `SERVICE__ENV__KEY`) for Variables to avoid collisions and speed up discovery.

================================================================================
FILE: quotas/api-integrations.mdx
================================================================================

---
title: "API & Integrations Quotas"
description: "Request rate, endpoint concurrency, schedules, and publishes"
---

## Dimensions

### API Request Rate

- What: Requests per second/minute to exposed APIs.
- Why: Guards latency and stability.
- Symptoms: HTTP 429 (Too Many Requests); advise retry with backoff.

### Endpoint Concurrency

- What: Max in‑flight requests per exposed top‑level Node.
- Why: Prevents hot‑spot overload on a single endpoint.
- Symptoms: Requests queue or fail fast.

### External Trigger Frequency (Schedules)

- What: Minimum interval between scheduled invocations.
- Why: Limits fan‑out and repeated runs.
- Symptoms: Skipped ticks or delayed triggers.

### Builds / Publish Actions

- What: Limit on number of build/publish operations per window.
- Why: Encourages batching of changes.
- Symptoms: Further publishes are deferred or rejected.

## Resilience patterns

- Implement retry with jitter on 429s (e.g., exponential backoff).
- Use idempotency keys to avoid duplicate processing on retries.
- Add circuit breakers around hot paths that call expensive tools.

> tip: Load shaping — Use lightweight validation Nodes at API edges to reject bad requests early and keep downstream capacity for real work.

================================================================================
FILE: quotas/execution-based.mdx
================================================================================

---
title: "Execution‑Based Quotas (Organization)"
description: "Dimensions, symptoms, and practical strategies"
---

## Dimensions

### Total Executions (per window)

- What: The count of Action/Flow runs initiated within a rolling window.
- Why: Protects the platform from unbounded load.
- Symptoms: New runs are rejected until the window resets.

### Concurrent Executions (max in‑flight)

- What: The number of executions running at the same moment.
- Why: Prevents saturation and keeps latency predictable.
- Symptoms: Additional runs queue or fail fast with a concurrency error.

### Max Execution Duration (per run)

- What: The upper bound on runtime of a single Action/Flow.
- Why: Avoids hangs and runaway costs.
- Symptoms: The run is terminated when the limit is reached.

### Flow Depth (graph depth per run)

- What: The number of sequential Node steps allowed in one Flow execution.
- Why: Encourages composition and prevents overly deep graphs.
- Symptoms: Validation error before execution if the graph exceeds the limit.

### Recursion Depth (self/nested re‑entry)

- What: The maximum allowed nested calls (Flows calling Flows/Agents recursively).
- Why: Stops infinite recursion and explosion of work.
- Symptoms: The triggering call is blocked beyond the threshold.

## Practical strategies

- Batch & Bound: Break large inputs into lists; toggle Parallel on the consumer Node.
- Short‑circuit: Validate inputs early and exit fast on no‑op paths.
- Compose shallow: Prefer sub‑Flows for reuse but keep individual run paths short.
- Time‑box: Use conservative duration expectations in Actions; handle partial progress gracefully.

> note: Reproduction aid — Save Payloads that reflect your highest‑throughput and longest‑duration cases so you can validate changes against quota‑sensitive scenarios.

================================================================================
FILE: quotas/index.mdx
================================================================================

---
title: "Quotas (beta)"
description: "Categories, enforcement, monitoring, and design tips"
---

Quotas define the operational boundaries of your Organization, Projects, and components. They exist to provide predictability (know what will run), fairness (shared resources), and safety (avoid runaway executions).

While specific thresholds may vary by plan or tier, this section explains each quota dimension, how usage is counted, typical symptoms when you hit limits, and ways to design within constraints.

## Quota categories

- Execution‑Based (Organization) — totals and concurrency of runs.
- Resource‑Based — payload sizes, script sizes, stored artifacts.
- API & Integrations — request rates, schedules, publish/build operations.
- Account‑Specific — environments, variables/secrets, custom alerts/evaluations.

## How limits are enforced

- Hard limits: Requests are rejected (e.g., HTTP 429 or a validation error).
- Soft limits: You receive warnings and are encouraged to reduce usage.
- Windows: Most rolling counters reset after a defined time window.

## Monitoring your usage

- Inspect execution history and metrics to understand run counts and durations.
- Review project surfaces (APIs/Schedules) for request rates and failures.
- Keep a small dashboard Flow to periodically snapshot usage into a log or table.

> tip: Design for headroom — Target 70–80% of allowed limits under peak load so you can absorb traffic spikes without breaching quotas.

## When you hit a limit

- Backoff and retry (exponential) for rate‑limit errors.
- Reduce payload sizes or split work into batches.
- Consider parallel fan‑out with smaller items rather than single large payloads.
- Upgrade your plan if sustained traffic requires more capacity.

================================================================================
FILE: quotas/resource-based.mdx
================================================================================

---
title: "Resource‑Based Quotas"
description: "Payload size, script size, stored payloads, module dependencies"
---

These quotas constrain the size and number of artifacts that executions read and write.

## Dimensions

### Payload Size (input/output)

- What: Max size for input requests and produced outputs.
- Why: Keeps requests responsive and storage efficient.
- Symptoms: Validation error on submit or truncated output rejection.

### Script Size (per Action)

- What: Limit on `Action.py` and supporting code footprint.
- Why: Ensures fast packaging and predictable build times.
- Symptoms: Build fails with size error.

### Stored Payloads (per Node)

- What: Number of saved example Payloads retained per Node.
- Why: Encourages curation of high‑value test cases.
- Symptoms: Cannot save additional Payloads until you prune.

### Module Size / Dependencies

- What: Limit on Module package size and number of dependencies.
- Why: Keeps deployments lightweight and secure.
- Symptoms: Module publish fails; dependency validation error.

## Design patterns

- Stream or paginate external data instead of sending mega‑payloads.
- Summarize or hash large intermediate artifacts; persist the source elsewhere.
- Refactor helpers into Modules to avoid duplicating code across Actions.
- Curate Payloads: keep `minimal`, `typical`, and a few `edge_case_*` variants.

> warning: Binary dependencies — Only pure‑Python dependencies are supported for Actions/Modules. Packages requiring binary system installs are not available in the build image.

================================================================================
FILE: security/compliance.mdx
================================================================================

---
title: "Compliance"
description: "Standards, certifications, and regulatory compliance"
---

## Overview

Triform is committed to meeting industry standards and regulatory requirements to protect your data and ensure service reliability.

## Current compliance status

### GDPR (General Data Protection Regulation)

**Status:** ✅ Compliant

**Scope:** All users in the European Union

**Key practices:**
- Lawful basis for processing (contract, consent, legitimate interest)
- Data minimization (collect only what's needed)
- Purpose limitation (use data only for stated purposes)
- Storage limitation (retain only as long as necessary)
- Security measures (encryption, access control)
- Data subject rights (access, deletion, portability)
- Cross-border transfer safeguards (Standard Contractual Clauses)

**Your rights under GDPR:**
- **Right to access** — Request a copy of your data
- **Right to rectification** — Correct inaccurate data
- **Right to erasure** — "Right to be forgotten"
- **Right to portability** — Export your data
- **Right to object** — Object to certain processing
- **Right to restrict** — Limit how we use your data

**Exercise rights:** Account Settings → Privacy or email privacy@triform.ai

**DPO contact:** dpo@triform.ai

### CCPA (California Consumer Privacy Act)

**Status:** ✅ Compliant

**Scope:** California residents

**Key practices:**
- Disclosure of data collection and use
- Opt-out of data "sales" (we don't sell data)
- Do Not Sell My Personal Information
- Equal service regardless of privacy choices
- Access and deletion rights

**Your rights under CCPA:**
- **Right to know** — What data we collect and how it's used
- **Right to delete** — Request deletion of your data
- **Right to opt-out** — Opt out of "sales" (not applicable—we don't sell)
- **Right to non-discrimination** — Equal service regardless

**Exercise rights:** Email privacy@triform.ai

**Verification:** We'll verify your identity before fulfilling requests

### SOC 2 Type II

**Status:** 🟡 Planned (2026)

**Scope:** Trust Services Criteria

**Framework:**
- **Security** — Protection against unauthorized access
- **Availability** — Service uptime and reliability
- **Processing integrity** — Complete, accurate, timely processing
- **Confidentiality** — Protection of confidential information
- **Privacy** — Collection, use, retention, disclosure aligned with commitments

**What this means:**
- Independent audit of our security controls
- Verification of implementation over time (6-12 months)
- Report available to customers upon request (when complete)

### ISO 27001

**Status:** 🟡 Planned (2026)

**Scope:** Information Security Management System (ISMS)

**Framework:**
- Risk assessment and treatment
- Security policies and procedures
- Asset management
- Access control
- Cryptography
- Physical and environmental security
- Operations security
- Communications security
- Supplier relationships
- Incident management
- Business continuity
- Compliance

**What this means:**
- International standard for information security
- Comprehensive security management
- Regular audits and continuous improvement

## Data protection measures

### Technical controls

**Encryption:**
- AES-256 at rest
- TLS 1.2+ in transit
- Encrypted backups

**Access control:**
- Role-based access (RBAC)
- Session management
- IP allowlisting (Enterprise)

**Network security:**
- Firewalls and network segmentation
- DDoS protection
- Intrusion detection
- Vulnerability scanning

**Code execution:**
- Sandboxed environments
- Resource limits
- Dependency scanning
- Static analysis

### Organizational controls

**Policies:**
- Information security policy
- Data retention policy
- Incident response plan
- Business continuity plan

**Vendor management:**
- Security questionnaires
- Contract reviews
- Regular assessments

**Physical security:**
- Data centers with 24/7 security
- Biometric access controls
- Video surveillance
- Environmental controls

## Sub-processors

We use trusted third-party services:

### Infrastructure

**Scaleway**
- **Service:** Cloud infrastructure
- **Data processed:** All customer data
- **Location:** France, EU/APAC
- **Compliance:** SOC 2, ISO 27001, GDPR, many others
- **DPA:** Standard Contractual Clauses

**6G AI Sweden**
- **Service:** Inference
- **Data processed:** Data sent through Agents
- **Location:** Sweden
- **Compliance:** SOC 2, ISO 27001, GDPR
- **DPA:** Standard Contractual Clauses

### Payment processing

**Polar**
- **Service:** Payment processing, Record of Merchant
- **Data processed:** Payment methods, billing info
- **Location:** Sweden
- **Compliance:** PCI DSS Level 1, SOC 2, ISO 27001, GDPR
- **DPA:** Standard Contractual Clauses

**Full list:** Available upon request to compliance@triform.ai

## Data Processing Agreement (DPA)

For GDPR and other regulations, we offer a Data Processing Agreement.

**What's included:**
- Roles and responsibilities (controller vs. processor)
- Data processing terms
- Security measures
- Sub-processor list
- Data breach notification procedures
- Audit rights
- Data deletion procedures
- Standard Contractual Clauses (for EU transfers)

**How to request:**
1. Email compliance@triform.ai
2. Provide Organization name and contact
3. We'll send DPA for review
4. Both parties sign
5. DPA effective upon execution

**Available to:** All Pro and Enterprise customers

## Audits and certifications

### Internal audits

- Quarterly security reviews
- Annual risk assessments
- Continuous penetration testing
- Code security reviews
- Access reviews

### External audits

- SOC 2 audit (planned)
- ISO 27001 (planned)
- Penetration tests (annual)
- Third-party security assessments

### Bug bounty program

**Status:** Coming soon (2026)

**Scope:** Responsible disclosure program

**Details:** Will be announced on our security page

## Regulatory response

### Data breach notification

**EU (GDPR):** Within 72 hours to supervisory authority, without undue delay to affected individuals

**California (CCPA):** Without unreasonable delay

**Other jurisdictions:** Per applicable law

**Our commitment:**
- Investigate thoroughly
- Notify promptly
- Provide clear information
- Assist with mitigation

## Industry-specific guidance

### Healthcare

**If you're in healthcare:**
- Use de-identified or anonymized data
- Implement additional access controls
- Document your compliance approach

### Financial services

**If you're in finance:**
- Use encryption (automatic)
- Enable audit logging (automatic)
- Implement access reviews
- Document your data flows

### Education

**If you're in education:**
- Minimize student data collection
- Use access controls (automatic)
- Review who has access regularly
- Implement data retention policies

### Government

**If you're in government:**
- Document security practices
- Consider on-premises options (contact sales)

## Requesting compliance documentation

**What's available:**
- Security whitepaper
- DPA (Data Processing Agreement)
- Sub-processor list
- SOC 2 report (when complete)
- Custom security questionnaires

**How to request:**
1. Email compliance@triform.ai
2. Specify what you need
3. Provide Organization name
4. Include your contact information

**NDA required for:** SOC 2 reports, detailed security architecture

**Response time:** 5 business days

## Attestations

We can provide attestations for:

- Data encryption practices
- Access control measures
- Backup and recovery procedures
- Incident response capabilities
- Business continuity planning

**Contact:** compliance@triform.ai

## FAQs

**Q: Where is my data stored?**  
A: EU by default. EU and APAC options for Enterprise.

**Q: Do you sell my data?**  
A: No, we never sell customer data.

**Q: Do you support on-premises deployment?**  
A: Contact sales for availability.

## Contact

**Compliance inquiries:** compliance@triform.ai  
**Privacy questions:** privacy@triform.ai  
**Security concerns:** security@triform.ai  
**DPO (Data Protection Officer):** dpo@triform.ai

**Response time:** 5 business days for compliance requests

## Related

Continue exploring the security documentation to learn about security overview, data retention, and compliance measures.



================================================================================
FILE: security/data-retention.mdx
================================================================================

---
title: "Data Retention"
description: "How long we store your data and how to manage it"
---

## Overview

Triform retains different types of data for varying periods based on product functionality, legal requirements, and user preferences.

## Retention periods

### Project data

**What:** Projects, Actions, Agents, Flows, code, configurations

**Retention:** Indefinite while Project is active

**After deletion:** 30-day soft delete, then permanent

**Purpose:** Core product functionality

**User control:** Delete Projects anytime

### Execution data

**What:** Execution records, inputs, outputs, logs, traces

| Tier | Retention |
|------|-----------|
| Free | 7 days |
| Pro | 30 days |
| Enterprise | 90 days or custom |

**After period:** Execution details deleted, summary statistics retained

**Purpose:** Debugging, monitoring, compliance

**User control:** Export before deletion, star important executions for longer retention

### Project Variables

**What:** Configuration values, secrets, API keys

**Retention:** Indefinite while active

**After deletion:** Immediate permanent deletion (no soft delete)

**Purpose:** Application configuration

**User control:** Delete Variables anytime

**Note:** Secrets deleted from our systems within 24 hours (key rotation cycle)

### API keys

**What:** API key metadata (not the secret itself)

**Retention:** Active until revoked or expired

**After revocation:** 90 days for audit purposes

**Purpose:** Authentication, audit trails

**User control:** Revoke anytime

**Note:** The secret key itself is hashed and cannot be recovered

### Audit logs

**What:** Security events, access logs, changes

**Retention:** 90 days (standard), 1+ year (Enterprise)

**Purpose:** Security, compliance, debugging

**User control:** Export anytime

**Compliance:** May be required longer for regulated industries

### User data

**What:** Account info, profile, email, preferences

**Retention:** While account is active

**After deletion:** 30-day soft delete, then permanent

**Purpose:** Account management, communication

**User control:** Delete account anytime

### Payment data

**What:** Billing history, invoices, payment methods

**Retention:** 7 years

**Purpose:** Legal/tax requirements, dispute resolution

**User control:** Cannot be deleted (legal obligation)

**Note:** We don't store full credit card numbers (tokenized via payment processor)

### Analytics data

**What:** Aggregated usage statistics, performance metrics

**Retention:** 2 years

**Purpose:** Product improvement, capacity planning

**User control:** Anonymized, cannot be deleted

**Note:** No personally identifiable information

## Soft deletion

Some data types use soft deletion for recovery.

### How it works

1. **Mark as deleted** — Data hidden from UI and API
2. **Grace period** — 30 days to recover
3. **Permanent deletion** — After grace period

### What uses soft deletion

✅ **Projects** — 30-day recovery window  
✅ **User accounts** — 30-day recovery window  
❌ **Project Variables** — Immediate deletion (security)  
❌ **API keys** — Immediate revocation  
❌ **Executions** — Immediate deletion after retention period

### Recovery process

**Within grace period:**
1. Contact support@triform.ai
2. Provide Project/account ID
3. Confirm identity
4. We'll restore within 24 hours

**After grace period:** Cannot be recovered

## Data minimization

We collect and retain only what's necessary.

### What we collect

✅ **Necessary for service:**
- Project code and config
- Execution data for debugging and AI model training (only free community version)
- Account info for authentication
- Billing info for payments

❌ **Not collected:**
- Personal data beyond account basics
- Tracking cookies (beyond essential)
- Browsing history outside Triform
- Third-party service credentials (unless you provide in Project Variables)

### Anonymization

**Analytics and training data** is anonymized:
- User IDs replaced with random identifiers
- IP addresses hashed
- Names and emails removed
- Aggregated only (no individual tracking)

## Geographic data storage

### Data residency

**Standard:** EU-based data centers

**Data localization:** Customer data stays in EU.

### Data transfer

**Within region:** No cross-border transfer

**GDPR compliance:** Data stored in EU

## Deletion procedures

### Deleting Projects

1. Project menu → **Delete**
2. Confirm by typing Project name
3. Project soft-deleted
4. 30-day recovery window
5. Permanent deletion after

**What happens:**
- Project removed from UI
- Executions deleted per retention policy
- Audit logs retained

### Deleting account

1. Account Settings → Privacy → **Delete Account**
2. Confirm by typing email
3. Account soft-deleted
4. 30-day recovery window
5. Permanent deletion after

**What happens:**
- Account deactivated immediately
- Projects you own deleted (after grace period)
- Shared Projects remain (your access removed)
- Payment history retained (legal requirement)
- Audit logs retained (90 days)

### Deleting Organization

1. Organization Settings → **Delete Organization**
2. Requires Admin role
3. Confirm by typing Organization name
4. All Projects deleted
5. All members removed
6. Billing canceled

**Cannot delete if:**
- You're not the last Admin (transfer ownership first)
- Active subscription (cancel first)
- Recent executions (wait for retention period)

## Data breach procedures

If a data breach occurs:

### Our response

**Within 72 hours:**
1. Identify scope and affected data
2. Contain the breach
3. Notify affected users
4. File regulatory reports (if required)

**Notification includes:**
- What data was affected
- How many users/records
- What we've done to fix it
- What you should do (e.g., rotate keys)

### User actions

**If notified of a breach:**
1. **Rotate API keys** immediately
2. **Update Project Variables** (especially secrets)
3. **Review audit logs** for suspicious activity
4. **Enable 2FA** if not already
5. **Monitor for unusual activity**

## Compliance

### GDPR (EU)

**Rights you have:**
- **Access:** Request copy of your data
- **Rectification:** Correct inaccurate data
- **Erasure:** "Right to be forgotten"
- **Portability:** Export in machine-readable format
- **Object:** Object to processing

**Exercise rights:** Account Settings → Privacy or email privacy@triform.ai

### Other regulations

We comply with applicable data protection laws in EU.

## FAQs

**Q: Can I extend execution retention?**  
A: Enterprise plans offer custom retention. Contact sales.

**Q: What happens to my data if Triform shuts down?**  
A: We'll provide at least 90 days notice and export tools.

**Q: Are backups deleted too?**  
A: Yes, backups are deleted per the same schedule.

**Q: Can I request early deletion?**  
A: Yes, contact support for manual deletion requests.

**Q: Is deleted data truly unrecoverable?**  
A: After permanent deletion, yes. We use secure deletion methods.

## Contact

**Data requests:** privacy@triform.ai  
**Security concerns:** security@triform.ai  
**General support:** support@triform.ai

**Response time:** 5 business days for data requests, 24 hours for security issues

## Related

Continue exploring the security documentation to learn about compliance and data protection measures.



================================================================================
FILE: security/overview.mdx
================================================================================

---
title: "Security Overview"
description: "How Triform protects your data and systems"
---

## Our commitment to security

Security is fundamental to Triform. We implement industry-standard practices to protect your Projects, data, and credentials.

## Data encryption

### At rest

All data stored in Triform is encrypted using **AES-256** encryption.

**Encrypted data includes:**
- Project code and configurations
- Execution logs and traces
- Project Variables
- API keys
- User information

**Key management:** Encryption keys are managed using industry-standard key management services, rotated regularly, and never stored alongside encrypted data.

### In transit

All communication with Triform uses **TLS 1.2+** encryption.

**Encrypted connections:**
- Web interface (HTTPS)
- API requests (HTTPS)
- Webhook callbacks (HTTPS)
- Database connections (encrypted)
- Internal service communication (mutual TLS)

**Certificate management:** We use trusted Certificate Authorities and monitor certificate expiration.

## Authentication

### Supported methods

**OAuth 2.0 providers:**
- Discord
- GitHub  

**Email:** Passwordless email authentication

**Benefits:**
- No password management burden
- Leverage provider security (2FA, etc.)
- Single sign-on capabilities

### Session management

**Security features:**
- Sessions expire after inactivity
- IP address validation (optional)
- Device fingerprinting
- Concurrent session limits

**User controls:**
- View active sessions
- Revoke individual sessions
- Revoke all sessions (sign out everywhere)

### Two-factor authentication (2FA)

**Strongly recommended for:**
- Organization Admins
- Users with deployment permissions
- Users handling sensitive Projects

**Supported methods:**
- Authenticator apps (TOTP)
- SMS (where available)
- Backup codes

**Enforcement:** Organization Admins can require 2FA for all members.

## Authorization

### Role-based access control (RBAC)

**Organization roles:**
- **Admin** — Full access
- **Editor** — Create, edit, execute
- **Viewer** — Read-only

**Project-level permissions:**
- Fine-grained per-Project overrides
- Specific user grants
- Temporary access grants

### API key security

**Best practices enforced:**
- Keys are hashed in storage (irreversible)
- Keys shown only once upon creation
- Keys can be scoped to specific Projects
- Keys can be restricted by permissions
- Keys can expire automatically
- Key usage is logged

**User responsibilities:**
- Store keys securely (environment variables, secret managers)
- Rotate keys regularly (90-day recommendation)
- Revoke unused keys
- Never commit to version control

## Network security

### API security

**Protection mechanisms:**
- Rate limiting (prevents abuse)
- DDoS mitigation
- Input validation and sanitization

**Monitoring:**
- Anomaly detection
- Suspicious activity alerts
- Failed authentication tracking

### Webhook security

**Verification options:**
- API key authentication

**Replay attack prevention:**
- Timestamp validation
- Nonce tracking
- Request idempotency

## Code execution security

### Sandboxed execution

Actions run in isolated environments:

**Isolation features:**
- Separate containers per execution
- No persistent file system
- Limited network access
- Resource quotas (CPU, memory, time)
- No access to Triform internals

**What Actions CANNOT do:**
- Access other users' data
- Modify Triform infrastructure
- Persist data between executions (unless using provided storage)
- Execute arbitrary system commands
- Fork processes or spawn daemons

### Dependency security

**Automated scanning:**
- Check for known vulnerabilities in dependencies
- Alert on critical CVEs
- Suggest updates for vulnerable packages

**User responsibilities:**
- Keep dependencies updated
- Review security advisories
- Avoid deprecated packages
- Audit third-party packages

### Code review

**Recommended for sensitive Projects:**
- Peer review before deployment
- Automated security scanning
- Manual audit of high-risk components

## Vulnerability management

### Responsible disclosure

Found a security issue? We appreciate responsible disclosure.

**Reporting:**
- Email: security@triform.ai
- Include detailed description
- Steps to reproduce (if applicable)
- Potential impact assessment

**Our commitment:**
- Acknowledge within 24 hours
- Triage and investigate promptly
- Keep you informed of progress
- Credit you in our security updates (unless anonymous)

**Please don't:**
- Publicly disclose before we've patched
- Exploit the vulnerability
- Access other users' data

### Security updates

**Communication:**
- Critical issues: Email all users immediately
- High severity: Status page + email within 24 hours
- Medium/low: Included in regular updates

**Patching:**
- Critical vulnerabilities: Emergency patch within hours
- High severity: Patch within 7 days
- Medium/low: Patch in next regular release

## Incident response

### Detection

**Monitoring systems:**
- Failed authentication attempts
- Unusual API usage patterns
- Data exfiltration attempts
- Privilege escalation attempts

**Alerts trigger:** Automated response + human investigation

### Response process

1. **Detect & triage** — Identify and assess severity
2. **Contain** — Limit impact, isolate affected systems
3. **Investigate** — Determine root cause and scope
4. **Remediate** — Fix vulnerability, restore service
5. **Communicate** — Inform affected users
6. **Learn** — Post-mortem, improve processes

### User notification

**We notify if:**
- Data breach occurs
- Unauthorized access detected
- Service compromise affects your Projects

**Notification includes:**
- What happened
- What data was affected
- What we've done
- What you should do

## User security responsibilities

### Account security

> **Use strong passwords** — If using email authentication

> **Monitor sessions** — Review and revoke suspicious sessions

> **Keep email secure** — Your account recovery method

### API key security

> **Never commit keys to git** — Use environment variables

> **Rotate keys regularly** — Every 90 days recommended

> **Scope keys minimally** — Only grant needed permissions

> **Revoke unused keys** — Reduce attack surface

### Code security

> **Validate inputs** — Don't trust external data

> **Sanitize outputs** — Prevent injection attacks

> **Review dependencies** — Check for vulnerabilities

> **Keep secrets in Project Variables** — Never hardcode

### Organization security

> **Audit members regularly** — Remove ex-employees

> **Follow least privilege** — Grant minimum necessary permissions

## Security features roadmap

**Coming soon:**
- SSO/SAML support (Enterprise)
- Advanced threat detection
- Custom security policies
- Bring Your Own Key (BYOK) encryption
- Enhanced audit logging
- Security compliance dashboards

## Security resources

### Contact

**Security issues:** security@triform.ai  
**General support:** Join our [Discord community](https://discord.gg/triform)

### External resources

- [OWASP Top 10](https://owasp.org/Top10/)
- [Cloud Security Alliance](https://cloudsecurityalliance.org/)
- [NIST Cybersecurity Framework](https://www.nist.gov/cyberframework)

## Questions?

If you have security questions or concerns:

1. Review the security documentation
2. Contact security@triform.ai
3. For Enterprise, contact your account manager



================================================================================
FILE: api-reference/endpoint/create.mdx
================================================================================

---
title: 'Create Plant'
openapi: 'POST /plants'
---


================================================================================
FILE: api-reference/endpoint/delete.mdx
================================================================================

---
title: 'Delete Plant'
openapi: 'DELETE /plants/{id}'
---


================================================================================
FILE: api-reference/endpoint/get.mdx
================================================================================

---
title: 'Get Plants'
openapi: 'GET /plants'
---


================================================================================
FILE: api-reference/endpoint/webhook.mdx
================================================================================

---
title: 'New Plant'
openapi: 'WEBHOOK /plant/webhook'
---


================================================================================
FILE: api-reference/introduction.mdx
================================================================================

---
title: 'Introduction'
description: 'Example section for showcasing API endpoints'
---

<Note>
  If you're not looking to build API reference documentation, you can delete
  this section by removing the api-reference folder.
</Note>

## Welcome

There are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.

<Card
  title="Plant Store Endpoints"
  icon="leaf"
  href="https://github.com/mintlify/starter/blob/main/api-reference/openapi.json"
>
  View the OpenAPI specification file
</Card>

## Authentication

All API endpoints are authenticated using Bearer tokens and picked up from the specification file.

```json
"security": [
  {
    "bearerAuth": []
  }
]
```


================================================================================
FILE: roadmap/alerts.mdx
================================================================================

---
title: "Alerts — Backlog"
description: "Rules, channels, policies, examples, and planned UX"
---

Receive proactive notifications when critical conditions occur so you can respond quickly.

## Target capabilities

- Rules: Define conditions on logs, metrics, or evaluation scores.
- Channels: Send to email, chat, and webhooks.
- Policies: Group, dedupe, and throttle noisy signals.
- Runbooks: Link alerts to remediation steps or dashboards.

## Example rules

- Error rate for a top‑level Flow exceeds N% over M minutes.
- Evaluation score falls below threshold for k consecutive runs.
- Latency p95 exceeds T ms after a deploy.

## Planned UX

- Create/edit rules in Project settings.
- Preview firing conditions with recent data.
- Silence windows for planned maintenance.

## Next steps

- Provider integrations (email, chat, webhooks).
- Alert templates for common issues.
- Ownership & on‑call escalation options.

> note: Keep it actionable — Alerts should point to a concrete next step—include links, IDs, and a brief “what changed” summary.

================================================================================
FILE: roadmap/chat-client.mdx
================================================================================

---
title: "Chat Client"
description: "Conversational interface for end-users (Coming Soon)"
---

## Overview

A ready-to-use chat interface component for deploying conversational Agents directly to your users.

**Status:** 🔮 Planned for Q1 2026

## What is the Chat Client?

A customizable, embeddable chat widget that connects directly to your Triform Agents, enabling end-users to interact with your AI systems through natural conversation.

## Key Features

### Embeddable Widget

**Easily integrate into any website:**
- Single line of JavaScript
- Customizable styling
- Mobile-responsive
- Supports iframes and native integration

### Conversation Management

**Handle multi-turn conversations:**
- Message history
- Context persistence
- Session management
- Conversation branching

### Rich Media Support

**Beyond text:**
- Images and files
- Code snippets with syntax highlighting
- Tables and structured data
- Interactive buttons and forms
- Link previews

### Customization

**Brand it your way:**
- Custom colors and themes
- Logo and avatar
- Welcome messages
- Suggested prompts
- Custom CSS

### Security & Privacy

**Built-in safeguards:**
- Rate limiting
- Content filtering
- User authentication (optional)
- PII redaction
- Conversation encryption

## Use Cases

**Customer Support Bot**
- Embedded on help pages
- Answers common questions
- Escalates to human when needed

**Product Assistant**
- Guides users through features
- Provides personalized recommendations
- Collects feedback

**Internal Tools**
- Employee knowledge base
- IT support chatbot
- HR assistant

**E-commerce**
- Product recommendations
- Order tracking
- Shopping assistance

## Example Integration

```html
<!-- Add to your website -->
<script src="https://cdn.triform.ai/chat-client.js"></script>
<script>
  TriformChat.init({
    agentId: 'agent_abc123',
    apiKey: 'tri_pk_...',
    theme: 'light',
    position: 'bottom-right',
    welcomeMessage: 'Hi! How can I help you today?'
  });
</script>
```

## Configuration Options

```javascript
{
  // Required
  agentId: 'agent_abc123',
  apiKey: 'tri_pk_...',
  
  // Styling
  theme: 'light' | 'dark' | 'auto',
  primaryColor: '#432dd7',
  position: 'bottom-right' | 'bottom-left',
  
  // Behavior
  welcomeMessage: 'Hello! How can I help?',
  suggestedPrompts: [
    'How do I get started?',
    'What features are available?',
    'Contact support'
  ],
  
  // Features
  enableFileUpload: true,
  enableFeedback: true,
  maxMessageLength: 2000,
  
  // Privacy
  enableAnalytics: true,
  anonymizeUsers: false
}
```

## Mobile App Support

**Native SDKs** for iOS and Android:
- Swift package for iOS
- Kotlin library for Android
- React Native component
- Flutter plugin

## Analytics & Insights

**Track usage:**
- Conversation volume
- User satisfaction (thumbs up/down)
- Common questions
- Drop-off points
- Response times

## Pricing

**Included in all plans** — No additional cost

**Usage limits:**
- Free: 1,000 messages/month
- Pro: 50,000 messages/month
- Enterprise: Unlimited

## Timeline

**Q1 2026:** Beta release  
**Q2 2026:** General availability  
**Q3 2026:** Mobile SDKs  
**Q4 2026:** Advanced features (voice, video)

## Get Notified

Want early access?

**Sign up:** [triform.ai/chat-client-beta](https://triform.ai/chat-client-beta)

We'll notify you when beta is available and provide documentation and examples.

## Questions?

- [Join Discord](https://discord.gg/triform) to discuss
- Email: product@triform.ai



================================================================================
FILE: roadmap/community-library.mdx
================================================================================

---
title: "Community Library — Upcoming"
description: "Goals, submission guidelines, review, roadmap"
---

## Goals

- Provide ready‑to‑use Actions and Flows for common tasks.
- Showcase exemplars with transparent performance metrics.
- Encourage reuse and consistent patterns.

## Submission guidelines (draft)

- Clear README with intent, I/O, and examples.
- Minimal, pure‑Python dependencies.
- Payloads for minimal, typical, and edge cases.
- License compatibility and attribution.

## Review & quality bar

- Automated checks (lint, size, dependency hygiene).
- Human review for clarity and correctness.
- Versioning and changelog requirements.

## Roadmap

- Launch starter set (text preprocessing, validation, routing).
- Add rating and usage signals.
- Introduce “trusted” badges for vetted components.

> tip: Design to be remixed — Keep components small, well‑named, and composable so others can slot them into their Flows with minimal changes.

================================================================================
FILE: roadmap/custom-dashboards.mdx
================================================================================

---
title: "Custom Dashboards"
description: "Build personalized monitoring dashboards (Coming Soon)"
---

## Overview

Create custom dashboards to monitor your Projects, track metrics, and visualize data exactly the way you need.

**Status:** 🔮 Planned for Q4 2026

## What are Custom Dashboards?

Flexible, customizable views combining charts, metrics, logs, and execution data into personalized monitoring interfaces.

## Key Features

### Drag-and-Drop Builder

**Visual dashboard creation:**
- Drag widgets onto canvas
- Resize and arrange freely
- No code required
- Real-time preview

### Widget Library

**Pre-built components:**
- **Line chart** — Time series data
- **Bar chart** — Comparisons
- **Pie chart** — Proportions
- **Table** — Raw data
- **Number** — Single metric (big number)
- **Gauge** — Visual indicator with thresholds
- **Heatmap** — Distribution over time
- **Markdown** — Custom text and links

### Data Sources

**Pull from multiple sources:**
- Execution metrics (success rate, duration)
- Custom metrics (your own tracked metrics)
- Logs (error counts, patterns)
- Cost data (LLM usage, API calls)
- External APIs (via Actions)

### Time Controls

**Flexible time ranges:**
- Last 5 minutes → Last 30 days
- Custom date range
- Real-time (auto-refresh)
- Compare time periods

### Variables

**Make dashboards dynamic:**
```
Project: [dropdown selector]
Environment: [staging | production]
Time range: [last 24 hours]
```

**Widgets update based on variable selection**

### Templates

**Start quickly:**
- Production Overview
- Cost Monitoring
- Quality Metrics
- Customer Support KPIs
- Developer Dashboard
- Executive Summary

**Or create from scratch**

## Example Dashboards

### Production Monitoring Dashboard

**Row 1:**
- Request rate (line chart)
- Error rate (gauge, red if >5%)
- Avg response time (number, ms)

**Row 2:**
- Success rate by Project (bar chart)
- Error types (pie chart)
- Recent errors (table)

**Row 3:**
- Execution timeline (area chart)
- Cost by Project (stacked bar)
- Active users (line chart)

### Cost Analysis Dashboard

**Row 1:**
- Total cost today (big number)
- vs. yesterday (% change)
- Daily cost trend (line chart)

**Row 2:**
- Cost by Project (pie chart)
- Cost by LLM model (bar chart)
- Token usage trend (area chart)

**Row 3:**
- Top expensive executions (table)
- Cost forecast (line chart)
- Budget vs. actual (gauge)

### Agent Quality Dashboard

**Row 1:**
- Avg response quality (gauge)
- User satisfaction (line chart)
- Thumbs up/down ratio (pie chart)

**Row 2:**
- Response time distribution (histogram)
- Tool usage (bar chart)
- Conversation volume (area chart)

**Row 3:**
- Recent feedback (table)
- Quality by Agent (comparison)
- Improvement over time (line chart)

## Sharing and Collaboration

### Share Dashboards

**With your team:**
- Share link (view-only or editable)
- Embed in Slack/Discord
- Email snapshots
- Public URLs (optional)

### Permissions

**Control access:**
- Organization members (all)
- Specific users
- Specific roles (Admin, Editor, Viewer)
- Public (anyone with link)

### Versioning

**Track changes:**
- Save dashboard versions
- Restore previous versions
- Compare changes
- Clone and modify

## Alerts from Dashboards

**Set up alerts directly:**
1. Click widget
2. "Create Alert from This"
3. Define threshold
4. Choose notification channel
5. Save

**Example:** Alert when error rate widget shows >5%

## Embedding

**Embed in your apps:**
```html
<iframe
  src="https://dashboards.triform.ai/embed/abc123?theme=dark"
  width="100%"
  height="600"
></iframe>
```

**Embed options:**
- Full dashboard or single widget
- Light/dark theme
- Auto-refresh interval
- Hide/show controls

## Mobile Support

**Responsive dashboards:**
- Automatic layout for mobile
- Swipe between sections
- Tap to drill down
- Push notifications for alerts

## API

**Programmatic access:**
```python
from triform import Dashboards

# Create dashboard
dashboard = Dashboards.create(
    name="My Dashboard",
    widgets=[
        {
            "type": "line_chart",
            "metric": "request_rate",
            "position": {"x": 0, "y": 0, "w": 6, "h": 4}
        },
        {
            "type": "gauge",
            "metric": "error_rate",
            "thresholds": [0, 1, 5],  # green, yellow, red
            "position": {"x": 6, "y": 0, "w": 3, "h": 4}
        }
    ]
)

# Get dashboard data
data = dashboard.get_data(
    start_time="2025-10-01",
    end_time="2025-10-02"
)

# Update dashboard
dashboard.add_widget({
    "type": "table",
    "query": "SELECT * FROM executions WHERE status='failed'"
})
```

## Export and Backup

**Export dashboards:**
- JSON format (import elsewhere)
- PDF snapshot (for reports)
- Image (PNG for sharing)
- CSV data (for analysis)

## Templates Gallery

**Pre-built dashboards:**
- Browse community templates
- Clone and customize
- Share your own
- Rate and review

## Advanced Features

### Drill Down

**Interactive exploration:**
- Click chart → See raw data
- Click metric → View logs
- Click execution → Full trace

### Annotations

**Mark important events:**
- Deployment markers
- Incident annotations
- Release notes
- Custom notes

### Comparison Mode

**Side-by-side comparison:**
- This week vs. last week
- Staging vs. production
- Agent v1 vs. v2
- Before vs. after changes

### Calculated Metrics

**Create derived metrics:**
```
conversion_rate = (successful_orders / total_orders) * 100
cost_per_success = total_cost / successful_executions
```

## Integrations

**Connect external data:**
- Google Analytics
- Stripe (revenue data)
- Custom APIs
- Databases (read-only)

## Use Cases

### DevOps Monitoring

**Track system health:**
- Uptime and availability
- Error rates and types
- Performance metrics
- Resource usage

### Product Analytics

**User behavior:**
- Feature usage
- User journey flows
- Conversion funnels
- Retention cohorts

### Executive Reporting

**High-level overview:**
- Business KPIs
- Cost trends
- User growth
- Revenue impact

### Team Collaboration

**Shared visibility:**
- Current sprint metrics
- Team performance
- Project status
- Blockers and issues

## Pricing

**Free tier:**
- 3 dashboards
- 10 widgets per dashboard
- Basic widgets
- Organization sharing only

**Pro tier:**
- 25 dashboards
- Unlimited widgets
- All widget types
- Public sharing
- Embedding

**Enterprise:**
- Unlimited dashboards
- Advanced features
- SSO for dashboard access
- White-label embedding
- Priority support

## Timeline

**Q4 2026:** Dashboard builder, basic widgets  
**2027 Q1:** Advanced widgets, templates  
**2027 Q2:** Mobile app, embedding

## Get Notified

**Sign up:** [triform.ai/dashboards-beta](https://triform.ai/dashboards-beta)

## Questions?

- [Join Discord](https://discord.gg/triform) #dashboards
- Email: product@triform.ai



================================================================================
FILE: roadmap/evaluations.mdx
================================================================================

---
title: "Evaluations"
description: "Test and validate Agent behavior systematically (Coming Soon)"
---

## Overview

Automated testing framework for Agents and Flows, enabling systematic evaluation of AI system quality, consistency, and performance.

**Status:** 🔮 Planned for Q2 2026

## What are Evaluations?

A comprehensive testing and benchmarking system that runs your Agents and Flows against test cases, measures quality, and tracks improvements over time.

## Key Features

### Test Suites

**Organize tests logically:**
- Group related test cases
- Run entire suites or individual tests
- Schedule regular runs
- Compare results across runs

### Test Cases

**Define expected behavior:**
```yaml
- name: "Customer greeting"
  input: "Hello, I need help"
  expected_output_contains: "How can I help you"
  expected_tone: "friendly"
  max_tokens: 100
```

### Evaluation Metrics

**Measure quality:**
- **Semantic similarity** — How close is output to expected?
- **Factual accuracy** — Are facts correct?
- **Tone matching** — Does tone match guidelines?
- **Safety** — No harmful content?
- **Latency** — Response time acceptable?
- **Cost** — Token usage within budget?

### A/B Testing

**Compare variants:**
- Test different prompts
- Compare models
- Evaluate tools configuration
- Measure impact of changes

### Regression Detection

**Catch quality drops:**
- Baseline established from passing tests
- Alert when quality degrades
- Track metrics over time
- Identify breaking changes

## Use Cases

**Prompt Engineering**
- Test prompt variations
- Measure impact of changes
- Find optimal configuration

**Model Comparison**
- GPT-4 vs Claude
- Cost vs quality tradeoffs
- Speed vs accuracy

**Quality Assurance**
- Verify Agent behavior
- Catch regressions
- Ensure consistency

**Continuous Improvement**
- Track progress over time
- Benchmark against goals
- Identify improvement areas

## Example Evaluation

```yaml
name: "Customer Support Bot Evaluation"
agent: "customer-support-agent"

test_cases:
  - name: "Polite greeting"
    input: "Hello"
    assertions:
      - contains: ["hello", "hi", "help"]
      - tone: "friendly"
      - max_tokens: 50
    
  - name: "Product question"
    input: "How much does Pro plan cost?"
    assertions:
      - contains: ["$49", "month", "Pro"]
      - factually_correct: true
      - tools_called: ["get_pricing"]
    
  - name: "Escalation"
    input: "I want to cancel my account immediately"
    assertions:
      - tools_called: ["escalate_to_human"]
      - tone: "empathetic"
      - contains: ["understand", "help"]

metrics:
  - semantic_similarity: 0.8  # 80% minimum
  - response_time: 3000  # 3 seconds max
  - cost_per_interaction: 0.05  # $0.05 max
```

## Evaluation Types

### Unit Tests

**Single component:**
- Test one Agent or Action
- Specific input/output
- Fast feedback

### Integration Tests

**Multiple components:**
- Test full Flows
- End-to-end scenarios
- Realistic workflows

### Regression Tests

**Prevent quality drops:**
- Run after every change
- Compare to baseline
- Alert on degradation

### Performance Tests

**Measure speed and cost:**
- Batch execution
- Latency under load
- Cost analysis

### Safety Tests

**Ensure responsible AI:**
- No harmful outputs
- No PII leakage
- No bias or toxicity

## Evaluation Dashboard

**Visual results:**
- Pass/fail rates
- Quality trends over time
- Cost and latency charts
- Failure analysis
- Comparison views

## Automated Runs

**Schedule evaluations:**
- After every deployment
- Daily/weekly cron
- Before production push
- On-demand via API

## CI/CD Integration

**Block bad deployments:**
```yaml
# GitHub Actions
- name: Run Triform Evaluations
  run: triform eval run --suite customer-support --require-pass

- name: Deploy if evaluations pass
  if: success()
  run: triform deploy
```

## Evaluation API

**Programmatic access:**
```python
from triform import Evaluations

# Create evaluation
eval = Evaluations.create(
    agent_id="agent_abc123",
    test_suite="customer_support_v1"
)

# Run evaluation
result = eval.run()

# Check results
if result.pass_rate >= 0.95:
    print("Quality threshold met!")
else:
    print(f"Failed: {result.failures}")
```

## Pricing

**Included in Pro and Enterprise plans**

**Usage:**
- Evaluations run as regular executions
- Count toward execution quota
- No additional cost beyond execution fees

## Timeline

**Q2 2026:** Beta release with basic test cases  
**Q3 2026:** Advanced metrics and A/B testing  
**Q4 2026:** CI/CD integrations and automation

## Get Notified

**Sign up:** [triform.ai/evaluations-beta](https://triform.ai/evaluations-beta)

## Questions?

- [Join Discord](https://discord.gg/triform) #evaluations channel
- Email: product@triform.ai



================================================================================
FILE: roadmap/logs.mdx
================================================================================

---
title: "Advanced Logging"
description: "Enhanced logging capabilities and log management (Coming Soon)"
---

## Overview

Advanced logging infrastructure with structured logs, powerful search, log streaming, and integrations with external log management systems.

**Status:** 🔮 Planned for Q3 2026

## Current State

Today, Triform provides basic logging:
- Execution logs viewable in UI
- Print statements captured from Actions
- Error messages and stack traces
- Retention per plan tier

## What's Coming

### Structured Logging

**Beyond plain text:**
```python
# Current (plain text)
print("User logged in")

# Future (structured)
log.info("user_logged_in", {
    "user_id": "user_123",
    "ip": "192.0.2.1",
    "timestamp": "2025-10-01T10:30:00Z"
})
```

**Benefits:**
- Searchable by field
- Aggregate and analyze
- Create dashboards
- Set up alerts

### Log Levels

**Standard severity levels:**
- **TRACE:** Most detailed
- **DEBUG:** Debugging information
- **INFO:** General information
- **WARN:** Warnings
- **ERROR:** Errors
- **FATAL:** Critical failures

**Control verbosity per environment:**
- Dev: DEBUG level
- Staging: INFO level
- Production: WARN level

### Advanced Search

**Powerful queries:**
```
level:ERROR AND user_id:user_123 AND timestamp:>2025-10-01
```

**Search by:**
- Log level
- Timestamp range
- Component (Action, Agent, Flow)
- Custom fields
- Free text
- Regular expressions

### Log Aggregation

**Group and count:**
- Errors by type
- Actions by execution count
- Users by activity
- Cost by component

### Log Streaming

**Real-time logs:**
- Tail logs live
- WebSocket streaming
- Server-Sent Events
- Filter while streaming

### Log Exports

**Send logs elsewhere:**
- **Datadog:** APM and logs
- **Splunk:** Enterprise log management
- **CloudWatch:** AWS logs
- **Elasticsearch:** Search and analytics
- **Custom webhook:** Your own system

**Configuration:**
```yaml
log_export:
  provider: datadog
  api_key: ${DATADOG_API_KEY}
  filters:
    - level: ERROR
    - level: WARN
```

### Log Context

**Automatic enrichment:**
- Trace ID
- Execution ID
- User ID
- Project ID
- Environment
- Git commit (if integrated)

### Sampling

**Control volume:**
- Log all in dev
- Sample in production
- Always log errors
- Smart sampling (log slow requests)

### Log Retention Policies

**Custom retention:**
```yaml
retention:
  ERROR: 90 days
  WARN: 60 days
  INFO: 30 days
  DEBUG: 7 days
```

## Use Cases

### Debugging

**Find issues quickly:**
```
level:ERROR AND component:payment_processor AND timestamp:last_hour
```

### Monitoring

**Track system health:**
- Error rate trends
- Slow request identification
- Resource usage patterns

### Analytics

**Business insights:**
- User behavior analysis
- Feature usage tracking
- Performance metrics

### Compliance

**Audit trails:**
- Who accessed what
- When and from where
- What actions were taken

### Cost Attribution

**Track spending:**
- LLM token usage by component
- API calls by user
- Execution costs by Project

## Log API

```python
from triform import Logger

# In your Action
log = Logger()

log.debug("Starting processing", {
    "item_count": len(items)
})

log.info("Processing complete", {
    "processed": 42,
    "failed": 2,
    "duration_ms": 1234
})

log.error("Failed to process item", {
    "item_id": "item_123",
    "error": str(e)
})

# Query logs
logs = Logger.query(
    level="ERROR",
    start_time="2025-10-01",
    filters={"component": "payment_processor"}
)
```

## Log Dashboard

**Visual log analysis:**
- Recent logs table
- Error rate chart
- Log volume by level
- Top error messages
- Slowest executions

## Alerts on Logs

**Get notified:**
```yaml
alert:
  name: "High error rate"
  condition: "count(level:ERROR) > 10 in 5 minutes"
  notification:
    - slack: "#alerts"
    - email: "oncall@example.com"
```

## Log Sampling Strategies

### Tail-based sampling

**Sample after execution:**
- Always keep errors
- Keep slow requests
- Sample normal requests

### Head-based sampling

**Sample at start:**
- Percentage-based (1%, 10%, 100%)
- Deterministic (by trace ID)

### Priority sampling

**Keep important logs:**
- All errors
- Specific users (VIP customers)
- High-value transactions
- Compliance-required events

## Integration Examples

### Datadog

```yaml
integrations:
  datadog:
    api_key: ${DATADOG_API_KEY}
    service: triform-prod
    tags:
      - env:production
      - team:backend
```

### Splunk

```yaml
integrations:
  splunk:
    endpoint: https://splunk.example.com:8088
    token: ${SPLUNK_HEC_TOKEN}
    index: triform_logs
```

### Custom Webhook

```yaml
integrations:
  webhook:
    url: https://logs.example.com/ingest
    headers:
      Authorization: "${API_KEY}"
    format: json
```

## Performance Considerations

**Logging overhead:**
- Structured logging: ~1-2ms per log
- Async logging: Minimal impact
- Sampling: Reduces volume and cost

**Storage:**
- Compressed storage
- Tiered retention
- Auto-archive to S3

## Pricing

**Included in all plans:**
- Basic logging (current)
- Structured logging
- Search and filtering

**Add-on for Enterprise:**
- Extended retention (>90 days)
- High-volume exports
- Premium integrations

## Timeline

**Q3 2026:** Structured logging, log levels  
**Q4 2026:** Advanced search, streaming  
**2027 Q1:** External integrations, alerts

## Get Notified

**Sign up:** [triform.ai/advanced-logging-beta](https://triform.ai/advanced-logging-beta)

## Questions?

- [Join Discord](https://discord.gg/triform) #observability
- Email: product@triform.ai



================================================================================
FILE: roadmap/metrics.mdx
================================================================================

---
title: "Metrics"
description: "Custom metrics and monitoring (Coming Soon)"
---

## Overview

Define, track, and visualize custom metrics for your Triform Projects, enabling data-driven optimization and monitoring.

**Status:** 🔮 Planned for Q3 2026

## What are Metrics?

Quantitative measurements that track system behavior, performance, and business outcomes over time.

## Built-in Metrics (Available Now)

**System metrics automatically tracked:**
- Execution count
- Success rate
- Execution duration
- Token usage
- Cost
- Error rate

**Available in:** Properties → Executions tab

## Custom Metrics (Coming Soon)

**Define your own metrics:**
```python
from triform import metrics

def process_order(order):
    start = time.time()
    
    # Your logic
    result = fulfill_order(order)
    
    # Track custom metrics
    metrics.increment("orders_processed")
    metrics.gauge("order_value", order.total)
    metrics.histogram("processing_time", time.time() - start)
    metrics.counter("items_shipped", order.item_count)
    
    return result
```

## Metric Types

### Counter

**Monotonically increasing value:**
```python
metrics.increment("api_calls")
metrics.increment("successful_logins")
metrics.add("messages_sent", count=5)
```

**Use for:** Total counts, events that accumulate

### Gauge

**Point-in-time value that can go up or down:**
```python
metrics.gauge("queue_size", current_size)
metrics.gauge("active_users", user_count)
metrics.set("temperature", 72.5)
```

**Use for:** Current state, levels, percentages

### Histogram

**Distribution of values:**
```python
metrics.histogram("request_duration", duration_ms)
metrics.histogram("payload_size", size_bytes)
metrics.observe("response_time", elapsed)
```

**Automatically calculates:** Min, max, mean, median, p50, p95, p99

**Use for:** Latencies, sizes, any distribution

### Summary

**Similar to histogram, less expensive:**
```python
metrics.summary("processing_time", duration)
```

**Use for:** High-cardinality measurements

## Metric Dimensions (Tags)

**Add context to metrics:**
```python
metrics.increment("api_calls", tags={
    "endpoint": "/users",
    "method": "GET",
    "status": "200"
})
```

**Query by dimensions:**
```
api_calls{endpoint="/users", status="200"}
```

## Aggregations

**Combine metrics:**
- **Sum:** Total across all instances
- **Average:** Mean value
- **Min/Max:** Extremes
- **Percentiles:** p50, p95, p99
- **Rate:** Change over time

## Time Windows

**Analyze over periods:**
- Last 5 minutes
- Last hour
- Last 24 hours
- Last 7 days
- Last 30 days
- Custom range

## Visualization

### Charts

**Built-in chart types:**
- Line charts (time series)
- Bar charts (comparisons)
- Area charts (cumulative)
- Pie charts (proportions)
- Heatmaps (distributions)

### Dashboards

**Custom dashboards:**
- Drag-and-drop widgets
- Multiple charts per dashboard
- Sharable links
- Auto-refresh
- Full-screen mode

## Alerts on Metrics

**Get notified when thresholds crossed:**
```yaml
alert:
  name: "High error rate"
  metric: error_rate
  condition: "> 5%"
  duration: "5 minutes"
  notify:
    - email: "oncall@example.com"
    - slack: "#alerts"
```

**Alert types:**
- Threshold (> or < value)
- Change detection (sudden spikes/drops)
- Anomaly detection (ML-based)
- Forecast-based (predicted to exceed)

## Use Cases

### Business Metrics

**Track outcomes:**
```python
metrics.increment("orders_completed")
metrics.gauge("revenue_today", daily_revenue)
metrics.histogram("customer_satisfaction", rating)
```

### Performance Monitoring

**System health:**
```python
metrics.histogram("api_latency", response_time)
metrics.gauge("memory_usage", memory_mb)
metrics.increment("cache_hits")
metrics.increment("cache_misses")
```

### Cost Tracking

**Spend attribution:**
```python
metrics.gauge("daily_llm_cost", llm_cost)
metrics.counter("api_calls_by_endpoint", tags={"endpoint": endpoint})
```

### Quality Metrics

**AI performance:**
```python
metrics.gauge("agent_response_quality", quality_score)
metrics.histogram("user_satisfaction", rating)
metrics.increment("thumbs_up")
metrics.increment("thumbs_down")
```

### Capacity Planning

**Usage trends:**
```python
metrics.gauge("active_projects", count)
metrics.gauge("executions_per_hour", rate)
```

## Example Dashboard

**Customer Support Bot Dashboard:**

**Row 1:**
- Total conversations (counter)
- Active conversations (gauge)
- Avg response time (histogram)

**Row 2:**
- Conversations over time (line chart)
- Resolution rate (gauge)
- User satisfaction (histogram)

**Row 3:**
- Escalations to human (counter)
- Top issues (bar chart)
- Response time distribution (heatmap)

## Metrics API

```python
from triform import Metrics

# Record metrics
Metrics.increment("orders", tags={"region": "us-west"})
Metrics.gauge("queue_depth", 42)
Metrics.histogram("latency", 123.45)

# Query metrics
data = Metrics.query(
    metric="api_latency",
    aggregation="p95",
    start_time="2025-10-01",
    end_time="2025-10-02",
    filters={"endpoint": "/users"}
)

# Create dashboard
dashboard = Metrics.create_dashboard(
    name="Production Overview",
    widgets=[
        {"type": "line", "metric": "request_rate"},
        {"type": "gauge", "metric": "error_rate"},
        {"type": "bar", "metric": "endpoint_usage"}
    ]
)
```

## Integrations

**Export metrics to:**
- Prometheus
- Graphite
- Datadog
- New Relic
- CloudWatch
- Custom StatsD

**Configuration:**
```yaml
metrics_export:
  provider: prometheus
  endpoint: https://prometheus.example.com
  scrape_interval: 15s
```

## Retention

**Metric retention:**
- Raw data points: 7 days
- 1-minute aggregates: 30 days
- 1-hour aggregates: 90 days
- 1-day aggregates: 1 year

**Longer retention available for Enterprise**

## Sampling

**For high-volume metrics:**
- Sample percentage of events
- Maintain accuracy with multiplier
- Reduce storage and cost

**Example:**
```python
metrics.increment("high_volume_event", sample_rate=0.1)
# Records 10% of events, multiplies by 10 for accurate count
```

## Best Practices

> **Name metrics clearly** — `user_login_success` not `event_1`

> **Use tags wisely** — Don't create too many dimensions (cardinality explosion)

> **Set appropriate types** — Counter for accumulation, Gauge for state

> **Alert on what matters** — Don't alert on everything

> **Dashboard for humans** — Clear, actionable, not overwhelming

## Pricing

**Included in all plans:**
- Basic built-in metrics
- Up to 100 custom metrics
- 30-day retention
- Basic dashboards

**Pro plan:**
- Up to 500 custom metrics
- 90-day retention
- Advanced dashboards
- Alerting

**Enterprise:**
- Unlimited custom metrics
- Custom retention
- External integrations
- Advanced analytics

## Timeline

**Q3 2026:** Custom metrics, basic dashboards  
**Q4 2026:** Advanced visualizations, alerting  
**2027 Q1:** External integrations, anomaly detection

## Get Notified

**Sign up:** [triform.ai/metrics-beta](https://triform.ai/metrics-beta)

## Questions?

- [Join Discord](https://discord.gg/triform) #observability
- Email: product@triform.ai



================================================================================
FILE: roadmap/monitoring.mdx
================================================================================

---
title: "Monitoring — In Progress"
description: "Logs, traces, metrics, usage and current focus"
---

## What Monitoring includes

- Logs — Detailed, structured runtime messages.
- Traces — Step‑by‑step paths with timing across Nodes.
- Metrics — Quantitative indicators (counts, durations, error rates).

## How you’ll use it

- Inspect an Execution to see its logs and trace waterfall.
- Drill from an erroring Node into its Action context.
- Compare metrics across Deploys to validate improvements.

## Current focus

- Consistent event schemas across Actions/Flows.
- Lightweight log ingestion with contextual fields (Node, Project, Execution).
- Trace propagation through Agent tool calls.

## Known limitations (while in progress)

- Partial cross‑Project stitching if an API calls another Project.
- Limited custom dashboards (predefined views first).

## Next steps

- Correlation IDs surfaced at API edges.
- Custom dimensions and filters for logs/metrics.
- Export hooks for shipping telemetry to external sinks.

> tip: Log with intent — Emit concise, structured logs that capture what happened and why—future you will thank present you.

================================================================================
FILE: roadmap/persistent-storage.mdx
================================================================================

---
title: "Persistent Storage — Upcoming"
description: "Backends, usage patterns, lifecycle, and next steps"
---

## Planned backends

- Object Storage (S3) — Large blobs and file artifacts.
- Vector Storage (Qdrant) — Embeddings, indexing, similarity search.
- Relational (Postgres) — Structured data with transactional guarantees.

## Usage patterns

- Object: Persist artifacts (e.g., pre/post‑transformation files), attach object IDs to Execution logs.
- Vector: Store embeddings for retrieval‑augmented generation; query during Agent tool calls.
- Relational: Keep normalized tables for user/config state; reference keys in Actions.

## Data lifecycle

- Define retention for logs and artifacts per environment.
- Tag records with Project/Execution IDs for traceability.
- Provide idempotent upserts to prevent duplication.

## Next steps

- Unified credentials/config via Variables.
- Node‑level connectors with simple read/write APIs.
- UI explorers to browse buckets, collections, and tables.

> tip: Prefer references — Store references (IDs, URIs) in your payloads and logs; fetch the heavy data on demand.

================================================================================
FILE: roadmap/schedules.mdx
================================================================================

---
title: "Advanced Schedules"
description: "Enhanced scheduling capabilities (Coming Soon)"
---

## Overview

Advanced scheduling features for more control over when and how your Projects execute automatically.

**Status:** 🔮 Planned for 2027 Q1

## Current Scheduling (Available Now)

**Basic schedule triggers:**
- Cron expressions
- Simple intervals
- One-time scheduled executions
- Timezone support

**See:** [Triggers documentation](/concepts/triggers) for current capabilities.

## What's Coming

### Visual Schedule Builder

**No more cron syntax:**
- Pick days and times visually
- See next 10 runs preview
- Natural language input
- Calendar view

**Example:** "Every weekday at 9 AM Pacific"

### Conditional Schedules

**Smart scheduling:**
```yaml
schedule:
  base: "Every hour"
  conditions:
    - skip_if: "no new data since last run"
    - skip_if: "execution_queue_length > 10"
    - run_only_if: "business_hours"
```

### Dynamic Schedules

**Adjust based on conditions:**
```python
# More frequent during business hours
if is_business_hours():
    schedule.set_interval("every 5 minutes")
else:
    schedule.set_interval("every hour")
```

### Schedule Dependencies

**Chain schedules:**
```yaml
schedules:
  - name: "Data Sync"
    cron: "0 0 * * *"  # Midnight
  
  - name: "Report Generation"
    after: "Data Sync"
    delay: "5 minutes"
  
  - name: "Email Report"
    after: "Report Generation"
    on_success_only: true
```

### Holiday Awareness

**Skip or adjust for holidays:**
```yaml
schedule:
  cron: "0 9 * * 1-5"  # Weekdays 9 AM
  holidays:
    calendar: "US_FEDERAL"
    action: "skip"  # or "reschedule"
```

### Load-Based Scheduling

**Adjust based on load:**
```yaml
schedule:
  base: "every 10 minutes"
  load_balancing:
    max_concurrent: 5
    queue_if_busy: true
    spread_execution: true
```

### Retry Policies

**Advanced retry logic:**
```yaml
schedule:
  cron: "0 * * * *"
  on_failure:
    retry_count: 3
    retry_delay: "exponential"  # 5m, 10m, 20m
    retry_on: ["timeout", "transient_error"]
    skip_on: ["validation_error"]
```

### Schedule Groups

**Manage related schedules:**
```yaml
schedule_group: "Daily Reports"
schedules:
  - sales_report: "0 8 * * *"
  - inventory_report: "0 8 * * *"
  - finance_report: "0 9 * * *"

actions:
  - pause_group  # Pause all during maintenance
  - resume_group
  - run_group_now  # Manual trigger
```

### Schedule Windows

**Restrict execution times:**
```yaml
schedule:
  cron: "*/5 * * * *"  # Every 5 minutes
  window:
    start: "08:00"
    end: "20:00"
    timezone: "America/Los_Angeles"
    action: "queue"  # Queue if outside window
```

### Jitter

**Prevent thundering herd:**
```yaml
schedule:
  cron: "0 * * * *"  # On the hour
  jitter:
    type: "random"
    max_delay: "5 minutes"
```

Actual execution: Randomly between :00 and :05

### Schedule Monitoring

**Track schedule health:**
- Missed executions
- Avg execution time per schedule
- Success rate by schedule
- Next scheduled time
- Last run status

### Schedule Alerts

**Get notified:**
```yaml
alerts:
  - name: "Schedule missed"
    condition: "schedule_missed"
    notify: "#ops"
  
  - name: "Schedule always fails"
    condition: "failure_rate > 80% over 24 hours"
    notify: "oncall@example.com"
```

## Use Cases

### Daily Reports

**Reliable reporting:**
```yaml
schedule:
  cron: "0 8 * * 1-5"  # Weekdays 8 AM
  holidays: "US_FEDERAL"
  on_failure:
    retry_count: 2
    notify: "reports-team@example.com"
```

### Data Sync

**Efficient synchronization:**
```yaml
schedule:
  cron: "*/15 * * * *"  # Every 15 minutes
  conditions:
    skip_if: "no_changes_detected"
    skip_if: "sync_already_running"
```

### Periodic Cleanup

**Maintenance tasks:**
```yaml
schedule:
  cron: "0 2 * * 0"  # Sundays 2 AM
  window:
    start: "01:00"
    end: "05:00"
  max_duration: "2 hours"
```

### Load Balancing

**Distribute load:**
```yaml
schedules:
  - id: "worker_1"
    cron: "0 * * * *"
    jitter: "0-10 minutes"
  
  - id: "worker_2"
    cron: "0 * * * *"
    jitter: "10-20 minutes"
```

## Calendar View

**Visual schedule management:**
- See all schedules on a calendar
- Click date to see scheduled executions
- Drag to reschedule
- Color-code by Project
- Filter by tag or Project

## Schedule API

```python
from triform import Schedules

# Create schedule
schedule = Schedules.create(
    project="my-project",
    cron="0 9 * * 1-5",
    timezone="America/New_York",
    holidays="US_FEDERAL",
    jitter_minutes=5,
    conditions={
        "skip_if_no_new_data": True
    }
)

# Update schedule
schedule.update(
    cron="0 8 * * 1-5",
    enabled=True
)

# Pause schedule
schedule.pause()

# Resume schedule
schedule.resume()

# Trigger now
schedule.run_now()

# Get next 10 runs
next_runs = schedule.get_next_runs(count=10)

# Check schedule health
health = schedule.get_health_metrics()
```

## Schedule Templates

**Quick setup:**
- Daily backup (2 AM)
- Business hours monitoring (8 AM - 6 PM)
- Weekly report (Monday 9 AM)
- Monthly cleanup (1st of month, 2 AM)
- High-frequency sync (every 5 min)
- Load-balanced workers

## Pricing

**Included in all plans:**
- Basic schedules (current features)
- Visual schedule builder
- Holiday awareness
- Conditional execution

**Pro and above:**
- Schedule dependencies
- Load balancing
- Advanced retry policies
- Schedule groups

**Enterprise:**
- Unlimited schedules
- Custom schedule logic
- Priority execution
- Dedicated scheduling infrastructure

## Timeline

**2027 Q1:** Visual builder, conditional schedules  
**2027 Q2:** Dependencies, groups, calendar view  
**2027 Q3:** Advanced features (load balancing, dynamic)

## Get Notified

**Sign up:** [triform.ai/schedules-beta](https://triform.ai/schedules-beta)

## Questions?

- [Join Discord](https://discord.gg/triform) #scheduling
- Email: product@triform.ai



================================================================================
FILE: roadmap/traces.mdx
================================================================================

---
title: "Traces"
description: "Detailed execution tracing and observability (Coming Soon)"
---

## Overview

Advanced execution tracing for deep observability into your AI systems, tracking every step, decision, and interaction.

**Status:** 🔮 Planned for Q2 2026

## What are Traces?

Comprehensive, granular records of execution flow showing exactly what happened at each step—LLM calls, tool invocations, data transformations, and decision points.

## Key Features

### Distributed Tracing

**Follow execution across components:**
- Trace ID propagation
- Parent-child relationships
- Cross-Project tracing
- Timeline visualization

### Span Details

**Every step recorded:**
- Function name and duration
- Input and output data
- Error messages and stack traces
- Metadata and tags
- Resource consumption

### LLM-Specific Tracing

**Deep visibility into Agent behavior:**
- Prompt templates and variables
- System and user messages
- Tool calls considered
- Tool execution results
- Model responses
- Token usage breakdown
- Reasoning/chain-of-thought

### Performance Analysis

**Identify bottlenecks:**
- Span duration breakdown
- Critical path analysis
- Concurrency visualization
- Resource usage hotspots

### Cost Attribution

**Track spending:**
- Cost per span
- LLM token costs
- API call costs
- Aggregate by component

## Example Trace

```
Trace ID: trace_abc123xyz
Duration: 3.2s
Cost: $0.04

└─ Flow: customer_support_flow (3.2s)
   ├─ Action: validate_input (0.1s, $0)
   │  └─ Input: {"message": "I need help"}
   │  └─ Output: {"valid": true}
   │
   ├─ Agent: support_agent (2.8s, $0.038)
   │  ├─ Prompt construction (0.01s)
   │  │  └─ Template: support_agent_system.txt
   │  │  └─ Variables: {company_name, policies}
   │  │
   │  ├─ LLM call: gpt-4 (1.5s, $0.030)
   │  │  └─ Prompt tokens: 850
   │  │  └─ Completion tokens: 200
   │  │  └─ Response: "I understand you need help..."
   │  │  └─ Tool calls: [check_account_status]
   │  │
   │  ├─ Tool: check_account_status (1.2s, $0)
   │  │  └─ Action: fetch_user_data
   │  │  └─ Duration: 1.2s
   │  │  └─ Result: {status: "active"}
   │  │
   │  └─ LLM call: gpt-4 (0.09s, $0.008)
   │     └─ With tool results
   │     └─ Final response generated
   │
   └─ Action: format_output (0.3s, $0)
      └─ Markdown formatting applied
```

## Visualization

### Timeline View

**Waterfall chart:**
- Horizontal bars showing duration
- Color-coded by type
- Nested structure
- Click to expand/collapse

### Graph View

**Node-edge diagram:**
- Components as nodes
- Data flow as edges
- Highlight critical path
- Filter by type

### Flamegraph

**Stack-based view:**
- Hierarchical time distribution
- Identify hot spots
- Interactive drill-down

## Search and Filter

**Find specific traces:**
```
status:error AND duration:>5s AND agent:customer_support
```

**Filter by:**
- Status (success, error, timeout)
- Duration range
- Cost range
- Component type
- Tag or metadata
- Time range

## Trace Sampling

**Control volume:**
- **All traces:** Full observability (dev/staging)
- **Sampled:** Random sample (production, high volume)
- **Error-only:** Trace only failures
- **Smart sampling:** Trace slow or expensive executions

## Integrations

### OpenTelemetry

**Industry standard:**
- Export to Jaeger
- Export to Zipkin
- Export to Datadog
- Export to New Relic

### Custom Backends

**Send traces anywhere:**
- S3/Cloud storage
- Your own database
- Analytics platforms
- SIEM systems

## Real-Time Streaming

**Live traces:**
- Watch executions in real-time
- Stream to dashboard
- WebSocket updates
- Tail logs live

## Trace Context Propagation

**Connect external systems:**
```python
# Your app
trace_id = generate_trace_id()

# Call Triform
response = triform.execute(
    project="my-project",
    payload={"data": "..."},
    trace_id=trace_id  # Propagate context
)

# Traces connected across systems
```

## Retention

**Trace storage:**
- Free: 7 days
- Pro: 30 days
- Enterprise: 90 days or custom

**Cost:** Included in plan (no extra charge for traces)

## Use Cases

### Debugging

**Find the root cause:**
- Why did execution fail?
- Where is the slowdown?
- What input caused this?

### Optimization

**Improve performance:**
- Identify slowest steps
- Find redundant calls
- Optimize critical path

### Cost Management

**Control spending:**
- Which components are expensive?
- Can we use cheaper models?
- Where to add caching?

### Compliance

**Audit trail:**
- What data was accessed?
- Which models were used?
- Who triggered execution?

## API

```python
from triform import Traces

# Get trace
trace = Traces.get("trace_abc123")

# List traces
traces = Traces.list(
    project="my-project",
    start_time="2025-10-01",
    end_time="2025-10-02",
    status="error"
)

# Export traces
Traces.export(
    trace_ids=["trace_1", "trace_2"],
    format="json",
    destination="s3://my-bucket/traces/"
)
```

## Pricing

**Included in all plans** — No additional cost

**Note:** Traces count toward storage quota if retained longer than default period.

## Timeline

**Q2 2026:** Basic tracing with timeline view  
**Q3 2026:** Advanced filtering, flamegraphs  
**Q4 2026:** OpenTelemetry export, real-time streaming

## Get Notified

**Sign up:** [triform.ai/traces-beta](https://triform.ai/traces-beta)

## Questions?

- [Join Discord](https://discord.gg/triform) #observability channel
- Email: product@triform.ai



================================================================================
FILE: changelog/index.mdx
================================================================================

---
title: "Changelog"
description: "Latest updates, features, and improvements to Triform"
---

## October 2025

### v2.8.0 — September 30, 2025

**New Features:**
- 🎯 **Flow View Enhancements** — Improved node handles
- 🤖 **Triton Improvements** — Better code generation, faster responses, improved error explanations
- 🔐 **Enhanced Security** — Improved HMAC signature validation

**Improvements:**
- Canvas performance optimized for large Projects (100+ nodes)
- Faster execution startup times (30% reduction in cold starts)
- Better error messages throughout the platform

**Bug Fixes:**
- Fixed: Project Variables not updating in real-time
- Fixed: Webhook retry logic not respecting exponential backoff

### v2.7.5 — September 25, 2025

**Improvements:**
- Properties Panel tabs reorganized for better UX
- Faster Project switching (reduced loading time)

**Bug Fixes:**
- Fixed: Schedule triggers sometimes skipping executions
- Fixed: Agent tool selection UI glitch
- Fixed: API key permissions not applying immediately

## September 2025

### v2.7.0 — September 20, 2025

**New Features:**
- 🔄 **Auto-Retry Logic** — Automatic retries with exponential backoff for Actions
- 📦 **Component Library** — Share and reuse Actions/Flows across Projects (backend, FE comes later)

**Improvements:**
- Triton now suggests optimizations for slow Flows
- API response time improvements (20% faster)

**Bug Fixes:**
- Fixed: Parallel Flow execution sometimes deadlocking
- Fixed: Agent token usage not tracked correctly
- Fixed: Deployment rollback not reverting all changes
- Fixed: Project Variables not substituting in nested objects

### v2.6.5 — September 5, 2025

**New Features:**
- ⚡ **Quick Actions** — Keyboard shortcuts for common operations

**Improvements:**
- Canvas now supports up to 500 nodes per Flow
- Execution logs load 50% faster
- Improved Agent prompt editor with syntax highlighting

**Bug Fixes:**
- Fixed: Input/Output schema validation edge cases
- Fixed: Webhook signature verification for some providers
- Fixed: Canvas edge rendering on high-DPI displays

## August 2025

### v2.6.0 — August 18, 2025

**Major Release: Triton 2.0**

**New Features:**
- 🤖 **Triton 2.0** — Complete rewrite with better understanding and faster responses
- 🧩 **Flow Templates** — Pre-built patterns for common workflows
- 📱 **Mobile App (Beta)** — View Projects and executions on mobile
- 🔐 **SSO Support** — SAML integration for Enterprise (beta)

**Improvements:**
- Agent system prompts can now reference Project Variables
- Execution replay with modified payloads
- Better webhook debugging tools
- Organization member management UI refresh

**Bug Fixes:**
- Fixed: Race condition in parallel Flow execution
- Fixed: API key scoping not working for sub-Projects
- Fixed: Canvas performance with many edges
- Fixed: Execution metrics occasionally showing wrong values

## July 2025

### v2.5.2 — July 28, 2025

**New Features:**
- 🏷️ **Project Tags** — Organize and filter Projects with custom tags
- 📈 **Success Rate Alerts** — Get notified when success rate drops
- 🔄 **Execution Replay** — Re-run failed executions easily

**Improvements:**
- Faster Project loading (especially large Projects)
- Better error messages for validation failures
- Improved documentation search
- Canvas grid snapping enhancements

**Bug Fixes:**
- Fixed: Deployments sometimes showing wrong status
- Fixed: Global Variable encryption edge cases
- Fixed: Agent tool descriptions not updating
- Fixed: Execution log timestamps in wrong timezone

## June 2025

### v2.5.0 — June 15, 2025

**New Features:**
- 🎯 **Conditional Routing** — Router nodes for Flow branching logic
- 📊 **Execution Dashboard** — New overview of all executions across Projects
- 🔗 **Webhook Testing** — Test webhooks directly from UI
- 💬 **Comments** — Add notes and comments to Canvas nodes

**Improvements:**
- Properties Panel is now resizable
- Execution logs support filtering and search
- Better handling of large payloads (up to 10MB)
- Improved Agent response streaming

**Bug Fixes:**
- Fixed: Schedule triggers in different timezones
- Fixed: Edge cases in dependency resolution
- Fixed: Canvas context menu positioning
- Fixed: API key usage tracking

## May 2025

### v2.4.0 — May 10, 2025

**New Features:**
- 🎨 **Custom Branding** — White-label option for Enterprise

**Improvements:**
- Canvas performance improvements (50% faster rendering)
- Improved error handling in Actions

**Bug Fixes:**
- Fixed: Deployment history not showing all versions
- Fixed: Agent tool calling timeout issues
- Fixed: Canvas undo/redo state issues
- Fixed: API rate limit headers incorrect

## Deprecation policy

When we deprecate features:

1. **Announce** — At least 90 days notice
2. **Document** — Migration guide provided
3. **Support** — Old feature continues working during deprecation period
4. **Remove** — Only after notice period and next major version

## Beta features

Features marked **(Beta)** are:
- Functional but may have bugs
- Subject to changes
- Available to test and provide feedback
- Not recommended for production-critical workloads

## Staying updated

**Email digest:** Opt-in for monthly update emails  
**Discord:** Announcements in community server  
**RSS feed:** https://triform.ai/changelog/feed.xml

## Reporting issues

Found a bug? We want to know!

**Report via:**
- Bug icon in app (right)
- Discord: #bugs channel

**Please include:**
- What happened
- What you expected
- Steps to reproduce
- Screenshots (if applicable)
- Browser/environment info

## Feature requests

Have an idea? We'd love to hear it!

**Submit via:**
- Feature request icon in app
- Email: product@triform.ai
- Discord: #feature-requests channel

**What makes a good feature request:**
- Clear description of the problem
- Why it matters to you
- How you'd use it
- Any examples or mockups

Popular requests often get built first!

## Next up

Check our Roadmap to see what's coming next.



================================================================================
FILE: community/join-discord.mdx
================================================================================

---
title: "Join the Conversation"
description: "Where to find us, how to get help, and code of conduct"
---

## Where to find us

- Discord — Chat with the team and community, share patterns, get help.
- Roadmap thread — Track progress and suggest features.

👉 Join here: https://www.notion.so/Roadmap-1fa0c5f9afca80fbba64e5d45d1bd951?pvs=21

## How to get the most out of the community

- Include context: Org/Project, component names, and a minimal Payload.
- Share what you tried and what you expected.
- Tag posts with #help, #feature‑request, or #show‑and‑tell.

## Code of conduct

- Be respectful. Assume good intent.
- No sensitive data or secrets in public channels.
- Credit sources and contributors.

> tip: Help us help you — A small, reproducible example (one Flow, one Payload) dramatically speeds up assistance.

================================================================================
FILE: support/contact-us.mdx
================================================================================

---
title: "Contact Us"
description: "Get in touch with the Triform team"
---

## General Support

For questions, technical support, or assistance:

**Discord:** [discord.gg/triform](https://discord.gg/triform)
- General questions: #help
- Community discussion: #general
- Response: Community-driven, usually < 1 hour

## Specific Inquiries

### Sales & Pricing

**For:** Enterprise quotes, volume pricing

**Email:** sales@triform.ai  
**Phone:** +46722 99 99 99 

**We'll discuss:**
- Your use case and requirements
- Appropriate plan tier
- Custom pricing (Enterprise)
- Onboarding and training
- SLA and support options

### Partnerships

**For:** Integration partnerships, reseller agreements, co-marketing

**Email:** sales@triform.ai

**Response time:** 1 week

**Partnership types:**
- Technology integrations
- Reseller/referral programs
- Co-marketing opportunities
- Strategic partnerships

### Press & Media

**For:** Press releases, media inquiries, interview requests

**Email:** press@triform.ai

**Response time:** 2 business days

**Available:**
- Press kit and brand assets
- Executive interviews
- Product demos
- Customer stories

### Careers

**For:** Job opportunities, internships, working at Triform

**Discord:** [discord.gg/triform](https://discord.gg/triform)

**We're hiring for:**
- Engineering (frontend, backend, ML)
- Product management
- Design
- Customer success
- Sales

## Social Media

Follow us for updates, tips, and community:

**Twitter/X:** [@triformai](https://x.com/triformai)  
**LinkedIn:** [linkedin.com/company/triformai](https://linkedin.com/company/triformai)  
**YouTube:** Coming soon  
**Discord:** [discord.gg/triform](https://discord.gg/triform)

## Office

Come visit at our office!

**Headquarters:** Götgatan 23, 116 30 Stockholm, Sweden
**Team:** Distributed across the Nordics

## Support Hours

### Email & Discord

**Available:** 24/7 (you can send anytime)  
**Response:** Based on tier and business hours

**Business hours:** Monday-Friday, 9 AM - 6 PM PT

### Phone Support

**Enterprise only**

**Hours:** Monday-Friday, 9 AM - 6 PM PT  
**Request:** Via your dedicated Slack channel or account manager

## What to Include

When contacting us, please include:

### For support requests:

- **Description:** Clear explanation of your issue or question
- **Account:** Your Organization ID or email
- **Context:** What you were doing when the issue occurred
- **Expected:** What you expected to happen
- **Actual:** What actually happened
- **Attempts:** What you've tried to fix it
- **Urgency:** How urgent is this for you?
- **Screenshots:** Visual evidence (if helpful)

### For sales inquiries:

- **Company:** Your company name and size
- **Use case:** What you're trying to build
- **Requirements:** Must-have features or compliance needs
- **Timeline:** When you need to start
- **Contact:** Best way to reach you

### For partnerships:

- **Company:** Your company and what you do
- **Proposal:** What kind of partnership
- **Value:** How it benefits both parties
- **Timeline:** When you'd like to start
- **Contact:** Best person to discuss with

## Response Expectations

### We will:

✅ Acknowledge your message within stated timeframe  
✅ Provide helpful, accurate information  
✅ Follow up until issue is resolved  
✅ Escalate if needed  
✅ Keep you informed of progress

### We may:

⚠️ Ask for more information to help better  
⚠️ Suggest alternative solutions  
⚠️ Refer you to documentation  
⚠️ Connect you with relevant team member

### We won't:

❌ Ignore your message  
❌ Provide inaccurate information  
❌ Be rude or unhelpful  
❌ Share your information inappropriately

## Language Support

**Primary:** English and Sweden

**Additional languages:** Coming soon (Spanish, French, German, Japanese)

**For now:** English-only for fastest support. Feel free to reach out in your language, but responses will be in English.

## Thank You

We appreciate you choosing Triform and taking the time to reach out. We're here to help you succeed!

**Looking forward to hearing from you!** 📧

## Related

Continue exploring the support documentation and join our Discord community for help.



================================================================================
FILE: support/overview.mdx
================================================================================

---
title: "Support Overview"
description: "Get help with Triform"
---

## Getting Help

We're here to help you succeed with Triform. Whether you have questions, need technical support, or want to provide feedback, we have multiple channels available.

## Support Channels

### Documentation

**Start here** — Most questions are answered in our comprehensive docs.

**Quick links:**
- [Getting Started](/getting-started/quickstart)
- [Tutorials](/tutorials/build-a-new-project)
- [Core Concepts](/concepts/projects)
- [API Reference](/api-reference/introduction)
- [FAQ](/support/faq)

**Search:** Use the search bar (Cmd+K / Ctrl+K) to find answers quickly.

### Triton (AI Assistant)

**Built-in help** — Ask Triton directly in the Chat Panel.

**Triton can help with:**
- How to use features
- Debugging errors
- Building components
- Best practices
- Quick answers

**Example questions:**
- _"How do I deploy a Project?"_
- _"Why is my execution failing?"_
- _"What's the best way to handle errors?"_

### Community Discord

**Join the conversation** — Chat with other Triform users and our team.

**Discord server:** [discord.gg/triform](https://discord.gg/triform)

**Channels:**
- `#general` — General discussion
- `#help` — Get help from community
- `#showcase` — Share what you've built
- `#feature-requests` — Suggest new features
- `#bugs` — Report issues
- `#announcements` — Product updates

**Response time:** Community-driven, usually < 1 hour during business hours

**Best for:** Quick questions, community interaction, sharing ideas

### Enterprise Support

**Premium support** — Dedicated support for Enterprise customers.

**Includes:**
- Dedicated Discord channel
- Phone support
- Video calls
- Custom onboarding
- Quarterly business reviews
- 99.9% uptime SLA

**Contact:** Your account manager or enterprise@triform.ai

## Common Issues

### Can't log in

**Try:**
1. Password reset (if using email auth)
2. Check 2FA device is correct time
3. Try different browser or incognito mode
4. Clear cookies for app.triform.ai
5. Check provider status (Discord, GitHub) if using OAuth

**Still stuck?** Email support@triform.ai with your email address

### Execution failing

**Debug steps:**
1. Check execution logs in Properties → Executions
2. Verify input payload matches schema
3. Test each component individually
4. Check Project Variables are set correctly
5. Ask Triton: _"Why did execution #[ID] fail?"_

**Still stuck?** Share execution ID with support

### Deployment not working

**Check:**
1. All components configured and tested
2. Project Variables set for production environment
3. API key has deployment permissions
4. No circular dependencies
5. Entry point is valid

**Still stuck?** [Contact us](/support/contact-us) with Project ID

### API requests failing

**Verify:**
1. API key is correct and active
2. Header format: `Authorization: YOUR_KEY`
3. Content-Type: `application/json`
4. Payload matches input schema
5. Not hitting rate limits

**Still stuck?** Share request and response details with support

### Can't find a feature

**Try:**
1. Search docs (Cmd+K / Ctrl+K)
2. Ask Triton: _"How do I..."_
3. Check if it's Pro/Enterprise only

**Still stuck?** Ask in Discord #help

## Reporting Issues

### Bugs

Found a bug? Please report it!

**Report via:**
- Bug icon in app (bottom-right)
- Discord #bugs channel
- Email: support@triform.ai

**Include:**
- What you were doing
- What you expected
- What actually happened
- Steps to reproduce
- Screenshots or video (if helpful)
- Browser/environment info

**We'll:**
Triage severity
Fix and update you on progress
Credit you in changelog (optional)

### Security Issues

**Critical:** If you've found a security vulnerability:

**Report to:** security@triform.ai

**Do NOT:**
- Post publicly
- Exploit the vulnerability
- Access other users' data

**We'll:**
- Acknowledge within 24 hours
- Investigate promptly
- Keep you informed
- Credit you in security updates (optional)
- Reward responsibly disclosed issues

**See:** [Security Overview](/security/overview) for details

## Feature Requests

Have an idea to improve Triform?

**Submit via:**
- Discord #feature-requests channel

**Good feature requests include:**
- What problem it solves
- How you'd use it
- Why it matters
- Examples or mockups (optional)

**We'll:**
- Review all requests
- Upvote popular ones
- Add to roadmap if aligned
- Update you on status

**Popular requests get built faster!**

## Feedback

We value your feedback to make Triform better.

**Share feedback on:**
- Product features
- Documentation
- User experience
- Performance
- Anything else!

**Via:**
- Feedback icon in app
- Discord #general

**Your input shapes our roadmap.**

**SLA credits:** If we don't meet guaranteed uptime, you receive service credits. See service agreement for details.

**Discord:** [discord.gg/triform](https://discord.gg/triform)  
**Twitter/X:** [@triformai](https://x.com/triformai)  
**LinkedIn:** [linkedin.com/company/triformai](https://linkedin.com/company/triformai)

## Self-Service Resources

Before contacting support, try:

✅ **Search documentation** — Most questions answered here  
✅ **Ask Triton** — Built-in AI assistant  
✅ **Search Discord** — Community might have answered it  
✅ **Review execution logs** — Error details help debug

**We're committed to resolving all issues.**

## International Support

**Languages:** Swedish, English and Finnish

**Regional data:** EU Only

## Thank You

We appreciate your patience and partnership as we build Triform together. Your questions, feedback, and support help us improve every day.

**Happy building! 🚀**



================================================================================
FILE: legal/community-terms.mdx
================================================================================

---
title: Triform Community Version Terms
description: Minimal terms for the free, public-by-default Triform Community Version.
---

# Triform Community Version Terms

**Effective date:** October 9, 2025  
**Entity:** Triform AB, Götgatan 23, 116 46 Stockholm, Sweden  
**Contact:** contact@triform.ai

> **Summary (not a substitute for the terms):** Your **creations and source code are public**. Your **executions, inputs, and outputs are not shared**; Triform may use **de-identified** execution data to operate, improve, and train systems and models. Use **secret variables** for credentials or sensitive values.

## 1) Eligibility
- You must be **16+** (or the age of digital consent in your country, if higher).  
- If you use Triform on behalf of an organization, you confirm you have authority to accept these terms.

## 2) Definitions
- **Creations**: your project assets that you intentionally publish to the project workspace, including **source code**, prompts, configs, docs, and non-secret assets.  
- **Execution Data**: runtime data from building/running agents (e.g., inputs, outputs, logs, traces, error messages, metrics).  
- **Secret Data**: values you pass via secret variables/inputs (e.g., API keys, credentials, personal data).

## 3) What is public vs. private
- **Public:** Your **Creations (source code and non-secret assets)** are **public by default** and visible to anyone.  
- **Not shared publicly:** **Execution Data** and **Secret Data** are **not disclosed to the public** by Triform.

## 4) MIT-Licenses for public Creations (code)
- **License to the public:** By using the Community Version, you grant everyone a **worldwide, royalty-free, non-exclusive, irrevocable, perpetual license** to **use, copy, modify, create derivative works of, and distribute** your **public code/Creations**, including commercially, with **reasonable attribution** (your Triform handle + project URL).
- **License to Triform:** You grant Triform a matching license to host, display, index, remix, and showcase your public Creations and to use them to operate and improve the service.

## 5) Use of Execution Data for training (private to the public)
- Triform may **process and use Execution Data** to operate, secure, evaluate, and **train/improve** our systems and models.  
- **PII cleaning/anonymization:** Before any training or analysis use, we **remove or transform personally identifiable data** and **exclude Secret Data** (e.g., secret variables, credentials). We do **not** intentionally include raw secrets in training corpora.  
- We do **not** publish or disclose Execution Data to the public. We do **not** attempt to re-identify de-identified data.  
- If you delete a project, we stop using it for future training. Already-trained models may not be retroactively "untrained." Backups may persist for a limited period for security/compliance.

## 6) Your responsibilities
- Do not upload or hard-code secrets; use **secret variables**.  
- Only include materials you have rights to share **publicly** in your Creations (e.g., your own code).  
- Do not use Triform for illegal, infringing, abusive, or harmful activities, or to violate third-party API/model terms.

## 7) Moderation; reporting
- We may remove content, restrict access, or take action where these terms or the law are violated.  
- Report issues or request takedown at **report@triform.ai** (include project URL and reason).

## 8) Service changes
The Community Version is provided **as-is** without SLA. We may change or discontinue features. If we make **material changes** to these terms, we will notify you and update the effective date.

## 9) Disclaimers; liability
THE SERVICE IS PROVIDED "AS IS" WITHOUT WARRANTIES. TO THE MAXIMUM EXTENT PERMITTED BY LAW, TRIFORM IS NOT LIABLE FOR INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, EXEMPLARY, OR PUNITIVE DAMAGES, OR LOST PROFITS/REVENUE/DATA/GOODWILL. DIRECT DAMAGES ARE LIMITED TO **€100**.

## 10) Governing law; arbitration
These terms are governed by the laws of **Sweden**. Any disputes arising from these terms shall be resolved through **binding arbitration in Stockholm, Sweden** in accordance with the rules of the Arbitration Institute of the Stockholm Chamber of Commerce.

---

By creating an account or using the Community Version, you agree to these terms.



================================================================================
FILE: appendix/quick-reference.mdx
================================================================================

---
title: "Quick Reference"
description: "Compact cheat sheet for everyday Triform tasks"
---

## Navigation

- Breadcrumbs (Top Bar): Jump between Org → Project → Component.
- Home (logo): Go to Projects dashboard.
- Profile: Switch Organization, view Org settings, log out.

## Canvas basics

- Click: Select a Node.
- Double‑click: Open Project/Agent/Flow internals.
- Right‑click: Context actions on Canvas or Node.
- Drag: Reposition Nodes to clarify flow.

## Building blocks

- Project View: Top‑level Agents/Flows eligible for API/Schedule.
- Agent Toolbox: Add tools (Actions/Flows/Agents) via ＋.
- Flow View:
  - Input/Output Nodes always present.
  - ＋ on Input/Output to add named ports.
  - Drag from a connection point to create Edges or new components.

## Properties Panel

- Project Variables: Project-scoped configuration values
- Components → Definition: Intent, requirements, constraints
- Components → Content:
  - Agents: Model, prompts, tools, advanced settings
  - Flows: Arrange and connect sub-components
  - Actions: `Action.py`, `README`, `requirements.txt`
- Execute: Run with a JSON payload; project-level API/Schedule exposure and API key management
- Input & Output: Define ports, types, defaults; toggle Parallel

## Executions

- Start manually, via API/Schedule, or from another Node.
- Each run is isolated, reproducible, and traceable.
- Use Payloads to save examples and rerun tests quickly.

## Quotas at a glance

- Execution‑Based: total runs, concurrency, duration, depth, recursion.
- Resource‑Based: payload size, script size, stored Payloads, module size/deps.
- API & Integrations: request rate, endpoint concurrency, schedules, publishes.
- Account‑Specific: environments, variables/secrets, alert/evaluation counts.

## Deployment & exposure

- Deploy (Top Bar) appears when local changes outpace last deploy.
- Expose top‑level Agents/Flows as APIs or Schedules under Execute.
- Manage API keys at the Project level.

## Troubleshooting quick wins

- Mismatched I/O: Align port names and types on both ends.
- Over‑large payload: Split into batches; toggle Parallel.
- Agent drift: Reduce tools, tighten System Prompt, re‑test with saved Payloads.

> tip: Golden path for a new Flow — 1) Input `text` → 2) Action `summarize_text` → 3) Output `summary`. Save `minimal` and `edge` Payloads, validate, then wire it into a larger Flow or expose it as an API.

================================================================================
END OF DOCUMENTATION
================================================================================
Total files processed: 81
Generated: Thu Oct  9 13:52:39 CEST 2025
================================================================================
